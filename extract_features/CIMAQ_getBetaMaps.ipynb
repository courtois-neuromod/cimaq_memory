{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial-Unique Beta Maps Creation Processing Steps\n",
    "\n",
    "## From the CIMAQ Memory Task (Image Encoding) fMRI Data - Within-Subject Level\n",
    "\n",
    "### Goal: Feed Outputs (Beta Maps) as Features to a Within-Subject Nilearn Classifier\n",
    "\n",
    "##### Input:\n",
    "\n",
    "- Event files \n",
    "- Confounds (motion, etc) files\n",
    "    - Generated by load_confound\n",
    "- Preprocessed FMRIPrep data (4D .nii file)\n",
    "\n",
    "**- Note: Data NOT Smoothed nor Denoised\n",
    "\n",
    "##### Output: \n",
    "\n",
    "- 1 map (3D .nii file) of beta (regression) weights for each trial\n",
    "- 1 concatenated 4D file of these 3D maps (trials ordered chronologically).\n",
    "\n",
    "#### Version 1: Separate Model for Each Trial\n",
    "\n",
    "- Trial of interest modelled as a separate condition (1 regressor)\n",
    "- All other trials modelled in either the Encoding or Control condition (2 regressors)\n",
    "   \n",
    "**NOTE: Best of both models for enc/ctl trial classification**\n",
    "\n",
    "#### Version 2: Separate model for Each Trial\n",
    "\n",
    "- Trial of interest modelled as a separate condition (1 regressor)\n",
    "- All other trials modelled as a single \"other\" condition (1 regressor)\n",
    "\n",
    "Reference: How to derive beta maps for MVPA classification (Mumford et al., 2012):\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1053811911010081\n",
    "\n",
    "#### Also creating contrasts per condition (to derive features for between-subject classification): \n",
    "\n",
    " - Modeling enconding and control conditions across trials\n",
    "     - 3 beta maps:\n",
    "         - encoding (enc) , control (ctl), and encoding minus control (enc_minus_ctl)\n",
    " - Modeling control condition, as well as the encoding condition according to task performance:\n",
    "    - miss and hit (post-scan image recognition performance)\n",
    "    - 5 beta maps:\n",
    "        - miss, hit hit_minus_miss, hit_minus_ctl, miss_minus_ctl\n",
    "    - Modeling control condition & encoding condition according to task performance:\n",
    "        - miss, wrong source, and correct source\n",
    "    - 7 beta maps:\n",
    "        - wrong_source, corr_source, cs_minus_ws, cs_minus_miss, ws_minus_miss, cs_minus_ctl, ws_minus_ctl\n",
    "        \n",
    "\n",
    "### Step 1: Load confound parameters\n",
    "\n",
    "```\n",
    "from load_confounds import Minimal\n",
    "\n",
    "confounds = Minimal().load(FMRIPrep/preprocessed/fmri_img/file/path)\n",
    "```\n",
    "\n",
    "### From the load_confounds README document:\n",
    "\n",
    "#### Note on low pass filtering\n",
    "\n",
    "Common operation in resting-state fMRI analysis\n",
    "- Featured in all preprocessing strategies of the Ciric et al. (2017) paper\n",
    "\n",
    "fMRIprep does not output low pass filtering discrete cosines\n",
    "- Can be implemented directly with the nilearn masker\n",
    "    - ``low_pass`` argument \n",
    "**Specify the nilearn masker argument ``t_r`` if low_pass is used\n",
    "\n",
    "#### Note on high pass filtering and detrending\n",
    "\n",
    "Nilearn maskers & first-level model can remove slow time drifts & noise:\n",
    "- ``high_pass`` & ``detrend`` arguments\n",
    "- Both **redundant** with fMRIprep high_pass regressors\n",
    "    - Both included in all load_confounds strategies\n",
    "**Do NOT use nilearn's ``high_pass`` or ``detrend`` options with the default strategies.**\n",
    "\n",
    "- A flexible ``Confounds`` loader can exclude fMRIprep high_pass noise components\n",
    "    - Allows relying on nilearn's ``high_pass`` or ``detrending`` options\n",
    "    **- NOT advised with compcor or ica_aroma analysis**\n",
    "\n",
    "#### Note on demeaning confounds\n",
    "\n",
    "**Confounds should be demeaned** (default load_confounds behaviour)\n",
    "- Required to properly regress out confounds using nilearn\n",
    "    - With the standardize=False, standardize=True or standardize=\"zscore\" options\n",
    "    - standardize=\"psc\" requires turning off load_confounds demeaning option\n",
    "    ```\n",
    "    from load_confounds import Params6\n",
    "    conf = Params6(demean=False)\n",
    "    ```\n",
    "    - Unless using nilearn maskers or first-level model ``detrend`` or ``high_pass`` options\n",
    "\n",
    "\n",
    "### Step 2: create events variable & events.tsv file\n",
    "\n",
    "#### From the 'sub-*_ses-V*_task-memory_events.tsv' file outputed by cimaq2bids.py \n",
    "\n",
    "Number of rows = number of trials\n",
    "\n",
    "- First-level model uses trial onset times to match trial conditions to fMRI frames\n",
    "\n",
    "Documentation:\n",
    "\n",
    "https://nistats.github.io/auto_examples/04_low_level_functions/write_events_file.html#sphx-glr-auto-examples-04-low-level-functions-write-events-file-py\n",
    "\n",
    "- Each encoding trial is modelled as a different condition (under trial_type column)\n",
    "    - Modelled separately in the design matrix\n",
    "        - Trial of interest has its own column in the design matrix\n",
    "        - Other columns = other trials &  confound regressors\n",
    "            - Modelled together as a single regressor\n",
    "\n",
    "**Note: Some scans were cut short**\n",
    "- The last few trials have NO associated brain activation frames\n",
    "    - These need to be left out of the analysis\n",
    "- MEMO: 310 frames = full scan, 288 frames = incomplete (~15 participants).\n",
    "\n",
    "- \"unscanned\" trials need to be excluded from the model (about ~2-4 trials missing).\n",
    "\n",
    "- E.g.:\n",
    "    - 288*2.5 = 720s.\n",
    "    - Trial #115 (out of 117) offset time ~ 710s\n",
    "    - Trial #116 (out of 117) onset ~ 723s\n",
    "\n",
    "\n",
    "### Step3 : Implement first-level model (implements regression in nilearn)\n",
    "\n",
    "#### Generates contrasts and output maps of beta values (parameter estimators; one map of betas per trial).\n",
    "\n",
    "**About first-level model:\n",
    "\n",
    "Note 1: ``nilearn.glm.first_level_model`` provides an interface for ``nilearn.glm``\n",
    "\n",
    "Note 2: Each encoding trial is modelled as a separate condition to obtain separate maps of beta values\n",
    "- Model's output type to get betas = **effect sizes**\n",
    "\n",
    "    - ``nilearn.glm.first_level.FisrtLevelModel.compute_contrast`` ``output_type`` parameter name\n",
    "\n",
    "- Version A:\n",
    "    - Control trials & encoding trials are modelled separately\n",
    "        - 2 regressors\n",
    "- Version B:\n",
    "    - Control trials & encoding trials are modelled together\n",
    "        - Single \"other_trials\" condition (1 regressor)\n",
    "\n",
    "Note 3: the first_level_model can either be given its parameters in 2 ways:\n",
    "\n",
    "1. Pre-constructed **design matrix**\n",
    "    - 2-step method (chosen method here)\n",
    "    - Built from the events and confounds files in a separate preparatory step\n",
    "    - Takes precedence over the events and confounds parameters (method 2)\n",
    "2. Events & confounds directly\n",
    "    - 1-step method\n",
    "    - Skipping the need to create a design matrix in a separate step\n",
    "    - The model will generate the design matrix automatically\n",
    "\n",
    "\n",
    "Nilearn links on design matrices:\n",
    "\n",
    "https://nilearn.github.io/modules/generated/nilearn.plotting.plot_design_matrix.html#nilearn.plotting.plot_design_matrix\n",
    "\n",
    "https://nilearn.github.io/modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html#nilearn.glm.first_level.make_first_level_design_matrix\n",
    "\n",
    "##### Examples\n",
    "\n",
    "- First-Level Model\n",
    "\n",
    "https://nilearn.github.io/modules/generated/nilearn.glm.first_level.FirstLevelModel.html#nilearn.glm.first_level.FirstLevelModel\n",
    "\n",
    "- Beta Map Extraction (Nistats)\n",
    "\n",
    "https://github.com/poldracklab/fitlins/pull/48\n",
    "\n",
    "**About contrasts and maps of beta values (parameter estimators):\n",
    "\n",
    "- To access the estimated coefficients (betas of the GLM model)\n",
    "    - We need to specify \"canonical contrasts\" (one per trial) isolating design matrix columns\n",
    "        - Each contrast has a single 1 in its corresponding colum, and 0s for all the other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import builtins\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import more_itertools\n",
    "import nibabel\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import tqdm\n",
    "import typing\n",
    "import warnings\n",
    "\n",
    "from load_confounds import Minimal\n",
    "from matplotlib import pyplot as plt\n",
    "from nibabel.nifti1 import Nifti1Image\n",
    "from os import PathLike\n",
    "from pathlib import Path, PosixPath\n",
    "from random import sample\n",
    "from builtins import FutureWarning\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "from nilearn.glm.first_level import FirstLevelModel, check_events\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn import image as nimage\n",
    "from nilearn.input_data import MultiNiftiMasker, NiftiLabelsMasker\n",
    "from nilearn.input_data import NiftiMapsMasker, NiftiMasker\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "from nilearn import plotting as niplot\n",
    "from nilearn import image as nimage\n",
    "from scipy.spatial import procrustes\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, VarianceThreshold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, recall_score, pairwise_distances\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import Bunch\n",
    "from tqdm import tqdm as tqdm_\n",
    "from typing import Iterable, Sequence, Union\n",
    "\n",
    "from get_difumo import get_difumo\n",
    "from get_difumo_cut_coords import get_difumo_cut_coords\n",
    "from cimaq_decoding_pipeline import get_fmri_sessions, fetch_fmriprep_session, validate_model\n",
    "from cimaq_decoding_pipeline import get_contrasts, get_all_contrasts, get_glm_events\n",
    "from cimaq_decoding_utils import flatten, get_t_r, get_frame_times\n",
    "\n",
    "sns.set(rc={'figure.figsize': (18,18)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_classif, f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import GenericUnivariateSelect\n",
    "\n",
    "\n",
    "def GetSklearnPairwiseMetrics():\n",
    "    while True:\n",
    "        sklearn_func_names = ['cityblock', 'cosine', 'euclidean',\n",
    "                              'l1', 'l2', 'manhattan']\n",
    "        sklearn_dict = sklearn.metrics.pairwise.distance_metrics()\n",
    "        yield pd.Series(itemgetter(*sklearn_func_names)(sklearn_dict),\n",
    "                        index=sklearn_func_names, name='sklearn_funcs')\n",
    "        \n",
    "\n",
    "def GetScipyPairwiseMetrics():\n",
    "    while True:\n",
    "        scipy_func_names = ['braycurtis','canberra', 'chebyshev',\n",
    "                            'correlation', 'dice', 'hamming',\n",
    "                            'jaccard', 'kulsinski', 'mahalanobis',\n",
    "                            'minkowski', 'rogerstanimoto',\n",
    "                            'russellrao', 'seuclidean',\n",
    "                            'sokalmichener', 'sokalsneath',\n",
    "                            'sqeuclidean', 'yule']\n",
    "        scipy_dict = dict(inspect.getmembers(scipy.spatial.distance))\n",
    "        yield pd.Series(data=itemgetter(*scipy_func_names)(scipy_dict),\n",
    "                        index=scipy_func_names, name='scipy_funcs')\n",
    "\n",
    "\n",
    "def GetPairwiseMetrics():\n",
    "    import inspect\n",
    "    import scipy\n",
    "    import sklearn\n",
    "    from operator import itemgetter\n",
    "\n",
    "    sklearn_funcs = next(GetSklearnPairwiseMetrics())\n",
    "    scipy_funcs = next(GetScipyPairwiseMetrics())\n",
    "    \n",
    "    yield pd.concat([sklearn_funcs, scipy_funcs])\n",
    "\n",
    "\n",
    "def ComputeWholePCA(pca: PCA,\n",
    "                    X,\n",
    "                    y = None,\n",
    "                    n_features: Union[int, float] = None\n",
    "                    ):\n",
    "#     if kwargs is None:\n",
    "#         kwargs = {}\n",
    "#     pca=PCA(**kwargs).fit(X)\n",
    "    setattr(pca, 'feature_ranks_', np.argsort(pca.explained_variance_)[:n_features])\n",
    "    setattr(pca, 'sorted_feature_names_in_',\n",
    "            pca.feature_names_in_[np.argsort(pca.explained_variance_)])\n",
    "    setattr(pca, 'feature_names_out_', pca.feature_names_in_[pca.feature_ranks_])\n",
    "\n",
    "    attribute_names = ['feature_ranks_',\n",
    "                       'explained_variance_', 'explained_variance_ratio_',\n",
    "                       'singular_values_', 'estimated_mean_',\n",
    "                       'log_likelihood_']\n",
    "\n",
    "    size = len(pca.feature_names_out_)\n",
    "    sorted_components = pd.DataFrame(pca.components_,\n",
    "                                     index=pca.sorted_feature_names_in_,\n",
    "                                     columns=pca.sorted_feature_names_in_)\n",
    "\n",
    "    attributes_ = pd.DataFrame((pca.feature_ranks_,\n",
    "                                pca.explained_variance_[pca.feature_ranks_],\n",
    "                                pca.explained_variance_ratio_[pca.feature_ranks_],\n",
    "                                pca.singular_values_[pca.feature_ranks_],\n",
    "                                pca.mean_[pca.feature_ranks_],\n",
    "                                pca.score_samples(sorted_components)[pca.feature_ranks_]),\n",
    "                               index=attribute_names,\n",
    "                               columns=pca.feature_names_out_)\n",
    "    \n",
    "    setattr(pca, 'attributes_', attributes_)\n",
    "    components_table_ = pd.DataFrame(pca.components_).iloc[pca.feature_ranks_,\n",
    "                                                          pca.feature_ranks_]\n",
    "    components_table_.set_axis(pca.feature_names_out_, axis=0, inplace=True)\n",
    "    components_table_.set_axis(pca.feature_names_out_, axis=1, inplace=True)\n",
    "    setattr(pca, 'components_table_', components_table_)\n",
    "\n",
    "    covariance_table_ = pd.DataFrame(pca.get_covariance()).iloc[pca.feature_ranks_,\n",
    "                                                                pca.feature_ranks_]\n",
    "    covariance_table_.set_axis(pca.feature_names_out_, axis=0, inplace=True)\n",
    "    covariance_table_.set_axis(pca.feature_names_out_, axis=1, inplace=True)\n",
    "    setattr(pca, 'covariance_table_', covariance_table_)\n",
    "\n",
    "    precision_table_ = pd.DataFrame(pca.get_precision()).iloc[pca.feature_ranks_,\n",
    "                                                              pca.feature_ranks_]\n",
    "    precision_table_.set_axis(pca.feature_names_out_, axis=0, inplace=True)\n",
    "    precision_table_.set_axis(pca.feature_names_out_, axis=1, inplace=True)\n",
    "    setattr(pca, 'precision_table_', precision_table_)\n",
    "\n",
    "    return pca\n",
    "#                                      index=pca.feature_names_out_,\n",
    "#                                   name='covariance_')\n",
    "#     return pd.concat([attributes, covariance_table_])\n",
    "\n",
    "\n",
    "def SortFeaturesByConditionPCA(X: Iterable, y: Iterable,\n",
    "                               method: str = 'pearson',\n",
    "                               n_features: Union[int, float] = 50\n",
    "                               ) -> Bunch:\n",
    "\n",
    "    pairwise_metrics = next(GetPairwiseMetrics())\n",
    "        \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    if isinstance(n_features, float):\n",
    "        n_features = int(round(X.shape[1] * n_features, 0))\n",
    "\n",
    "    X = X.set_axis(y,axis=0)\n",
    "    feature_names_in_ = X.columns\n",
    "    classes_, estimators_ = y.unique(), []\n",
    "\n",
    "    class_vectors = [X.loc[cond] for cond in classes_]\n",
    "    if method in pairwise_metrics:\n",
    "        method = pairwise_metrics[method]\n",
    "        class_vectors = [pd.DataFrame(method(vec.T),\n",
    "                                      index=feature_names_in_,\n",
    "                                      columns=feature_names_in_)\n",
    "                         for vec in class_vectors]\n",
    "        \n",
    "    else:\n",
    "        class_vectors = [vec.corr(method) for vec in class_vectors]\n",
    "    \n",
    "    [estimators_.append(PCA().fit(vec))\n",
    "     for vec in class_vectors]\n",
    "    \n",
    "    return [ComputeWholePCA(estimators_[vec[0]], X=vec[1],\n",
    "                            n_features=n_features)\n",
    "            for vec in enumerate(class_vectors)]\n",
    "\n",
    "\n",
    "def AgglomerateMulticollinearCorrelated(X, y,\n",
    "                                        n_clusters: int = 2,\n",
    "                                        get_connectivity: bool = True,\n",
    "                                        compute_distances=True,\n",
    "                                        kind: str = 'correlation',\n",
    "                                        agglo_kws: Union[dict, Bunch] = None\n",
    "                                        ):\n",
    "\n",
    "    from nilearn.connectome import ConnectivityMeasure as CM\n",
    "    from sklearn.cluster import FeatureAgglomeration\n",
    "    from sklearn.covariance import LedoitWolf\n",
    "\n",
    "    \n",
    "    agglo_defs = dict(affinity='euclidean',\n",
    "                      compute_full_tree='auto',\n",
    "                      linkage='ward',\n",
    "                      pooling_func=np.mean,\n",
    "                      distance_threshold=None,\n",
    "                      compute_distances=compute_distances)\n",
    "    \n",
    "    if get_connectivity is True:\n",
    "        connect_mat = CM(LedoitWolf(),\n",
    "                         kind='correlation').fit_transform([X.values])[0]\n",
    "    else:\n",
    "        connect_mat = None\n",
    "\n",
    "    if agglo_kws is None:\n",
    "        agglo_kws = {}\n",
    "    agglo_defs.update(agglo_kws)\n",
    "\n",
    "    agglo = FeatureAgglomeration(n_clusters=n_clusters,\n",
    "                                 connectivity=connect_mat,\n",
    "                                 **agglo_defs)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    agglo.fit(X, y)\n",
    "\n",
    "    rfe = RFE(estimator=PCA(), step=1,\n",
    "              n_features_to_select=1,\n",
    "              importance_getter='explained_variance_')\n",
    "\n",
    "\n",
    "    rfe.fit(X00[agglo00['Cs'].cluster_names_[164]], y=session00.tasks[2])\n",
    "    rfe.get_feature_names_out()\n",
    "    \n",
    "    setattr(agglo, 'classes_', y.unique())\n",
    "    setattr(agglo, 'feature_names_out_', agglo.feature_names_in_[agglo.labels_])\n",
    "    setattr(agglo, 'cluster_indexes_', pd.DataFrame(zip(agglo.labels_, agglo.feature_names_out_),\n",
    "                                   columns=['cluster_id',\n",
    "                                            'feature_name']).groupby('cluster_id').groups)\n",
    "    setattr(agglo, 'cluster_names_', dict(tuple((itm[0], agglo.feature_names_in_[itm[1]])\n",
    "                                                for itm in tuple(agglo.cluster_indexes_.items()))))\n",
    "\n",
    "    skb = SelectKBest(k=1, score_func='mutual_info_classif')\n",
    "    factor_leaders = [skb.fit(X[cluster_names], y).get_feature_names_out()\n",
    "                      for cluster_names in tuple(agglo.cluster_names_.values())]\n",
    "    \n",
    "    \n",
    "    setattr(agglo, 'new_features_', pd.concat([pd.Series(data=X.iloc[:, itm[1]].T.mean().values,\n",
    "                                        name=','.join(X.iloc[:, itm[1]].columns),\n",
    "                                        index=X.index)\n",
    "                              if len(itm[1]) > 1 else X.iloc[:, itm[1]]\n",
    "                              for itm in  tuple(agglo.cluster_indexes_.items())],\n",
    "                             axis=1))\n",
    "    \n",
    "    return agglo\n",
    "\n",
    "\n",
    "def RecurviseKBestLinearSVC(X: Iterable,\n",
    "                            y: Iterable,\n",
    "                            score_func: str = 'f_classif',\n",
    "                            step: Union[int, float] = 1\n",
    "                            ) -> pd.DataFrame:\n",
    "\n",
    "    from cimaq_decoding_utils import factorGenerator\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.feature_selection import f_classif, f_regression\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    from sklearn.feature_selection import mutual_info_regression\n",
    "    from sklearn.feature_selection import GenericUnivariateSelect\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    score_funcs = pd.Series([chi2,\n",
    "                             f_classif, f_regression,\n",
    "                             mutual_info_classif,\n",
    "                             mutual_info_regression,\n",
    "                             GenericUnivariateSelect],\n",
    "                            index=['chi2',\n",
    "                                   'f_classif', 'f_regression',\n",
    "                                   'mutual_info_classif',\n",
    "                                   'mutual_info_regression',\n",
    "                                   'GenericUnivariateSelect'])\n",
    "    \n",
    "    if isinstance(step, float):\n",
    "        step = int(round(divmod(X.shape[1], 2)[0] * step, 0))\n",
    "    X = X.set_axis(y, axis=0)\n",
    "    for n in tqdm_(range(step, divmod(X.shape[1], 2)[0], step)):\n",
    "        k = divmod(X.shape[1], 2)[0]-n\n",
    "        skb = SelectKBest(score_func=score_funcs.loc[score_func],\n",
    "                          k=k)\n",
    "        estimator_ = RFE(estimator=skb, step=step,\n",
    "                         n_features_to_select=k,\n",
    "                         importance_getter='scores_')\n",
    "\n",
    "        best_features = estimator_.fit(X=X, y=y).get_feature_names_out()\n",
    "\n",
    "        performance = round(validate_model(next(LinearSVCGen(\n",
    "                          **{\"class_weight\": \"balanced\"})),\n",
    "                                     X=X[best_features], y=y,\n",
    "                                     test_size=0.8).accuracy.mean(),\n",
    "                            2)\n",
    "\n",
    "        summary.append(({'n_features': X[best_features].shape[1],\n",
    "                         'selected': X[best_features],\n",
    "                         'performance': performance}))\n",
    "    results = pd.DataFrame(summary)\n",
    "    interval = (results.performance.max()*0.97,\n",
    "                results.performance.max()*1.03)\n",
    "    results = results.where(results.performance.between(*interval))\n",
    "    return results.dropna().reset_index(drop=True).iloc[-1:, :]\n",
    "\n",
    "\n",
    "def histogram_intersection(a, b):\n",
    "\n",
    "    v = np.minimum(a, b).sum().round(decimals=1)\n",
    "\n",
    "    return v\n",
    "\n",
    "def LinearSVCGen(**kwargs):\n",
    "    defs_kws = dict(max_iter=100000,\n",
    "                    class_weight='balanced')\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    defs_kws.update(kwargs)\n",
    "    yield LinearSVC(**defs_kws)\n",
    "\n",
    "    \n",
    "def get_corr_sign(positive: bool = True):\n",
    "    yield tuple(filter(lambda x: x[0],\n",
    "                           ((positive, pd.DataFrame.gt),\n",
    "                            (not positive, pd.DataFrame.lt))))[0][1]\n",
    "\n",
    "\n",
    "def trimmed_corr_mat(X, method: str = 'spearman',\n",
    "                     thresh: float = 0.9,\n",
    "                     positive: bool = True,\n",
    "                     ) -> pd.DataFrame:\n",
    "\n",
    "    corr_mat = X.corr(method)\n",
    "    corr_mat.where(corr_mat.values != np.triu(corr_mat.values),\n",
    "                   inplace=True)\n",
    "    eq_sign = next(get_corr_sign(thresh))\n",
    "  \n",
    "    corr_mat = corr_mat.dropna(how='all',\n",
    "                               axis=0).dropna(how='all', axis=1)\n",
    "    if thresh is not None:\n",
    "        corr_mat = corr_mat[eq_sign(corr_mat, thresh)]\n",
    "        corr_mat = corr_mat.dropna(how='all',\n",
    "                                   axis=0).dropna(how='all', axis=1)\n",
    "    return corr_mat\n",
    "\n",
    "\n",
    "def pairwise_correlates(X, method: str = 'spearman',\n",
    "                        thresh: float = 0.9,\n",
    "                        positive: bool = True\n",
    "                        ) -> pd.DataFrame:\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            new_names = list(trimmed_corr_mat(X, method=method,\n",
    "                                              thresh=thresh,\n",
    "                                              positive=positive\n",
    "                                              ).stack().index.tolist()[0])\n",
    "            new_feature = pd.Series(data=X[new_names].T.mean(),\n",
    "                                    name=','.join(new_names),\n",
    "                                    index=X.index)\n",
    "            X = pd.concat([new_feature, X.drop(new_names, axis=1)], axis=1)\n",
    "        except IndexError:\n",
    "            break\n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "def untangle(X: Iterable,\n",
    "             y: Iterable,\n",
    "             n_clusters: int = None,\n",
    "             get_connectivity: bool = True,\n",
    "             compute_distances: bool = True,\n",
    "             kind: str = 'correlation',\n",
    "             agglo_kws: Union[dict, Bunch] = None\n",
    "             ) -> FeatureAgglomeration:\n",
    "\n",
    "    from nilearn.connectome import ConnectivityMeasure as CM\n",
    "    from sklearn.cluster import FeatureAgglomeration\n",
    "    from sklearn.covariance import LedoitWolf\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "    agglo_defs = dict(affinity='euclidean',\n",
    "                      compute_full_tree='auto',\n",
    "                      linkage='ward',\n",
    "                      pooling_func=np.mean,\n",
    "                      distance_threshold=None,\n",
    "                      compute_distances=compute_distances)\n",
    "    \n",
    "    if get_connectivity is True:\n",
    "        connect_mat = CM(LedoitWolf(),\n",
    "                         kind=kind\n",
    "                         ).fit_transform([X.values])[0]\n",
    "    else:\n",
    "        connect_mat = None\n",
    "\n",
    "    if n_clusters is None:\n",
    "        n_clusters = divmod(X.shape[1], 2)[0] - 1\n",
    "        if n_clusters == 0:\n",
    "            n_clusters = 1\n",
    "\n",
    "    if agglo_kws is None:\n",
    "        agglo_kws = {}\n",
    "    agglo_defs.update(agglo_kws)\n",
    "\n",
    "    agglo = FeatureAgglomeration(n_clusters=n_clusters,\n",
    "                                 connectivity=connect_mat,\n",
    "                                 **agglo_defs)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    agglo.fit(X, y)\n",
    "\n",
    "    setattr(agglo, 'cluster_indexes_',\n",
    "            pd.DataFrame(zip(agglo.labels_,\n",
    "                             agglo.feature_names_in_),\n",
    "                         columns=['cluster', 'feature']\n",
    "                         ).groupby('cluster').feature)\n",
    "\n",
    "    skb = SelectKBest(k=1, score_func=mutual_info_classif)\n",
    "    factor_leaders_ = [skb.fit(X[itm[1]],\n",
    "                               y).get_feature_names_out()[0]\n",
    "                       for itm in tuple(agglo.cluster_indexes_)]\n",
    "    setattr(agglo, 'factor_leaders_', factor_leaders_)\n",
    "    return agglo\n",
    "\n",
    "\n",
    "from builtins import UserWarning\n",
    "\n",
    "def recursive_untangle(X: Iterable,\n",
    "                       y: Iterable,\n",
    "                       n_clusters: int = None,\n",
    "                       get_connectivity: bool = True,\n",
    "                       compute_distances: bool = True,\n",
    "                       kind: str = 'correlation',\n",
    "                       agglo_kws: Union[dict, Bunch] = None\n",
    "                       ) -> FeatureAgglomeration:\n",
    "#     while UserWarning:\n",
    "      while X.shape[1] > 8:\n",
    "#     half_size = divmod(X.shape[1], 2)[0]\n",
    "#     while X.shape[1] > half_size -1:\n",
    "        for n in range(X.shape[1] - 1):\n",
    "            try:\n",
    "                agg = (X[untangle(X, y,\n",
    "                                  n_clusters=n_clusters,\n",
    "                                  get_connectivity=get_connectivity,\n",
    "                                  compute_distances=compute_distances,\n",
    "                                  kind=kind,\n",
    "                                  agglo_kws=agglo_kws\n",
    "                                   ).factor_leaders_]\n",
    "                       for n in range(X.shape[1]))\n",
    "                next(agg)\n",
    "                X = X[agg.send(X).columns]\n",
    "            except (ValueError, UserWarning):\n",
    "                break\n",
    "        return X\n",
    "\n",
    "# def RecurviseKBestLinearSVC(X: Iterable,\n",
    "#                             y: Iterable,\n",
    "#                             score_func: str = 'f_classif',\n",
    "#                             step: Union[int, float] = 1\n",
    "#                             ) -> pd.DataFrame:\n",
    "\n",
    "#     from cimaq_decoding_utils import factorGenerator\n",
    "#     from sklearn.feature_selection import SelectKBest\n",
    "#     from sklearn.feature_selection import chi2\n",
    "#     from sklearn.feature_selection import f_classif, f_regression\n",
    "#     from sklearn.feature_selection import mutual_info_classif\n",
    "#     from sklearn.feature_selection import mutual_info_regression\n",
    "#     from sklearn.feature_selection import GenericUnivariateSelect\n",
    "#     from nilearn.connectome import ConnectivityMeasure as CM\n",
    "#     from sklearn.cluster import FeatureAgglomeration\n",
    "#     from sklearn.covariance import LedoitWolf\n",
    "#     from cimaq_decoding_utils import factorGenerator\n",
    "\n",
    "#     summary = []\n",
    "\n",
    "#     score_funcs = pd.Series([chi2,\n",
    "#                              f_classif, f_regression,\n",
    "#                              mutual_info_classif,\n",
    "#                              mutual_info_regression,\n",
    "#                              GenericUnivariateSelect],\n",
    "#                             index=['chi2',\n",
    "#                                    'f_classif', 'f_regression',\n",
    "#                                    'mutual_info_classif',\n",
    "#                                    'mutual_info_regression',\n",
    "#                                    'GenericUnivariateSelect'])\n",
    "    \n",
    "#     if isinstance(step, float):\n",
    "#         step = int(round(divmod(X.shape[1], 2)[0] * step, 0))\n",
    "#     X = X.set_axis(y, axis=0)\n",
    "#     for n in tqdm_(range(step, divmod(X.shape[1], 2)[0], step)):\n",
    "#         k = divmod(X.shape[1], 2)[0]-n\n",
    "#         skb = SelectKBest(score_func=score_funcs.loc[score_func],\n",
    "#                           k=k)\n",
    "#         estimator_ = RFECV(estimator=PCA(), step=1,\n",
    "#                          min_features_to_select=1,\n",
    "#                          importance_getter='explained_variance_ratio_')\n",
    "\n",
    "#         best_features = estimator_.fit(X=X, y=y).get_feature_names_out()\n",
    "\n",
    "#         performance = round(validate_model(next(LinearSVCGen(\n",
    "#                           **{\"class_weight\": \"balanced\"})),\n",
    "#                                      X=X[best_features], y=y,\n",
    "#                                      test_size=0.8).accuracy.mean(),\n",
    "#                             2)\n",
    "\n",
    "#         summary.append(({'n_features': X[best_features].shape[1],\n",
    "#                          'selected': X[best_features],\n",
    "#                          'performance': performance}))\n",
    "#     results = pd.DataFrame(summary)\n",
    "#     interval = (results.performance.max()*0.97,\n",
    "#                 results.performance.max()*1.03)\n",
    "#     results = results.where(results.performance.between(*interval))\n",
    "#     return results.dropna().reset_index(drop=True).iloc[-1:, :]\n",
    "\n",
    "# ComputeWholePCA(X00.corr('spearman'), session00.tasks[2], n_features=20).__dict__\n",
    "\n",
    "\n",
    "# method = 'cityblock'\n",
    "# pwm = next(GetPairwiseMetrics())\n",
    "# print(method in pwm)\n",
    "# method = pwm['cityblock']\n",
    "# method(X00).shape\n",
    "# dict(inspect.getmembers(sklearn.metrics.pairwise.distance_metrics))\n",
    "# callables_\n",
    "# testmean0=X00.iloc[:, np.array([0,44, 5])].T.mean()\n",
    "# testmean1=np.mean(X00.iloc[:, np.array([0,44, 5])].T.values)\n",
    "# display(testmean0, testmean1,\n",
    "#         np.mean(np.mean(X00.iloc[:, np.array([0,44, 5])].values, axis=1).T,\n",
    "#                 ),\n",
    "#         X00.iloc[:, 99].mean())\n",
    "\n",
    "\n",
    "import sklearn\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# FMRIPrepPathMatcher(sess5[0].fmri_path, **dict(events_dir=events_dir,\n",
    "#                                                behav_dir=events_dir)).__dir__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlases_dir = '/data/simexp/fnadeau/nilearn_atlases/difumo_atlases/'\n",
    "fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/'\n",
    "events_dir = '/data/simexp/fnadeau/CIMAQ_AS_BIDS_4/'\n",
    "masker_dir = '/data/simexp/fnadeau/cimaq_maps_maskers/'\n",
    "os.chdir('/data/simexp/fnadeau/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cimaq_decoding_utils import preprocess_events\n",
    "from cimaq_decoding_pipeline import get_fmri_sessions\n",
    "from cimaq_decoding_utils import get_sub_ses_key\n",
    "from cimaq_decoding_pipeline import fetch_fmriprep_session\n",
    "\n",
    "v03, v10 = [get_fmri_sessions(topdir=fmriprep_dir,\n",
    "                              events_dir=events_dir,\n",
    "#                               masker_dir=masker_dir,\n",
    "                              ses_id=ses)\n",
    "            for ses in [\"V03\", \"V10\"]]\n",
    "\n",
    "sess = v03+v10\n",
    "dst = '/data/simexp/fnadeau/cimaq_computed_data/'\n",
    "\n",
    "sessions = [ses for ses in sess\n",
    "            if bool(get_sub_ses_key(ses.fmri_path) not in\n",
    "                    list(map(get_sub_ses_key, os.listdir(dst))))]\n",
    "\n",
    "selected = sample(sessions, 5)\n",
    "\n",
    "sess5 = [fetch_fmriprep_session(session=ses)\n",
    "         for ses in tqdm_(selected)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/_utils/glm.py:309: UserWarning: Matrix is singular at working precision, regularizing...\n",
      "  warn('Matrix is singular at working precision, regularizing...')\n",
      "\n",
      "Computing Contrasts:   0%|                                                  | 0/117 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:   1%|▎                                         | 1/117 [00:00<00:53,  2.18it/s]\u001b[A\n",
      "Computing Contrasts:   2%|▋                                         | 2/117 [00:00<00:52,  2.19it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█                                         | 3/117 [00:01<00:51,  2.23it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█▍                                        | 4/117 [00:01<00:51,  2.19it/s]\u001b[A\n",
      "Computing Contrasts:   4%|█▊                                        | 5/117 [00:02<00:49,  2.24it/s]\u001b[A\n",
      "Computing Contrasts:   5%|██▏                                       | 6/117 [00:02<00:50,  2.21it/s]\u001b[A\n",
      "Computing Contrasts:   6%|██▌                                       | 7/117 [00:03<00:48,  2.27it/s]\u001b[A\n",
      "Computing Contrasts:   7%|██▊                                       | 8/117 [00:03<00:48,  2.24it/s]\u001b[A\n",
      "Computing Contrasts:   8%|███▏                                      | 9/117 [00:04<00:51,  2.11it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▌                                     | 10/117 [00:04<00:56,  1.89it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▊                                     | 11/117 [00:05<00:54,  1.95it/s]\u001b[A\n",
      "Computing Contrasts:  10%|████▏                                    | 12/117 [00:05<00:54,  1.94it/s]\u001b[A\n",
      "Computing Contrasts:  11%|████▌                                    | 13/117 [00:06<00:51,  2.02it/s]\u001b[A\n",
      "Computing Contrasts:  12%|████▉                                    | 14/117 [00:06<00:51,  1.99it/s]\u001b[A\n",
      "Computing Contrasts:  13%|█████▎                                   | 15/117 [00:07<00:50,  2.00it/s]\u001b[A\n",
      "Computing Contrasts:  14%|█████▌                                   | 16/117 [00:07<00:49,  2.03it/s]\u001b[A\n",
      "Computing Contrasts:  15%|█████▉                                   | 17/117 [00:08<00:48,  2.07it/s]\u001b[A\n",
      "Computing Contrasts:  15%|██████▎                                  | 18/117 [00:08<00:47,  2.08it/s]\u001b[A\n",
      "Computing Contrasts:  16%|██████▋                                  | 19/117 [00:09<00:46,  2.11it/s]\u001b[A\n",
      "Computing Contrasts:  17%|███████                                  | 20/117 [00:09<00:50,  1.93it/s]\u001b[A\n",
      "Computing Contrasts:  18%|███████▎                                 | 21/117 [00:10<00:46,  2.05it/s]\u001b[A\n",
      "Computing Contrasts:  19%|███████▋                                 | 22/117 [00:10<00:43,  2.19it/s]\u001b[A\n",
      "Computing Contrasts:  20%|████████                                 | 23/117 [00:10<00:41,  2.25it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▍                                | 24/117 [00:11<00:40,  2.30it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▊                                | 25/117 [00:11<00:39,  2.35it/s]\u001b[A\n",
      "Computing Contrasts:  22%|█████████                                | 26/117 [00:12<00:38,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  23%|█████████▍                               | 27/117 [00:12<00:37,  2.39it/s]\u001b[A\n",
      "Computing Contrasts:  24%|█████████▊                               | 28/117 [00:12<00:36,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  25%|██████████▏                              | 29/117 [00:13<00:36,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▌                              | 30/117 [00:13<00:35,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▊                              | 31/117 [00:14<00:35,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  27%|███████████▏                             | 32/117 [00:14<00:34,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  28%|███████████▌                             | 33/117 [00:15<00:34,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  29%|███████████▉                             | 34/117 [00:15<00:33,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  30%|████████████▎                            | 35/117 [00:15<00:33,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  31%|████████████▌                            | 36/117 [00:16<00:34,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  32%|████████████▉                            | 37/117 [00:16<00:33,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  32%|█████████████▎                           | 38/117 [00:17<00:32,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  33%|█████████████▋                           | 39/117 [00:17<00:31,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  34%|██████████████                           | 40/117 [00:17<00:30,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  35%|██████████████▎                          | 41/117 [00:18<00:29,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  36%|██████████████▋                          | 42/117 [00:18<00:29,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  37%|███████████████                          | 43/117 [00:19<00:29,  2.49it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▍                         | 44/117 [00:19<00:28,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▊                         | 45/117 [00:19<00:29,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  39%|████████████████                         | 46/117 [00:20<00:29,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  40%|████████████████▍                        | 47/117 [00:20<00:27,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  41%|████████████████▊                        | 48/117 [00:21<00:27,  2.52it/s]\u001b[A\n",
      "Computing Contrasts:  42%|█████████████████▏                       | 49/117 [00:21<00:26,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  43%|█████████████████▌                       | 50/117 [00:21<00:26,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  44%|█████████████████▊                       | 51/117 [00:22<00:25,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  44%|██████████████████▏                      | 52/117 [00:22<00:25,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  45%|██████████████████▌                      | 53/117 [00:22<00:24,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  46%|██████████████████▉                      | 54/117 [00:23<00:24,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  47%|███████████████████▎                     | 55/117 [00:23<00:25,  2.47it/s]\u001b[A\n",
      "Computing Contrasts:  48%|███████████████████▌                     | 56/117 [00:24<00:24,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  49%|███████████████████▉                     | 57/117 [00:24<00:24,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▎                    | 58/117 [00:25<00:23,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▋                    | 59/117 [00:25<00:22,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  51%|█████████████████████                    | 60/117 [00:25<00:22,  2.52it/s]\u001b[A\n",
      "Computing Contrasts:  52%|█████████████████████▍                   | 61/117 [00:26<00:22,  2.51it/s]\u001b[A\n",
      "Computing Contrasts:  53%|█████████████████████▋                   | 62/117 [00:26<00:21,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  54%|██████████████████████                   | 63/117 [00:26<00:21,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  55%|██████████████████████▍                  | 64/117 [00:27<00:21,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  56%|██████████████████████▊                  | 65/117 [00:27<00:20,  2.51it/s]\u001b[A\n",
      "Computing Contrasts:  56%|███████████████████████▏                 | 66/117 [00:28<00:20,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  57%|███████████████████████▍                 | 67/117 [00:28<00:19,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  58%|███████████████████████▊                 | 68/117 [00:28<00:19,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  59%|████████████████████████▏                | 69/117 [00:29<00:18,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  60%|████████████████████████▌                | 70/117 [00:29<00:19,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  61%|████████████████████████▉                | 71/117 [00:30<00:18,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▏               | 72/117 [00:30<00:17,  2.51it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▌               | 73/117 [00:30<00:17,  2.52it/s]\u001b[A\n",
      "Computing Contrasts:  63%|█████████████████████████▉               | 74/117 [00:31<00:16,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  64%|██████████████████████████▎              | 75/117 [00:31<00:16,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  65%|██████████████████████████▋              | 76/117 [00:32<00:15,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  66%|██████████████████████████▉              | 77/117 [00:32<00:15,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  67%|███████████████████████████▎             | 78/117 [00:33<00:16,  2.33it/s]\u001b[A\n",
      "Computing Contrasts:  68%|███████████████████████████▋             | 79/117 [00:33<00:15,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  68%|████████████████████████████             | 80/117 [00:33<00:15,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  69%|████████████████████████████▍            | 81/117 [00:34<00:14,  2.47it/s]\u001b[A\n",
      "Computing Contrasts:  70%|████████████████████████████▋            | 82/117 [00:34<00:14,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  71%|█████████████████████████████            | 83/117 [00:34<00:13,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  72%|█████████████████████████████▍           | 84/117 [00:35<00:12,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  73%|█████████████████████████████▊           | 85/117 [00:35<00:12,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▏          | 86/117 [00:36<00:12,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▍          | 87/117 [00:36<00:11,  2.57it/s]\u001b[A\n",
      "Computing Contrasts:  75%|██████████████████████████████▊          | 88/117 [00:36<00:11,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  76%|███████████████████████████████▏         | 89/117 [00:37<00:10,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  77%|███████████████████████████████▌         | 90/117 [00:37<00:10,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  78%|███████████████████████████████▉         | 91/117 [00:38<00:10,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▏        | 92/117 [00:38<00:09,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▌        | 93/117 [00:38<00:09,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  80%|████████████████████████████████▉        | 94/117 [00:39<00:08,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  81%|█████████████████████████████████▎       | 95/117 [00:39<00:08,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  82%|█████████████████████████████████▋       | 96/117 [00:39<00:08,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  83%|█████████████████████████████████▉       | 97/117 [00:40<00:07,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  84%|██████████████████████████████████▎      | 98/117 [00:40<00:07,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▋      | 99/117 [00:41<00:06,  2.64it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▏     | 100/117 [00:41<00:06,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  86%|██████████████████████████████████▌     | 101/117 [00:41<00:06,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  87%|██████████████████████████████████▊     | 102/117 [00:42<00:05,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  88%|███████████████████████████████████▏    | 103/117 [00:42<00:05,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  89%|███████████████████████████████████▌    | 104/117 [00:43<00:05,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  90%|███████████████████████████████████▉    | 105/117 [00:43<00:04,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▏   | 106/117 [00:43<00:04,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▌   | 107/117 [00:44<00:03,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  92%|████████████████████████████████████▉   | 108/117 [00:44<00:03,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:  93%|█████████████████████████████████████▎  | 109/117 [00:44<00:02,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  94%|█████████████████████████████████████▌  | 110/117 [00:45<00:02,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  95%|█████████████████████████████████████▉  | 111/117 [00:45<00:02,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  96%|██████████████████████████████████████▎ | 112/117 [00:46<00:01,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▋ | 113/117 [00:46<00:01,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▉ | 114/117 [00:46<00:01,  2.64it/s]\u001b[A\n",
      "Computing Contrasts:  98%|███████████████████████████████████████▎| 115/117 [00:47<00:00,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  99%|███████████████████████████████████████▋| 116/117 [00:47<00:00,  2.63it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████| 117/117 [00:48<00:00,  2.44it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 1/2 [00:00<00:00,  3.08it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  3.13it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  33%|██████████████▋                             | 1/3 [00:00<00:00,  3.25it/s]\u001b[A\n",
      "Computing Contrasts:  67%|█████████████████████████████▎              | 2/3 [00:00<00:00,  3.31it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  3.30it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  25%|███████████                                 | 1/4 [00:00<00:00,  3.20it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 2/4 [00:00<00:00,  3.29it/s]\u001b[A\n",
      "Computing Contrasts:  75%|█████████████████████████████████           | 3/4 [00:00<00:00,  3.29it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  3.24it/s]\u001b[A\n",
      " 20%|████████▊                                   | 1/5 [04:03<16:12, 243.04s/it]/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/_utils/glm.py:309: UserWarning: Matrix is singular at working precision, regularizing...\n",
      "  warn('Matrix is singular at working precision, regularizing...')\n",
      "\n",
      "Computing Contrasts:   0%|                                                  | 0/117 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:   1%|▎                                         | 1/117 [00:00<00:45,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:   2%|▋                                         | 2/117 [00:00<00:50,  2.29it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█                                         | 3/117 [00:01<00:46,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█▍                                        | 4/117 [00:01<00:45,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:   4%|█▊                                        | 5/117 [00:02<00:43,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:   5%|██▏                                       | 6/117 [00:02<00:42,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:   6%|██▌                                       | 7/117 [00:02<00:41,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:   7%|██▊                                       | 8/117 [00:03<00:40,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:   8%|███▏                                      | 9/117 [00:03<00:40,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▌                                     | 10/117 [00:03<00:40,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▊                                     | 11/117 [00:04<00:40,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  10%|████▏                                    | 12/117 [00:04<00:39,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  11%|████▌                                    | 13/117 [00:05<00:40,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  12%|████▉                                    | 14/117 [00:05<00:39,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  13%|█████▎                                   | 15/117 [00:05<00:39,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  14%|█████▌                                   | 16/117 [00:06<00:37,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:  15%|█████▉                                   | 17/117 [00:06<00:36,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  15%|██████▎                                  | 18/117 [00:06<00:35,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  16%|██████▋                                  | 19/117 [00:07<00:34,  2.85it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Contrasts:  17%|███████                                  | 20/117 [00:07<00:33,  2.91it/s]\u001b[A\n",
      "Computing Contrasts:  18%|███████▎                                 | 21/117 [00:07<00:32,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:  19%|███████▋                                 | 22/117 [00:08<00:32,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  20%|████████                                 | 23/117 [00:08<00:31,  2.99it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▍                                | 24/117 [00:08<00:31,  3.00it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▊                                | 25/117 [00:09<00:30,  3.01it/s]\u001b[A\n",
      "Computing Contrasts:  22%|█████████                                | 26/117 [00:09<00:30,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:  23%|█████████▍                               | 27/117 [00:09<00:29,  3.04it/s]\u001b[A\n",
      "Computing Contrasts:  24%|█████████▊                               | 28/117 [00:10<00:29,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  25%|██████████▏                              | 29/117 [00:10<00:29,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▌                              | 30/117 [00:10<00:28,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▊                              | 31/117 [00:11<00:29,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  27%|███████████▏                             | 32/117 [00:11<00:28,  2.96it/s]\u001b[A\n",
      "Computing Contrasts:  28%|███████████▌                             | 33/117 [00:11<00:28,  2.99it/s]\u001b[A\n",
      "Computing Contrasts:  29%|███████████▉                             | 34/117 [00:12<00:27,  2.98it/s]\u001b[A\n",
      "Computing Contrasts:  30%|████████████▎                            | 35/117 [00:12<00:27,  2.98it/s]\u001b[A\n",
      "Computing Contrasts:  31%|████████████▌                            | 36/117 [00:12<00:27,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  32%|████████████▉                            | 37/117 [00:13<00:26,  3.00it/s]\u001b[A\n",
      "Computing Contrasts:  32%|█████████████▎                           | 38/117 [00:13<00:26,  3.01it/s]\u001b[A\n",
      "Computing Contrasts:  33%|█████████████▋                           | 39/117 [00:13<00:25,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  34%|██████████████                           | 40/117 [00:14<00:25,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  35%|██████████████▎                          | 41/117 [00:14<00:28,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  36%|██████████████▋                          | 42/117 [00:15<00:32,  2.29it/s]\u001b[A\n",
      "Computing Contrasts:  37%|███████████████                          | 43/117 [00:15<00:30,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▍                         | 44/117 [00:15<00:28,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▊                         | 45/117 [00:16<00:28,  2.49it/s]\u001b[A\n",
      "Computing Contrasts:  39%|████████████████                         | 46/117 [00:16<00:27,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  40%|████████████████▍                        | 47/117 [00:17<00:28,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  41%|████████████████▊                        | 48/117 [00:17<00:27,  2.51it/s]\u001b[A\n",
      "Computing Contrasts:  42%|█████████████████▏                       | 49/117 [00:17<00:26,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  43%|█████████████████▌                       | 50/117 [00:18<00:26,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  44%|█████████████████▊                       | 51/117 [00:18<00:26,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  44%|██████████████████▏                      | 52/117 [00:19<00:25,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  45%|██████████████████▌                      | 53/117 [00:19<00:24,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  46%|██████████████████▉                      | 54/117 [00:19<00:24,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  47%|███████████████████▎                     | 55/117 [00:20<00:25,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  48%|███████████████████▌                     | 56/117 [00:20<00:25,  2.38it/s]\u001b[A\n",
      "Computing Contrasts:  49%|███████████████████▉                     | 57/117 [00:21<00:26,  2.31it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▎                    | 58/117 [00:21<00:25,  2.31it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▋                    | 59/117 [00:21<00:22,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  51%|█████████████████████                    | 60/117 [00:22<00:21,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  52%|█████████████████████▍                   | 61/117 [00:22<00:19,  2.83it/s]\u001b[A\n",
      "Computing Contrasts:  53%|█████████████████████▋                   | 62/117 [00:22<00:18,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  54%|██████████████████████                   | 63/117 [00:23<00:18,  2.96it/s]\u001b[A\n",
      "Computing Contrasts:  55%|██████████████████████▍                  | 64/117 [00:23<00:17,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:  56%|██████████████████████▊                  | 65/117 [00:23<00:17,  3.06it/s]\u001b[A\n",
      "Computing Contrasts:  56%|███████████████████████▏                 | 66/117 [00:24<00:16,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  57%|███████████████████████▍                 | 67/117 [00:24<00:16,  3.11it/s]\u001b[A\n",
      "Computing Contrasts:  58%|███████████████████████▊                 | 68/117 [00:24<00:15,  3.14it/s]\u001b[A\n",
      "Computing Contrasts:  59%|████████████████████████▏                | 69/117 [00:25<00:15,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:  60%|████████████████████████▌                | 70/117 [00:25<00:15,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  61%|████████████████████████▉                | 71/117 [00:25<00:14,  3.15it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▏               | 72/117 [00:26<00:14,  3.20it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▌               | 73/117 [00:26<00:13,  3.20it/s]\u001b[A\n",
      "Computing Contrasts:  63%|█████████████████████████▉               | 74/117 [00:26<00:13,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  64%|██████████████████████████▎              | 75/117 [00:27<00:13,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  65%|██████████████████████████▋              | 76/117 [00:27<00:14,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  66%|██████████████████████████▉              | 77/117 [00:27<00:13,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  67%|███████████████████████████▎             | 78/117 [00:28<00:13,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  68%|███████████████████████████▋             | 79/117 [00:28<00:12,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:  68%|████████████████████████████             | 80/117 [00:28<00:11,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  69%|████████████████████████████▍            | 81/117 [00:29<00:11,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  70%|████████████████████████████▋            | 82/117 [00:29<00:12,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  71%|█████████████████████████████            | 83/117 [00:29<00:11,  2.94it/s]\u001b[A\n",
      "Computing Contrasts:  72%|█████████████████████████████▍           | 84/117 [00:30<00:11,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  73%|█████████████████████████████▊           | 85/117 [00:30<00:10,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▏          | 86/117 [00:30<00:10,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▍          | 87/117 [00:31<00:10,  2.78it/s]\u001b[A\n",
      "Computing Contrasts:  75%|██████████████████████████████▊          | 88/117 [00:31<00:10,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  76%|███████████████████████████████▏         | 89/117 [00:31<00:09,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  77%|███████████████████████████████▌         | 90/117 [00:32<00:09,  2.82it/s]\u001b[A\n",
      "Computing Contrasts:  78%|███████████████████████████████▉         | 91/117 [00:32<00:09,  2.83it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▏        | 92/117 [00:32<00:08,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▌        | 93/117 [00:33<00:08,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  80%|████████████████████████████████▉        | 94/117 [00:33<00:08,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  81%|█████████████████████████████████▎       | 95/117 [00:33<00:07,  2.95it/s]\u001b[A\n",
      "Computing Contrasts:  82%|█████████████████████████████████▋       | 96/117 [00:34<00:07,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  83%|█████████████████████████████████▉       | 97/117 [00:34<00:07,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  84%|██████████████████████████████████▎      | 98/117 [00:35<00:06,  2.94it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▋      | 99/117 [00:35<00:05,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▏     | 100/117 [00:35<00:05,  3.07it/s]\u001b[A\n",
      "Computing Contrasts:  86%|██████████████████████████████████▌     | 101/117 [00:35<00:05,  3.14it/s]\u001b[A\n",
      "Computing Contrasts:  87%|██████████████████████████████████▊     | 102/117 [00:36<00:04,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:  88%|███████████████████████████████████▏    | 103/117 [00:36<00:04,  3.17it/s]\u001b[A\n",
      "Computing Contrasts:  89%|███████████████████████████████████▌    | 104/117 [00:36<00:04,  3.19it/s]\u001b[A\n",
      "Computing Contrasts:  90%|███████████████████████████████████▉    | 105/117 [00:37<00:03,  3.20it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▏   | 106/117 [00:37<00:03,  3.22it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▌   | 107/117 [00:37<00:03,  3.21it/s]\u001b[A\n",
      "Computing Contrasts:  92%|████████████████████████████████████▉   | 108/117 [00:38<00:02,  3.22it/s]\u001b[A\n",
      "Computing Contrasts:  93%|█████████████████████████████████████▎  | 109/117 [00:38<00:02,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  94%|█████████████████████████████████████▌  | 110/117 [00:38<00:02,  3.26it/s]\u001b[A\n",
      "Computing Contrasts:  95%|█████████████████████████████████████▉  | 111/117 [00:39<00:01,  3.27it/s]\u001b[A\n",
      "Computing Contrasts:  96%|██████████████████████████████████████▎ | 112/117 [00:39<00:01,  3.26it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▋ | 113/117 [00:39<00:01,  3.28it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▉ | 114/117 [00:39<00:00,  3.29it/s]\u001b[A\n",
      "Computing Contrasts:  98%|███████████████████████████████████████▎| 115/117 [00:40<00:00,  3.30it/s]\u001b[A\n",
      "Computing Contrasts:  99%|███████████████████████████████████████▋| 116/117 [00:40<00:00,  3.22it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████| 117/117 [00:40<00:00,  2.86it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 1/2 [00:00<00:00,  2.82it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  2.89it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  33%|██████████████▋                             | 1/3 [00:00<00:00,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  67%|█████████████████████████████▎              | 2/3 [00:00<00:00,  2.92it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 3/3 [00:01<00:00,  2.85it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  25%|███████████                                 | 1/4 [00:00<00:01,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 2/4 [00:00<00:00,  2.49it/s]\u001b[A\n",
      "Computing Contrasts:  75%|█████████████████████████████████           | 3/4 [00:01<00:00,  2.61it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  2.65it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 2/5 [08:11<12:18, 246.05s/it]/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/_utils/glm.py:309: UserWarning: Matrix is singular at working precision, regularizing...\n",
      "  warn('Matrix is singular at working precision, regularizing...')\n",
      "\n",
      "Computing Contrasts:   0%|                                                  | 0/117 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:   1%|▎                                         | 1/117 [00:00<00:39,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:   2%|▋                                         | 2/117 [00:00<00:41,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█                                         | 3/117 [00:01<00:38,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█▍                                        | 4/117 [00:01<00:37,  3.04it/s]\u001b[A\n",
      "Computing Contrasts:   4%|█▊                                        | 5/117 [00:01<00:36,  3.10it/s]\u001b[A\n",
      "Computing Contrasts:   5%|██▏                                       | 6/117 [00:01<00:35,  3.14it/s]\u001b[A\n",
      "Computing Contrasts:   6%|██▌                                       | 7/117 [00:02<00:34,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:   7%|██▊                                       | 8/117 [00:02<00:34,  3.15it/s]\u001b[A\n",
      "Computing Contrasts:   8%|███▏                                      | 9/117 [00:02<00:34,  3.17it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▌                                     | 10/117 [00:03<00:33,  3.19it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▊                                     | 11/117 [00:03<00:33,  3.19it/s]\u001b[A\n",
      "Computing Contrasts:  10%|████▏                                    | 12/117 [00:04<00:39,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  11%|████▌                                    | 13/117 [00:04<00:38,  2.69it/s]\u001b[A\n",
      "Computing Contrasts:  12%|████▉                                    | 14/117 [00:04<00:36,  2.82it/s]\u001b[A\n",
      "Computing Contrasts:  13%|█████▎                                   | 15/117 [00:05<00:35,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  14%|█████▌                                   | 16/117 [00:05<00:36,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  15%|█████▉                                   | 17/117 [00:05<00:35,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  15%|██████▎                                  | 18/117 [00:06<00:34,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  16%|██████▋                                  | 19/117 [00:06<00:33,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  17%|███████                                  | 20/117 [00:06<00:33,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:  18%|███████▎                                 | 21/117 [00:07<00:33,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  19%|███████▋                                 | 22/117 [00:07<00:33,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  20%|████████                                 | 23/117 [00:07<00:33,  2.78it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▍                                | 24/117 [00:08<00:32,  2.89it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▊                                | 25/117 [00:08<00:32,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  22%|█████████                                | 26/117 [00:08<00:31,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  23%|█████████▍                               | 27/117 [00:09<00:31,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  24%|█████████▊                               | 28/117 [00:09<00:30,  2.94it/s]\u001b[A\n",
      "Computing Contrasts:  25%|██████████▏                              | 29/117 [00:09<00:30,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▌                              | 30/117 [00:10<00:33,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▊                              | 31/117 [00:10<00:32,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  27%|███████████▏                             | 32/117 [00:11<00:31,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  28%|███████████▌                             | 33/117 [00:11<00:30,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  29%|███████████▉                             | 34/117 [00:11<00:29,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  30%|████████████▎                            | 35/117 [00:12<00:28,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  31%|████████████▌                            | 36/117 [00:12<00:27,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:  32%|████████████▉                            | 37/117 [00:12<00:27,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:  32%|█████████████▎                           | 38/117 [00:13<00:26,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:  33%|█████████████▋                           | 39/117 [00:13<00:26,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:  34%|██████████████                           | 40/117 [00:13<00:26,  2.93it/s]\u001b[A\n",
      "Computing Contrasts:  35%|██████████████▎                          | 41/117 [00:14<00:25,  2.96it/s]\u001b[A\n",
      "Computing Contrasts:  36%|██████████████▋                          | 42/117 [00:14<00:25,  2.98it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Contrasts:  37%|███████████████                          | 43/117 [00:14<00:24,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▍                         | 44/117 [00:15<00:24,  3.00it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▊                         | 45/117 [00:15<00:28,  2.54it/s]\u001b[A\n",
      "Computing Contrasts:  39%|████████████████                         | 46/117 [00:16<00:27,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  40%|████████████████▍                        | 47/117 [00:16<00:26,  2.69it/s]\u001b[A\n",
      "Computing Contrasts:  41%|████████████████▊                        | 48/117 [00:16<00:26,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  42%|█████████████████▏                       | 49/117 [00:17<00:26,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  43%|█████████████████▌                       | 50/117 [00:17<00:25,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  44%|█████████████████▊                       | 51/117 [00:18<00:25,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  44%|██████████████████▏                      | 52/117 [00:18<00:24,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:  45%|██████████████████▌                      | 53/117 [00:18<00:24,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  46%|██████████████████▉                      | 54/117 [00:19<00:24,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  47%|███████████████████▎                     | 55/117 [00:19<00:23,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  48%|███████████████████▌                     | 56/117 [00:19<00:23,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  49%|███████████████████▉                     | 57/117 [00:20<00:23,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▎                    | 58/117 [00:20<00:22,  2.64it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▋                    | 59/117 [00:21<00:21,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  51%|█████████████████████                    | 60/117 [00:21<00:21,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  52%|█████████████████████▍                   | 61/117 [00:21<00:20,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  53%|█████████████████████▋                   | 62/117 [00:22<00:21,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  54%|██████████████████████                   | 63/117 [00:22<00:19,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  55%|██████████████████████▍                  | 64/117 [00:22<00:18,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  56%|██████████████████████▊                  | 65/117 [00:23<00:17,  2.96it/s]\u001b[A\n",
      "Computing Contrasts:  56%|███████████████████████▏                 | 66/117 [00:23<00:16,  3.04it/s]\u001b[A\n",
      "Computing Contrasts:  57%|███████████████████████▍                 | 67/117 [00:23<00:16,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  58%|███████████████████████▊                 | 68/117 [00:24<00:15,  3.13it/s]\u001b[A\n",
      "Computing Contrasts:  59%|████████████████████████▏                | 69/117 [00:24<00:15,  3.17it/s]\u001b[A\n",
      "Computing Contrasts:  60%|████████████████████████▌                | 70/117 [00:24<00:14,  3.20it/s]\u001b[A\n",
      "Computing Contrasts:  61%|████████████████████████▉                | 71/117 [00:24<00:14,  3.25it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▏               | 72/117 [00:25<00:13,  3.25it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▌               | 73/117 [00:25<00:13,  3.26it/s]\u001b[A\n",
      "Computing Contrasts:  63%|█████████████████████████▉               | 74/117 [00:25<00:13,  3.26it/s]\u001b[A\n",
      "Computing Contrasts:  64%|██████████████████████████▎              | 75/117 [00:26<00:12,  3.25it/s]\u001b[A\n",
      "Computing Contrasts:  65%|██████████████████████████▋              | 76/117 [00:26<00:12,  3.26it/s]\u001b[A\n",
      "Computing Contrasts:  66%|██████████████████████████▉              | 77/117 [00:26<00:12,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  67%|███████████████████████████▎             | 78/117 [00:27<00:12,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  68%|███████████████████████████▋             | 79/117 [00:27<00:11,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  68%|████████████████████████████             | 80/117 [00:27<00:11,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  69%|████████████████████████████▍            | 81/117 [00:28<00:11,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  70%|████████████████████████████▋            | 82/117 [00:28<00:10,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  71%|█████████████████████████████            | 83/117 [00:28<00:10,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  72%|█████████████████████████████▍           | 84/117 [00:28<00:10,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  73%|█████████████████████████████▊           | 85/117 [00:29<00:09,  3.23it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▏          | 86/117 [00:29<00:09,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▍          | 87/117 [00:29<00:09,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  75%|██████████████████████████████▊          | 88/117 [00:30<00:09,  3.12it/s]\u001b[A\n",
      "Computing Contrasts:  76%|███████████████████████████████▏         | 89/117 [00:30<00:09,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  77%|███████████████████████████████▌         | 90/117 [00:31<00:11,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  78%|███████████████████████████████▉         | 91/117 [00:31<00:11,  2.32it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▏        | 92/117 [00:32<00:10,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▌        | 93/117 [00:32<00:09,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  80%|████████████████████████████████▉        | 94/117 [00:32<00:09,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  81%|█████████████████████████████████▎       | 95/117 [00:33<00:08,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  82%|█████████████████████████████████▋       | 96/117 [00:33<00:07,  2.83it/s]\u001b[A\n",
      "Computing Contrasts:  83%|█████████████████████████████████▉       | 97/117 [00:33<00:06,  2.91it/s]\u001b[A\n",
      "Computing Contrasts:  84%|██████████████████████████████████▎      | 98/117 [00:34<00:06,  3.04it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▋      | 99/117 [00:34<00:06,  2.91it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▏     | 100/117 [00:34<00:05,  2.99it/s]\u001b[A\n",
      "Computing Contrasts:  86%|██████████████████████████████████▌     | 101/117 [00:35<00:05,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:  87%|██████████████████████████████████▊     | 102/117 [00:35<00:04,  3.12it/s]\u001b[A\n",
      "Computing Contrasts:  88%|███████████████████████████████████▏    | 103/117 [00:35<00:04,  3.11it/s]\u001b[A\n",
      "Computing Contrasts:  89%|███████████████████████████████████▌    | 104/117 [00:36<00:04,  3.15it/s]\u001b[A\n",
      "Computing Contrasts:  90%|███████████████████████████████████▉    | 105/117 [00:36<00:03,  3.17it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▏   | 106/117 [00:36<00:03,  3.21it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▌   | 107/117 [00:36<00:03,  3.15it/s]\u001b[A\n",
      "Computing Contrasts:  92%|████████████████████████████████████▉   | 108/117 [00:37<00:02,  3.14it/s]\u001b[A\n",
      "Computing Contrasts:  93%|█████████████████████████████████████▎  | 109/117 [00:37<00:02,  3.14it/s]\u001b[A\n",
      "Computing Contrasts:  94%|█████████████████████████████████████▌  | 110/117 [00:37<00:02,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:  95%|█████████████████████████████████████▉  | 111/117 [00:38<00:01,  3.21it/s]\u001b[A\n",
      "Computing Contrasts:  96%|██████████████████████████████████████▎ | 112/117 [00:38<00:01,  3.09it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▋ | 113/117 [00:38<00:01,  3.08it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▉ | 114/117 [00:39<00:00,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:  98%|███████████████████████████████████████▎| 115/117 [00:39<00:00,  3.19it/s]\u001b[A\n",
      "Computing Contrasts:  99%|███████████████████████████████████████▋| 116/117 [00:39<00:00,  3.17it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████| 117/117 [00:40<00:00,  2.92it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 1/2 [00:00<00:00,  3.74it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  3.68it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  33%|██████████████▋                             | 1/3 [00:00<00:00,  3.43it/s]\u001b[A\n",
      "Computing Contrasts:  67%|█████████████████████████████▎              | 2/3 [00:00<00:00,  3.49it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  3.47it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  25%|███████████                                 | 1/4 [00:00<00:00,  3.11it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 2/4 [00:00<00:00,  3.16it/s]\u001b[A\n",
      "Computing Contrasts:  75%|█████████████████████████████████           | 3/4 [00:00<00:00,  3.24it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  3.22it/s]\u001b[A\n",
      " 60%|██████████████████████████▍                 | 3/5 [12:30<08:23, 251.93s/it]/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/_utils/glm.py:309: UserWarning: Matrix is singular at working precision, regularizing...\n",
      "  warn('Matrix is singular at working precision, regularizing...')\n",
      "\n",
      "Computing Contrasts:   0%|                                                  | 0/117 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:   1%|▎                                         | 1/117 [00:00<00:40,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:   2%|▋                                         | 2/117 [00:00<00:38,  2.98it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█                                         | 3/117 [00:01<00:38,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█▍                                        | 4/117 [00:01<00:39,  2.89it/s]\u001b[A\n",
      "Computing Contrasts:   4%|█▊                                        | 5/117 [00:01<00:38,  2.95it/s]\u001b[A\n",
      "Computing Contrasts:   5%|██▏                                       | 6/117 [00:02<00:37,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:   6%|██▌                                       | 7/117 [00:02<00:37,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:   7%|██▊                                       | 8/117 [00:02<00:37,  2.89it/s]\u001b[A\n",
      "Computing Contrasts:   8%|███▏                                      | 9/117 [00:03<00:37,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▌                                     | 10/117 [00:03<00:37,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▊                                     | 11/117 [00:03<00:36,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  10%|████▏                                    | 12/117 [00:04<00:36,  2.92it/s]\u001b[A\n",
      "Computing Contrasts:  11%|████▌                                    | 13/117 [00:04<00:35,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:  12%|████▉                                    | 14/117 [00:04<00:40,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  13%|█████▎                                   | 15/117 [00:05<00:38,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  14%|█████▌                                   | 16/117 [00:05<00:37,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  15%|█████▉                                   | 17/117 [00:05<00:35,  2.82it/s]\u001b[A\n",
      "Computing Contrasts:  15%|██████▎                                  | 18/117 [00:06<00:34,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  16%|██████▋                                  | 19/117 [00:06<00:35,  2.78it/s]\u001b[A\n",
      "Computing Contrasts:  17%|███████                                  | 20/117 [00:07<00:33,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  18%|███████▎                                 | 21/117 [00:07<00:33,  2.90it/s]\u001b[A\n",
      "Computing Contrasts:  19%|███████▋                                 | 22/117 [00:07<00:33,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  20%|████████                                 | 23/117 [00:08<00:32,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▍                                | 24/117 [00:08<00:32,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▊                                | 25/117 [00:08<00:31,  2.89it/s]\u001b[A\n",
      "Computing Contrasts:  22%|█████████                                | 26/117 [00:09<00:32,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  23%|█████████▍                               | 27/117 [00:09<00:31,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  24%|█████████▊                               | 28/117 [00:09<00:31,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  25%|██████████▏                              | 29/117 [00:10<00:31,  2.82it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▌                              | 30/117 [00:10<00:30,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▊                              | 31/117 [00:10<00:32,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  27%|███████████▏                             | 32/117 [00:11<00:32,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  28%|███████████▌                             | 33/117 [00:11<00:31,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  29%|███████████▉                             | 34/117 [00:12<00:30,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  30%|████████████▎                            | 35/117 [00:12<00:30,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  31%|████████████▌                            | 36/117 [00:12<00:29,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  32%|████████████▉                            | 37/117 [00:13<00:29,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  32%|█████████████▎                           | 38/117 [00:13<00:28,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  33%|█████████████▋                           | 39/117 [00:13<00:28,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  34%|██████████████                           | 40/117 [00:14<00:28,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  35%|██████████████▎                          | 41/117 [00:14<00:27,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  36%|██████████████▋                          | 42/117 [00:14<00:27,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  37%|███████████████                          | 43/117 [00:15<00:26,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▍                         | 44/117 [00:15<00:26,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▊                         | 45/117 [00:16<00:25,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  39%|████████████████                         | 46/117 [00:16<00:25,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  40%|████████████████▍                        | 47/117 [00:16<00:28,  2.47it/s]\u001b[A\n",
      "Computing Contrasts:  41%|████████████████▊                        | 48/117 [00:17<00:27,  2.53it/s]\u001b[A\n",
      "Computing Contrasts:  42%|█████████████████▏                       | 49/117 [00:17<00:26,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  43%|█████████████████▌                       | 50/117 [00:18<00:26,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  44%|█████████████████▊                       | 51/117 [00:18<00:25,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  44%|██████████████████▏                      | 52/117 [00:18<00:27,  2.34it/s]\u001b[A\n",
      "Computing Contrasts:  45%|██████████████████▌                      | 53/117 [00:19<00:26,  2.40it/s]\u001b[A\n",
      "Computing Contrasts:  46%|██████████████████▉                      | 54/117 [00:19<00:25,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  47%|███████████████████▎                     | 55/117 [00:20<00:25,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  48%|███████████████████▌                     | 56/117 [00:20<00:25,  2.38it/s]\u001b[A\n",
      "Computing Contrasts:  49%|███████████████████▉                     | 57/117 [00:21<00:24,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▎                    | 58/117 [00:21<00:23,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▋                    | 59/117 [00:21<00:23,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  51%|█████████████████████                    | 60/117 [00:22<00:22,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  52%|█████████████████████▍                   | 61/117 [00:22<00:22,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  53%|█████████████████████▋                   | 62/117 [00:22<00:21,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  54%|██████████████████████                   | 63/117 [00:23<00:20,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  55%|██████████████████████▍                  | 64/117 [00:23<00:20,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  56%|██████████████████████▊                  | 65/117 [00:24<00:19,  2.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Contrasts:  56%|███████████████████████▏                 | 66/117 [00:24<00:18,  2.69it/s]\u001b[A\n",
      "Computing Contrasts:  57%|███████████████████████▍                 | 67/117 [00:24<00:18,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  58%|███████████████████████▊                 | 68/117 [00:25<00:17,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  59%|████████████████████████▏                | 69/117 [00:25<00:17,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  60%|████████████████████████▌                | 70/117 [00:25<00:16,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  61%|████████████████████████▉                | 71/117 [00:26<00:16,  2.82it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▏               | 72/117 [00:26<00:15,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▌               | 73/117 [00:26<00:15,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  63%|█████████████████████████▉               | 74/117 [00:27<00:15,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  64%|██████████████████████████▎              | 75/117 [00:27<00:14,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  65%|██████████████████████████▋              | 76/117 [00:27<00:14,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  66%|██████████████████████████▉              | 77/117 [00:28<00:13,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  67%|███████████████████████████▎             | 78/117 [00:28<00:13,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  68%|███████████████████████████▋             | 79/117 [00:28<00:13,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  68%|████████████████████████████             | 80/117 [00:29<00:12,  2.88it/s]\u001b[A\n",
      "Computing Contrasts:  69%|████████████████████████████▍            | 81/117 [00:29<00:12,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  70%|████████████████████████████▋            | 82/117 [00:30<00:12,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  71%|█████████████████████████████            | 83/117 [00:30<00:11,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  72%|█████████████████████████████▍           | 84/117 [00:30<00:11,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  73%|█████████████████████████████▊           | 85/117 [00:31<00:11,  2.84it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▏          | 86/117 [00:31<00:10,  2.86it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▍          | 87/117 [00:31<00:10,  2.87it/s]\u001b[A\n",
      "Computing Contrasts:  75%|██████████████████████████████▊          | 88/117 [00:32<00:10,  2.85it/s]\u001b[A\n",
      "Computing Contrasts:  76%|███████████████████████████████▏         | 89/117 [00:32<00:10,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  77%|███████████████████████████████▌         | 90/117 [00:32<00:10,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  78%|███████████████████████████████▉         | 91/117 [00:33<00:09,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▏        | 92/117 [00:33<00:09,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▌        | 93/117 [00:34<00:08,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  80%|████████████████████████████████▉        | 94/117 [00:34<00:08,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  81%|█████████████████████████████████▎       | 95/117 [00:34<00:07,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  82%|█████████████████████████████████▋       | 96/117 [00:35<00:07,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  83%|█████████████████████████████████▉       | 97/117 [00:35<00:07,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  84%|██████████████████████████████████▎      | 98/117 [00:35<00:06,  2.83it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▋      | 99/117 [00:36<00:06,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▏     | 100/117 [00:36<00:06,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  86%|██████████████████████████████████▌     | 101/117 [00:36<00:05,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  87%|██████████████████████████████████▊     | 102/117 [00:37<00:05,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  88%|███████████████████████████████████▏    | 103/117 [00:37<00:05,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  89%|███████████████████████████████████▌    | 104/117 [00:37<00:04,  2.77it/s]\u001b[A\n",
      "Computing Contrasts:  90%|███████████████████████████████████▉    | 105/117 [00:38<00:04,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▏   | 106/117 [00:38<00:03,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▌   | 107/117 [00:39<00:03,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  92%|████████████████████████████████████▉   | 108/117 [00:39<00:03,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  93%|█████████████████████████████████████▎  | 109/117 [00:39<00:02,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  94%|█████████████████████████████████████▌  | 110/117 [00:40<00:02,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  95%|█████████████████████████████████████▉  | 111/117 [00:40<00:02,  2.80it/s]\u001b[A\n",
      "Computing Contrasts:  96%|██████████████████████████████████████▎ | 112/117 [00:40<00:01,  2.78it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▋ | 113/117 [00:41<00:01,  2.77it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▉ | 114/117 [00:41<00:01,  2.81it/s]\u001b[A\n",
      "Computing Contrasts:  98%|███████████████████████████████████████▎| 115/117 [00:41<00:00,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  99%|███████████████████████████████████████▋| 116/117 [00:42<00:00,  2.76it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████| 117/117 [00:42<00:00,  2.74it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 1/2 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  3.47it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  33%|██████████████▋                             | 1/3 [00:00<00:00,  3.24it/s]\u001b[A\n",
      "Computing Contrasts:  67%|█████████████████████████████▎              | 2/3 [00:00<00:00,  3.34it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 3/3 [00:00<00:00,  3.28it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  25%|███████████                                 | 1/4 [00:00<00:00,  3.68it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 2/4 [00:00<00:00,  3.51it/s]\u001b[A\n",
      "Computing Contrasts:  75%|█████████████████████████████████           | 3/4 [00:00<00:00,  3.41it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  3.48it/s]\u001b[A\n",
      " 80%|███████████████████████████████████▏        | 4/5 [16:38<04:10, 250.42s/it]/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/_utils/glm.py:309: UserWarning: Matrix is singular at working precision, regularizing...\n",
      "  warn('Matrix is singular at working precision, regularizing...')\n",
      "\n",
      "Computing Contrasts:   0%|                                                  | 0/117 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:   1%|▎                                         | 1/117 [00:00<00:43,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:   2%|▋                                         | 2/117 [00:00<00:41,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█                                         | 3/117 [00:01<00:40,  2.81it/s]\u001b[A\n",
      "Computing Contrasts:   3%|█▍                                        | 4/117 [00:01<00:39,  2.83it/s]\u001b[A\n",
      "Computing Contrasts:   4%|█▊                                        | 5/117 [00:01<00:38,  2.91it/s]\u001b[A\n",
      "Computing Contrasts:   5%|██▏                                       | 6/117 [00:02<00:37,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:   6%|██▌                                       | 7/117 [00:02<00:36,  3.00it/s]\u001b[A\n",
      "Computing Contrasts:   7%|██▊                                       | 8/117 [00:02<00:36,  3.01it/s]\u001b[A\n",
      "Computing Contrasts:   8%|███▏                                      | 9/117 [00:03<00:35,  3.02it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▌                                     | 10/117 [00:03<00:35,  3.03it/s]\u001b[A\n",
      "Computing Contrasts:   9%|███▊                                     | 11/117 [00:03<00:35,  2.99it/s]\u001b[A\n",
      "Computing Contrasts:  10%|████▏                                    | 12/117 [00:04<00:43,  2.42it/s]\u001b[A\n",
      "Computing Contrasts:  11%|████▌                                    | 13/117 [00:04<00:42,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  12%|████▉                                    | 14/117 [00:05<00:42,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  13%|█████▎                                   | 15/117 [00:05<00:41,  2.45it/s]\u001b[A\n",
      "Computing Contrasts:  14%|█████▌                                   | 16/117 [00:05<00:41,  2.46it/s]\u001b[A\n",
      "Computing Contrasts:  15%|█████▉                                   | 17/117 [00:06<00:41,  2.40it/s]\u001b[A\n",
      "Computing Contrasts:  15%|██████▎                                  | 18/117 [00:06<00:41,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  16%|██████▋                                  | 19/117 [00:07<00:40,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  17%|███████                                  | 20/117 [00:07<00:40,  2.39it/s]\u001b[A\n",
      "Computing Contrasts:  18%|███████▎                                 | 21/117 [00:08<00:41,  2.31it/s]\u001b[A\n",
      "Computing Contrasts:  19%|███████▋                                 | 22/117 [00:08<00:40,  2.36it/s]\u001b[A\n",
      "Computing Contrasts:  20%|████████                                 | 23/117 [00:08<00:39,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▍                                | 24/117 [00:09<00:39,  2.36it/s]\u001b[A\n",
      "Computing Contrasts:  21%|████████▊                                | 25/117 [00:09<00:39,  2.35it/s]\u001b[A\n",
      "Computing Contrasts:  22%|█████████                                | 26/117 [00:10<00:39,  2.30it/s]\u001b[A\n",
      "Computing Contrasts:  23%|█████████▍                               | 27/117 [00:10<00:37,  2.40it/s]\u001b[A\n",
      "Computing Contrasts:  24%|█████████▊                               | 28/117 [00:11<00:36,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  25%|██████████▏                              | 29/117 [00:11<00:35,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▌                              | 30/117 [00:11<00:34,  2.55it/s]\u001b[A\n",
      "Computing Contrasts:  26%|██████████▊                              | 31/117 [00:12<00:32,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  27%|███████████▏                             | 32/117 [00:12<00:31,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  28%|███████████▌                             | 33/117 [00:12<00:30,  2.79it/s]\u001b[A\n",
      "Computing Contrasts:  29%|███████████▉                             | 34/117 [00:13<00:30,  2.76it/s]\u001b[A\n",
      "Computing Contrasts:  30%|████████████▎                            | 35/117 [00:13<00:29,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  31%|████████████▌                            | 36/117 [00:13<00:29,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  32%|████████████▉                            | 37/117 [00:14<00:29,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  32%|█████████████▎                           | 38/117 [00:14<00:28,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  33%|█████████████▋                           | 39/117 [00:15<00:28,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  34%|██████████████                           | 40/117 [00:15<00:28,  2.72it/s]\u001b[A\n",
      "Computing Contrasts:  35%|██████████████▎                          | 41/117 [00:15<00:27,  2.72it/s]\u001b[A\n",
      "Computing Contrasts:  36%|██████████████▋                          | 42/117 [00:16<00:27,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  37%|███████████████                          | 43/117 [00:16<00:28,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▍                         | 44/117 [00:17<00:31,  2.35it/s]\u001b[A\n",
      "Computing Contrasts:  38%|███████████████▊                         | 45/117 [00:17<00:29,  2.42it/s]\u001b[A\n",
      "Computing Contrasts:  39%|████████████████                         | 46/117 [00:17<00:29,  2.44it/s]\u001b[A\n",
      "Computing Contrasts:  40%|████████████████▍                        | 47/117 [00:18<00:28,  2.43it/s]\u001b[A\n",
      "Computing Contrasts:  41%|████████████████▊                        | 48/117 [00:18<00:28,  2.39it/s]\u001b[A\n",
      "Computing Contrasts:  42%|█████████████████▏                       | 49/117 [00:19<00:29,  2.34it/s]\u001b[A\n",
      "Computing Contrasts:  43%|█████████████████▌                       | 50/117 [00:19<00:28,  2.35it/s]\u001b[A\n",
      "Computing Contrasts:  44%|█████████████████▊                       | 51/117 [00:19<00:28,  2.35it/s]\u001b[A\n",
      "Computing Contrasts:  44%|██████████████████▏                      | 52/117 [00:20<00:26,  2.42it/s]\u001b[A\n",
      "Computing Contrasts:  45%|██████████████████▌                      | 53/117 [00:20<00:27,  2.33it/s]\u001b[A\n",
      "Computing Contrasts:  46%|██████████████████▉                      | 54/117 [00:21<00:27,  2.33it/s]\u001b[A\n",
      "Computing Contrasts:  47%|███████████████████▎                     | 55/117 [00:21<00:26,  2.37it/s]\u001b[A\n",
      "Computing Contrasts:  48%|███████████████████▌                     | 56/117 [00:22<00:25,  2.39it/s]\u001b[A\n",
      "Computing Contrasts:  49%|███████████████████▉                     | 57/117 [00:22<00:24,  2.42it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▎                    | 58/117 [00:22<00:25,  2.34it/s]\u001b[A\n",
      "Computing Contrasts:  50%|████████████████████▋                    | 59/117 [00:23<00:24,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  51%|█████████████████████                    | 60/117 [00:23<00:22,  2.51it/s]\u001b[A\n",
      "Computing Contrasts:  52%|█████████████████████▍                   | 61/117 [00:24<00:21,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  53%|█████████████████████▋                   | 62/117 [00:24<00:21,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  54%|██████████████████████                   | 63/117 [00:24<00:20,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  55%|██████████████████████▍                  | 64/117 [00:25<00:19,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:  56%|██████████████████████▊                  | 65/117 [00:25<00:19,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  56%|███████████████████████▏                 | 66/117 [00:25<00:18,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  57%|███████████████████████▍                 | 67/117 [00:26<00:18,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  58%|███████████████████████▊                 | 68/117 [00:26<00:17,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  59%|████████████████████████▏                | 69/117 [00:26<00:17,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  60%|████████████████████████▌                | 70/117 [00:27<00:17,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  61%|████████████████████████▉                | 71/117 [00:27<00:16,  2.75it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▏               | 72/117 [00:28<00:16,  2.72it/s]\u001b[A\n",
      "Computing Contrasts:  62%|█████████████████████████▌               | 73/117 [00:28<00:16,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  63%|█████████████████████████▉               | 74/117 [00:28<00:15,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  64%|██████████████████████████▎              | 75/117 [00:29<00:15,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  65%|██████████████████████████▋              | 76/117 [00:29<00:14,  2.74it/s]\u001b[A\n",
      "Computing Contrasts:  66%|██████████████████████████▉              | 77/117 [00:29<00:15,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  67%|███████████████████████████▎             | 78/117 [00:30<00:16,  2.41it/s]\u001b[A\n",
      "Computing Contrasts:  68%|███████████████████████████▋             | 79/117 [00:30<00:15,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  68%|████████████████████████████             | 80/117 [00:31<00:14,  2.50it/s]\u001b[A\n",
      "Computing Contrasts:  69%|████████████████████████████▍            | 81/117 [00:31<00:14,  2.56it/s]\u001b[A\n",
      "Computing Contrasts:  70%|████████████████████████████▋            | 82/117 [00:31<00:13,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  71%|█████████████████████████████            | 83/117 [00:32<00:12,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  72%|█████████████████████████████▍           | 84/117 [00:32<00:12,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  73%|█████████████████████████████▊           | 85/117 [00:33<00:12,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▏          | 86/117 [00:33<00:11,  2.65it/s]\u001b[A\n",
      "Computing Contrasts:  74%|██████████████████████████████▍          | 87/117 [00:33<00:11,  2.64it/s]\u001b[A\n",
      "Computing Contrasts:  75%|██████████████████████████████▊          | 88/117 [00:34<00:10,  2.67it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing Contrasts:  76%|███████████████████████████████▏         | 89/117 [00:34<00:10,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  77%|███████████████████████████████▌         | 90/117 [00:34<00:10,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  78%|███████████████████████████████▉         | 91/117 [00:35<00:09,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▏        | 92/117 [00:35<00:09,  2.60it/s]\u001b[A\n",
      "Computing Contrasts:  79%|████████████████████████████████▌        | 93/117 [00:36<00:09,  2.59it/s]\u001b[A\n",
      "Computing Contrasts:  80%|████████████████████████████████▉        | 94/117 [00:36<00:08,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  81%|█████████████████████████████████▎       | 95/117 [00:36<00:08,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  82%|█████████████████████████████████▋       | 96/117 [00:37<00:08,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  83%|█████████████████████████████████▉       | 97/117 [00:37<00:07,  2.61it/s]\u001b[A\n",
      "Computing Contrasts:  84%|██████████████████████████████████▎      | 98/117 [00:38<00:07,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▋      | 99/117 [00:38<00:06,  2.62it/s]\u001b[A\n",
      "Computing Contrasts:  85%|██████████████████████████████████▏     | 100/117 [00:38<00:06,  2.63it/s]\u001b[A\n",
      "Computing Contrasts:  86%|██████████████████████████████████▌     | 101/117 [00:39<00:06,  2.66it/s]\u001b[A\n",
      "Computing Contrasts:  87%|██████████████████████████████████▊     | 102/117 [00:39<00:05,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  88%|███████████████████████████████████▏    | 103/117 [00:39<00:05,  2.73it/s]\u001b[A\n",
      "Computing Contrasts:  89%|███████████████████████████████████▌    | 104/117 [00:40<00:04,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  90%|███████████████████████████████████▉    | 105/117 [00:40<00:04,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▏   | 106/117 [00:40<00:04,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  91%|████████████████████████████████████▌   | 107/117 [00:41<00:03,  2.70it/s]\u001b[A\n",
      "Computing Contrasts:  92%|████████████████████████████████████▉   | 108/117 [00:41<00:03,  2.72it/s]\u001b[A\n",
      "Computing Contrasts:  93%|█████████████████████████████████████▎  | 109/117 [00:42<00:02,  2.71it/s]\u001b[A\n",
      "Computing Contrasts:  94%|█████████████████████████████████████▌  | 110/117 [00:42<00:02,  2.68it/s]\u001b[A\n",
      "Computing Contrasts:  95%|█████████████████████████████████████▉  | 111/117 [00:42<00:02,  2.72it/s]\u001b[A\n",
      "Computing Contrasts:  96%|██████████████████████████████████████▎ | 112/117 [00:43<00:01,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▋ | 113/117 [00:43<00:01,  2.67it/s]\u001b[A\n",
      "Computing Contrasts:  97%|██████████████████████████████████████▉ | 114/117 [00:43<00:01,  2.64it/s]\u001b[A\n",
      "Computing Contrasts:  98%|███████████████████████████████████████▎| 115/117 [00:44<00:00,  2.58it/s]\u001b[A\n",
      "Computing Contrasts:  99%|███████████████████████████████████████▋| 116/117 [00:44<00:00,  2.64it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████| 117/117 [00:45<00:00,  2.59it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 1/2 [00:00<00:00,  3.04it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 2/2 [00:00<00:00,  3.01it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  33%|██████████████▋                             | 1/3 [00:00<00:00,  2.97it/s]\u001b[A\n",
      "Computing Contrasts:  67%|█████████████████████████████▎              | 2/3 [00:00<00:00,  2.99it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 3/3 [00:01<00:00,  2.97it/s]\u001b[A\n",
      "\n",
      "Computing Contrasts:   0%|                                                    | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Computing Contrasts:  25%|███████████                                 | 1/4 [00:00<00:01,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  50%|██████████████████████                      | 2/4 [00:00<00:00,  2.48it/s]\u001b[A\n",
      "Computing Contrasts:  75%|█████████████████████████████████           | 3/4 [00:01<00:00,  2.48it/s]\u001b[A\n",
      "Computing Contrasts: 100%|████████████████████████████████████████████| 4/4 [00:01<00:00,  2.48it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████| 5/5 [20:53<00:00, 250.79s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool, Process\n",
    "\n",
    "\n",
    "# print(len(cortex_names))\n",
    "\n",
    "from cimaq_decoding_utils import save_masker\n",
    "from builtins import FutureWarning\n",
    "import warnings\n",
    "labels_path = '/data/simexp/fnadeau/cortex-difumo-693-labels.txt'\n",
    "cortex_labels = Path(labels_path).read_text().splitlines()\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# maps_masker = NiftiMapsMasker(maps_img=cortex_atlas,\n",
    "#                               mask_img=session.mask_img,\n",
    "#                               t_r=get_t_r(session.fmri_img),\n",
    "#                               resampling_target='mask',\n",
    "#                               **session.masker_defs).fit()\n",
    "# save_masker(dst=masker_dir, masker=maps_masker, session=session)\n",
    "\n",
    "\n",
    "\n",
    "trial_type_cols=['trial_type', 'recognition_performance', 'ctl_miss_ws_cs']\n",
    "\n",
    "[session.update(dict(computed_ = get_all_contrasts(fmri_img=nimage.smooth_img(session.fmri_img, 8),\n",
    "                               events=session.events,\n",
    "                               masker=session.masker,\n",
    "                               output_type='effect_size',\n",
    "                               trial_type_cols=trial_type_cols,\n",
    "                               standardize=True,\n",
    "                               scale=False,\n",
    "                               maximize=False,\n",
    "                               glm_kws=session.glm_defs,\n",
    "                               design_kws=session.design_defs,\n",
    "                               feature_labels=session.feature_labels)))\n",
    " for session in tqdm_(sess5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████▌                          | 2/5 [04:58<07:36, 152.12s/it]"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "dst = '/data/simexp/fnadeau/cimaq_computed_data/'\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "for ses in tqdm_(sess5):\n",
    "    name = '_'.join([ses.sub_id, ses.ses_id, ses.task,\n",
    "                     'computed-data.pickle'])\n",
    "    with open(os.path.join(dst, name), mode='wb') as cornichon:\n",
    "        pickle.dump(ses, cornichon, protocol=5)\n",
    "    cornichon.close()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = []\n",
    "for ses in sess5:\n",
    "    pca = PCA()\n",
    "    pca.fit(ses.computed_.signal_matrix.corr())\n",
    "    cols = pca.feature_names_in_[np.argsort(pca.explained_variance_)].tolist()\n",
    "    score_cols.append(cols)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['corpus callosum genu rh',\n",
       "  'caudate superior anterior',\n",
       "  'cuneus postero-inferior lh',\n",
       "  'pars opercularis lateral lh',\n",
       "  'dorsolateral prefrontal cortex lh',\n",
       "  'precuneus posterior',\n",
       "  'lingual gyrus middle rh',\n",
       "  'amygdala posterior',\n",
       "  'insula superior lh',\n",
       "  'parietal operculum medial rh',\n",
       "  'precuneus posterior lh',\n",
       "  'optic chiasm anterior',\n",
       "  'anterior orbital gyrus rh',\n",
       "  'anterior corona radiata superior lh',\n",
       "  'cerebrospinal fluid (between postcentral gyrus and skull)'],\n",
       " ['corpus callosum genu rh',\n",
       "  'caudate superior anterior',\n",
       "  'cerebellum ix',\n",
       "  'pars opercularis lateral lh',\n",
       "  'dorsolateral prefrontal cortex lh',\n",
       "  'precuneus posterior',\n",
       "  'lingual gyrus middle rh',\n",
       "  'amygdala posterior',\n",
       "  'insula superior lh',\n",
       "  'parietal operculum medial rh',\n",
       "  'precuneus posterior lh',\n",
       "  'optic chiasm anterior',\n",
       "  'anterior orbital gyrus rh',\n",
       "  'anterior corona radiata superior lh',\n",
       "  'cerebrospinal fluid (between postcentral gyrus and skull)'],\n",
       " ['corpus callosum genu rh',\n",
       "  'caudate superior anterior',\n",
       "  'cerebellum ix',\n",
       "  'pars opercularis lateral lh',\n",
       "  'dorsolateral prefrontal cortex lh',\n",
       "  'precuneus posterior',\n",
       "  'lingual gyrus middle rh',\n",
       "  'amygdala posterior',\n",
       "  'insula superior lh',\n",
       "  'parietal operculum medial rh',\n",
       "  'precuneus posterior lh',\n",
       "  'optic chiasm anterior',\n",
       "  'anterior orbital gyrus rh',\n",
       "  'anterior corona radiata superior lh',\n",
       "  'cerebrospinal fluid (between postcentral gyrus and skull)'],\n",
       " ['corpus callosum genu rh',\n",
       "  'caudate superior anterior',\n",
       "  'cerebellum ix',\n",
       "  'pars opercularis lateral lh',\n",
       "  'dorsolateral prefrontal cortex lh',\n",
       "  'precuneus posterior',\n",
       "  'lingual gyrus middle rh',\n",
       "  'amygdala posterior',\n",
       "  'insula superior lh',\n",
       "  'parietal operculum medial rh',\n",
       "  'precuneus posterior lh',\n",
       "  'optic chiasm anterior',\n",
       "  'anterior orbital gyrus rh',\n",
       "  'anterior corona radiata superior lh',\n",
       "  'cerebrospinal fluid (between postcentral gyrus and skull)'],\n",
       " ['corpus callosum genu rh',\n",
       "  'caudate superior anterior',\n",
       "  'cuneus postero-inferior lh',\n",
       "  'pars opercularis lateral lh',\n",
       "  'dorsolateral prefrontal cortex lh',\n",
       "  'precuneus posterior',\n",
       "  'lingual gyrus middle rh',\n",
       "  'amygdala posterior',\n",
       "  'insula superior lh',\n",
       "  'parietal operculum medial rh',\n",
       "  'precuneus posterior lh',\n",
       "  'optic chiasm anterior',\n",
       "  'anterior orbital gyrus rh',\n",
       "  'anterior corona radiata superior lh',\n",
       "  'cerebrospinal fluid (between postcentral gyrus and skull)']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sc[:15] for sc in score_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess5.keys()\n",
    "import pickle\n",
    "# help(pickle.load)\n",
    "from nilearn import datasets\n",
    "help(datasets.fetch_atlas_difumo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.40308973, -0.40333027,  0.3742795 , ...,  0.44217795,\n",
       "         0.572422  ,  0.4070244 ],\n",
       "       [-1.0280784 ,  0.9977893 , -0.97348243, ..., -1.0475073 ,\n",
       "        -0.9361223 , -1.0114284 ],\n",
       "       [ 0.4178063 , -0.40144676,  0.36165103, ...,  0.41062665,\n",
       "         0.5699006 ,  0.4056827 ],\n",
       "       ...,\n",
       "       [ 0.40492907, -0.40445405,  0.40498817, ...,  0.40297922,\n",
       "         0.57172173,  0.4046711 ],\n",
       "       [ 0.40492907, -0.40445405,  0.40498817, ...,  0.40297922,\n",
       "         0.57172173,  0.4046711 ],\n",
       "       [ 0.40492907, -0.40445405,  0.40498817, ...,  0.40297922,\n",
       "         0.57172173,  0.4046711 ]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cimaq_decoding_params import _params\n",
    "StandardScaler().fit_transform(sess5[1].computed_.weighted_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anat_img',\n",
       " 'anat_path',\n",
       " 'apply_defs',\n",
       " 'behav',\n",
       " 'behav_path',\n",
       " 'clean_defs',\n",
       " 'computed_',\n",
       " 'confounds',\n",
       " 'confounds_loader',\n",
       " 'confounds_strategy',\n",
       " 'design_defs',\n",
       " 'events',\n",
       " 'events_path',\n",
       " 'feature_labels',\n",
       " 'fmri_img',\n",
       " 'fmri_path',\n",
       " 'frame_times',\n",
       " 'get_frame_times',\n",
       " 'get_t_r',\n",
       " 'glm_defs',\n",
       " 'mask_img',\n",
       " 'mask_path',\n",
       " 'masker',\n",
       " 'masker_defs',\n",
       " 'masker_path',\n",
       " 'ses_id',\n",
       " 'smoothing_fwhm',\n",
       " 'space',\n",
       " 'sub_id',\n",
       " 't_r',\n",
       " 'task']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sess5[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse=False, categories='auto')#NoneX.index.tolist())\n",
    "# help(OneHotEncoder)\n",
    "ohe_encoded = ohe.fit_transform(np.array(X.index.tolist()).reshape(-1, 1))\n",
    "ohe_encoded\n",
    "# ohe_trans = ohe.transform(y.values.reshape(-1,1))\n",
    "# ohe_trans\n",
    "# help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Miss\n",
       "1        Ws\n",
       "2      Miss\n",
       "3        Cs\n",
       "4      Miss\n",
       "       ... \n",
       "112      Cs\n",
       "113      Cs\n",
       "114    Miss\n",
       "115      Cs\n",
       "116      Cs\n",
       "Name: ctl_miss_ws_cs, Length: 117, dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess5[0].events.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/pandas/core/internals/construction.py:540: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0.021768276479579792, -0.06940673021982627, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0.01755201481798901, 0.04948430157839832, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0.02170379823905372, -0.04131972278692515, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [[0.021768276479579792, -0.06940673021982627, ...\n",
       "1  [[0.01755201481798901, 0.04948430157839832, -0...\n",
       "2  [[0.02170379823905372, -0.04131972278692515, 0..."
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# help(MLPClassifier)\n",
    "ses1 = sample(sess5, 1)[0]\n",
    "X, y = ses1.computed_.signal_matrix, ses1.events.iloc[:, -1].values\n",
    "\n",
    "mlpc_params = dict(hidden_layer_sizes=(X.shape), activation='identity',\n",
    "                   solver='lbfgs', alpha=0.0001, batch_size='auto',\n",
    "                   learning_rate='constant', learning_rate_init=0.001,\n",
    "                   power_t=0.5, max_iter=200, shuffle=True,\n",
    "                   random_state=None, tol=0.0001, warm_start=False,\n",
    "                   momentum=0.9, nesterovs_momentum=True,\n",
    "                   early_stopping=False, validation_fraction=0.1,\n",
    "                   beta_1=0.9, beta_2=0.999, epsilon=1e-08,\n",
    "     \n",
    "                   n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# y = OneHotEncoder(sparse=False, categories='auto').fit_transform(y_base)\n",
    "# help(PLSRegression)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1, 1),\n",
    "                                                    stratify=y.reshape(-1, 1),\n",
    "                                                    test_size=0.8,\n",
    "                                                    random_state=4)\n",
    "\n",
    "\n",
    "y_train.shape, y_test.shape\n",
    "y_train, y_test = y_train.reshape(-1, 1), y_test.reshape(-1, 1)\n",
    "y_train.shape, y_test.shape\n",
    "\n",
    "mlpc = MLPClassifier(**mlpc_params)\n",
    "mlpc.fit(X_train, y_train)\n",
    "\n",
    "mlpc.__dict__ = Bunch(**mlpc.__dict__)\n",
    "\n",
    "\n",
    "    \n",
    "train_data = Bunch(**{'y_pred_train': mlpc.predict(X_train),\n",
    "                      'log_proba_train': mlpc.predict_log_proba(X_train),\n",
    "                      'proba_train': mlpc.predict_proba(X_train)})\n",
    "setattr(mlpc, 'training', train_data)\n",
    "# mlpc.__dict__.y_pred_train\n",
    "\n",
    "setattr(mlpc, 'test_score', mlpc.score(X_test, y_test))\n",
    "\n",
    "ses2 = sample([s for s in sess5\n",
    "               if s != ses1], 1)[0]\n",
    "\n",
    "X2, y2 = ses2.computed_.signal_matrix, ses.events.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "assert ses1 != ses2\n",
    "setattr(mlpc, 'test_score', mlpc.score(X_test, y_test))\n",
    "setattr(mlpc, 'unseen_test_score', mlpc.score(X2, y2))\n",
    "\n",
    "pd.DataFrame(mlpc.coefs_)\n",
    "# mlpc.__dict__.proba_train.shape, np.array(mlpc.coefs_).shape#,\n",
    "# mlpc.score(mlpc.__dict__.proba_train, y_train)\n",
    "#                       'train_score': mlpc.score(mlpc.predict(X_train), y_train)})\n",
    "#                       })\n",
    "\n",
    "# mlpc.__dict__.update(**{'y_pred_test': mlpc.predict(X_test),\n",
    "#                       'log_proba_test': mlpc.predict_log_proba(X_test),\n",
    "#                       'proba_test': mlpc.predict_proba(X_test)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15018/2729879642.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpca_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m multi_pca = [pca_list[mat[0]].fit(mat[1].corr('spearman')).transform(mat[1])\n\u001b[0m\u001b[1;32m     21\u001b[0m              for mat in enumerate(coef_weights)]\n",
      "\u001b[0;32m/tmp/ipykernel_15018/2729879642.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mpca_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m multi_pca = [pca_list[mat[0]].fit(mat[1].corr('spearman')).transform(mat[1])\n\u001b[0m\u001b[1;32m     21\u001b[0m              for mat in enumerate(coef_weights)]\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mitself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \"\"\"\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    428\u001b[0m             )\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         )\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"allow-nan\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    112\u001b[0m         ):\n\u001b[1;32m    113\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"infinity\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"NaN, infinity\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    115\u001b[0m                 msg_err.format(\n\u001b[1;32m    116\u001b[0m                     \u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "estimators = [next(LinearSVCGen())]*len(sess5)\n",
    "\n",
    "trial_type_cols=['trial_type', 'recognition_performance', 'ctl_miss_ws_cs']\n",
    "\n",
    "condition_coef_ = [[next(LinearSVCGen()).fit(ses[1].computed_.signal_matrix,\n",
    "                                             ses[1].events[col].values)\n",
    "                    for col in trial_type_cols]\n",
    "                   for ses in enumerate(sess5)]\n",
    "allcoefs = flatten(condition_coef_)\n",
    "\n",
    "coef_weights = [pd.DataFrame(est.coef_,\n",
    "                     columns=est.feature_names_in_[np.argsort(np.abs(est.coef_)).T])\n",
    "                for est in allcoefs]\n",
    "\n",
    "# [data[1].set_axis(condition_coef_[data[0]], axis=0, inplace=True)\n",
    "#  if data[1].shape[0] > 1 else data[1] for data in enumerate(coef_weights)]\n",
    "\n",
    "\n",
    "pca_list = [PCA()]*len(coef_weights)\n",
    "multi_pca = [pca_list[mat[0]].fit(mat[1].corr('spearman')).transform(mat[1])\n",
    "             for mat in enumerate(coef_weights)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33]),\n",
       " array([9.18407854e-03, 3.24485943e-03, 5.37142058e-04, 9.69072651e-33])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display([pca.explained_variance_ for pca in pca_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.mean([validate_model(next(LinearSVCGen()),\n",
    "                         X=ses.computed_.signal_matrix,\n",
    "                         y=ses.events[col],\n",
    "                         stratify=ses.events[col],\n",
    "                         test_size=0.8).accuracy.mean()\n",
    "  for ntimes in range(20)])\n",
    " for col in ['trial_type', \n",
    "             'recognition_performance',\n",
    "             'ctl_miss_ws_cs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but PLSRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 1 features, but PLSRegression is expecting 1024 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15018/2412308131.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mplsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# linsvc.fit(y_pred, y_train).coef_.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# pd.DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m    451\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_x_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    401\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 1 features, but PLSRegression is expecting 1024 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "X = sess5[1].computed_.signal_matrix\n",
    "y_base = sess5[1].events.iloc[:, -1].values.reshape(-1, 1)\n",
    "# y = OneHotEncoder(sparse=False, categories='auto').fit_transform(y_base)\n",
    "# help(PLSRegression)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_base,\n",
    "                                                    stratify=y_base,\n",
    "                                                    test_size=0.8,\n",
    "                                                    random_state=4)\n",
    "\n",
    "linsvc = next(LinearSVCGen())\n",
    "pca = PCA()\n",
    "\n",
    "linsvc.fit(X_train, y_train)\n",
    "pca.fit(X_train)\n",
    "\n",
    "# linsvc.coef_.shape, .shape\n",
    "# X_train.shape, pca.explained_variance_.shape\n",
    "\n",
    "plsr = PLSRegression(n_components=X_train.shape[0])\n",
    "\n",
    "plsr.fit(X_train, pca.explained_variance_) #pd.DataFrame(linsvc.coef_).mean().values.reshape(-1, 1))\n",
    "y_pred = plsr.predict(X_test)\n",
    "\n",
    "plsr.score(y_pred, linsvc\n",
    "# linsvc.fit(y_pred, y_train).coef_.shape\n",
    "# pd.DataFrame\n",
    "# wtf=pd.DataFrame(linsvc.coef_, columns=linsvc.feature_names_in_,\n",
    "#                   index=linsvc.classes_)#[np.argsort(linsvc.coef_.T)])\n",
    "# wtf\n",
    "# np.argsort(linsvc.coef_).shape\n",
    "# help(np.ravel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 1024), (90, 1))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.ravel(linsvc.coef_).shape\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Ctl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15018/381637374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mplsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLSRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mplsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         )\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Ctl'"
     ]
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    " \n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X = sess5[1].computed_.signal_matrix\n",
    "y_base = sess5[1].events.iloc[:, -1].values.reshape(-1, 1)\n",
    "# y = OneHotEncoder(sparse=False, categories='auto').fit_transform(y_base)\n",
    "# help(PLSRegression)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_base,\n",
    "                                                    stratify=y_base,\n",
    "                                                    test_size=0.8,\n",
    "                                                    random_state=4)\n",
    "\n",
    "\n",
    "\n",
    "plsr = PLSRegression(n_components=X.shape[1])\n",
    "\n",
    "plsr.fit(X_train, y_train)\n",
    "y_pred = plsr.predict(X_train)\n",
    "y_pred\n",
    "# plsr.__dict__\n",
    "# plsr.score(, X_train)\n",
    "# pca = PCA()\n",
    "\n",
    "# pca.fit(wtest[0].corr())\n",
    "# \n",
    "# display(pca.feature_names_in_.shape,\n",
    "#  pca.feature_names_in_[np.argsort(pca.explained_variance_ratio_)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di1024 = get_difumo(data_dir=atlases_dir,\n",
    "                    dimension=1024,\n",
    "                    resolution_mm=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # sorted(Path(os.getcwd()).rglob('difumo_atlases/'))\n",
    "# # help(Path().rglob)\n",
    "# # kwargs = dict(dimension=1024, resolution_mm=3)\n",
    "\n",
    "# # attempt = f'{kwargs['resolution_mm']}mm/'\n",
    "# sorted(sessions[0].keys())\n",
    "\n",
    "\n",
    "# # sorted(Path(os.getcwd()).rglob())\n",
    "help(NiftiMapsMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_maskers(sessions):\n",
    "#     maskers = [NiftiMapsMasker(maps_img=di1024.maps,\n",
    "#                                      mask_img=nimage.load_img(session.mask_path),\n",
    "#                                      resampling_target='maps',\n",
    "#                                      **{'standardize': False,\n",
    "#                                         'standardize_confounds': False,\n",
    "#                                         'high_variance_confounds': False,\n",
    "#                                         'smoothing_fwhm': None,\n",
    "#                                         'detrend': False,\n",
    "#                                         'dtype': 'f',\n",
    "#                                         'low_pass': None,\n",
    "#                                         'high_pass': None,\n",
    "#                                         'allow_overlap': True})\n",
    "#                for session in sessions]\n",
    "#     return tuple(map(NiftiMapsMasker.fit, maskers))\n",
    "\n",
    "# makser_list = make_maskers(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def MaskerGen(**kwargs):\n",
    "#     while True:\n",
    "#         yield NiftiMapsMasker(maps_img=di1024.maps,\n",
    "#                               mask_img=session.mask_img,\n",
    "#                               t_r=get_t_r(session.fmri_img),\n",
    "#                               resampling_target='mask',\n",
    "#                               **session.masker_defs).fit()\n",
    "\n",
    "# common_maps_masker = NiftiMapsMasker(maps_img=di1024.maps,\n",
    "#                                      mask_img=session.mask_img,\n",
    "#                                      resampling_target='maps',\n",
    "#                                      **{'standardize': False,\n",
    "#                                         'standardize_confounds': False,\n",
    "#                                         'high_variance_confounds': False,\n",
    "#                                         'smoothing_fwhm': None,\n",
    "#                                         'detrend': False,\n",
    "#                                         'dtype': 'f',\n",
    "#                                         'low_pass': None,\n",
    "#                                         'high_pass': None,\n",
    "#                                         'allow_overlap': True}).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session00, session01, session02 = [fetch_fmriprep_session(session=ses)\n",
    "#                                    for ses in sessions[:3]\n",
    "#                                    if ses.sub_id not in [s.sub_id for s in sessions2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(*sess5[0].computed_.weighted_matrices)\n",
    "len(os.listdir(dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(sess5[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[{'trial_type': 0.9906382978723406}],\n",
       "       [{'trial_type': 0.9914893617021278}],\n",
       "       [{'trial_type': 0.9914893617021278}],\n",
       "       [{'trial_type': 1.0}],\n",
       "       [{'trial_type': 0.9910638297872342}]], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_validation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 5/5 [23:20<00:00, 280.08s/it]\n"
     ]
    }
   ],
   "source": [
    "w_validation = pd.DataFrame([[{ses.computed_.weighted_matrices[n].index.name:\n",
    "      np.mean([validate_model(next(LinearSVCGen()),\n",
    "                              X=ses.computed_.weighted_matrices[n],\n",
    "                              y=ses.computed_.weighted_matrices[n].index,\n",
    "                              test_size=0.8,\n",
    "                              stratify=ses.computed_.weighted_matrices[n].index,\n",
    "                              ).accuracy.mean()\n",
    "         for x in range(25)])\n",
    " for n in range(3)}]\n",
    " for ses in tqdm_(sess5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([validate_model(next(LinearSVCGen()),\n",
    "                      X=ses.computed_.weighted_matrices[n],\n",
    "                      y=ses.computed_.weighted_matrices[n].index,\n",
    "                      test_size=0.8,\n",
    "                      stratify=ses.computed_.weighted_matrices[n].index,\n",
    "                      ).accuracy.mean()\n",
    " for x in range(25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(type(com_)).rsplit('.', maxsplit=1)[1]\n",
    "# dict(getmembers(com_))\n",
    "\n",
    "\n",
    "html_list\n",
    "# os.listdir(html_dir).__len__()\n",
    "# valid_html = [apath for apath in html_list\n",
    "#               if get_sub_ses_key(apath)[0] in to_validate]\n",
    "[apath for apath in html_list if\n",
    " os.path.basename(apath).split('.')[0]\n",
    " in [s.sub_id for s in sessions]].__len__()\n",
    "# valid_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "a, b, c = sess5[0].computed_.weighted_matrices\n",
    "\n",
    "vt = [VarianceThreshold()]*len([a, b, c])\n",
    "\n",
    "\n",
    "[vt[i[0]].fit(i[1]) for i in enumerate([a, b, c])]\n",
    "\n",
    "\n",
    "\n",
    "# display(*[pd.Series(vt[0].variances_,\n",
    "#                     index=vt[0].get_feature_names_out()).sort_values()[:20],\n",
    "#           pd.DataFrame(a).var().sort_values().iloc[:20]])\n",
    "from collections import Counter\n",
    "Counter(flatten([d.index.tolist() for d in [pd.Series(vt[0].variances_,\n",
    "                    index=vt[0].get_feature_names_out()).sort_values()[:20],\n",
    "          pd.DataFrame(a).var().sort_values().iloc[:20]]])).most_common()\n",
    "# vt.fit_transform(a).shape\n",
    "\n",
    "# pd.DataFrame(enumerate(tuple(zip(vt.get_feature_names_out(),))))\n",
    "    \n",
    "    \n",
    "# vt.var = pd.Series(data=vt.variances_, index=vt.get_feature_names_out())\n",
    "\n",
    "# vt.var\n",
    "# vt.variances_table_.where(vt.variances_table_>vt.variances_table_.quantile(0.25)).dropna(axis=0, how='all')\n",
    "#.sort_values('variances_')\n",
    "\n",
    "# pd.DataFrame(describe(X000))\n",
    "# sorted(describe(X000).__dir__())\n",
    "# from scipy.stats.stats import DescribeResult\n",
    "# help(np.argsort)\n",
    "# pca0 = PCA().fit(weighted_matrices[2].corr('spearman'))\n",
    "# pd.DataFrame(pca0.explained_variance_)\n",
    "# display(np.argsort(pca0.explained_variance_, axis=0),\n",
    "#         np.argsort(pca0.explained_variance_, axis=1))\n",
    "# from collections import namedtuple\n",
    "# namedtuple(describe(X000))\n",
    "# describe(X000)._asdict()['minmax']\n",
    "\n",
    "with open(os.getcwd(), mode='r') as pwd:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_data(X: Iterable\n",
    "                  ) -> pd.DataFrame:\n",
    "    from scipy.stats import describe\n",
    "    from scipy.stats.stats import DescribeResult\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    _fields = DescribeResult._fields[2:]\n",
    "    min_, max_ = _desc_.minmax\n",
    "    _desc_ = pd.DataFrame(([min_, max_] +\n",
    "                           list(describe(X)[2:])),\n",
    "                          index=(['min', 'max',] +\n",
    "                                 list(_fields)))\n",
    "    \n",
    "    desc_ = X.describe()\n",
    "\n",
    "    return _desc_, desc_\n",
    "\n",
    "display(describe_data(X000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sorted(sessions3[0].computed_.keys())\n",
    "\n",
    "# display(*sessions3[0].computed_.weighted_matrices,\n",
    "#         sessions3[0].computed_.whole.signals)\n",
    "from itertools import combinations\n",
    "# [mat.values[0] == mat.values[1] for mat in sessions3[0].computed_.weighted_matrices]\n",
    "# display(sessions3[0].computed_.weighted_matrices[].values ==\n",
    "#         sessions3[0].computed_.weighted_matrices[3].values)\n",
    "\n",
    "[itm[0].values == itm[1].values for itm in\n",
    " tuple(combinations(sessions3[0].computed_.matrices, r=2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(sessions3[0].tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cimaq_decoding_pipeline import weightings\n",
    "\n",
    "l0 = [sessions3[0].computed_[col].signals\n",
    "          for col in trial_type_cols]\n",
    "matrices = [sessions3[0].computed_.whole.signals.copy(deep=True).set_axis(sessions3[0].tasks[n])\n",
    "            for n in range(len(l0))]\n",
    "\n",
    "# display(*matrices)\n",
    "weighted_matrices = [weightings(matrices[task[0]], l0[task[0]])\n",
    "                     for task in enumerate(sessions3[0].tasks)]\n",
    "\n",
    "display(*weighted_matrices)\n",
    "\n",
    "\n",
    "\n",
    "# l1 = sessions3[0].computed_.weights\n",
    "# display(*[itm[0].values==itm[1].values\n",
    "#         for itm in tuple(zip(l0, l1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca0 = PCA().fit(weighted_matrices[2].corr('spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_matrices[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks_ = np.argsort(pca0.explained_variance_)\n",
    "pd.DataFrame(pca0.explained_variance_,\n",
    "             index=pca0.feature_names_in_[ranks_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pca0.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import describe\n",
    "from operator import attrgetter, itemgetter\n",
    "\n",
    "X000 = sessions3[0].computed_.trial_type.signals\n",
    "\n",
    "# display(X000.describe(), describe(X000))\n",
    "# PCA().fit()\n",
    "\n",
    "# describe(X000)._asdict()\n",
    "\n",
    "# sorted(dir(describe(X000)))\n",
    "# itemgetter(*describe(X000)._fields)\n",
    "# pd.DataFrame(describe(X000)[1:])\n",
    "\n",
    "# help(scipy.stats.stats.DescribeResult)\n",
    "scipy.stats.stats.DescribeResult._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions3 = [session00, session01, session02]\n",
    "\n",
    "[setattr(ses, 'tasks', [ses.events[col] for col in trial_type_cols])\n",
    " for ses in sessions3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[ses.computed_.signal_matrix for ses in sessions3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_maps_signals = [pd.DataFrame(next(MaskerGen(ses)).fit_transform(\n",
    "                           nimage.smooth_img(ses.contr, 8)),\n",
    "                                    columns=di1024.labels.difumo_names)\n",
    "                       for ses in tqdm_(sessions2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*common_maps_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal_paths = sorted(Path('/data/simexp/cimaq_extracted_signals/').iterdir())\n",
    "# trial_type_cols=['trial_type', 'recognition_performance', 'ctl_miss_ws_cs']\n",
    "\n",
    "# [ses.update({'signals_path':[apath for apath in signal_paths\n",
    "#                              if get_sub_ses_key(apath) ==\n",
    "#                              get_sub_ses_key(ses.fmri_path)][0]})\n",
    "#  for ses in sessions]\n",
    "\n",
    "# [ses.update({'signal_matrix': pd.read_csv(ses.signals_path, sep='\\t')})\n",
    "#  for ses in sessions]\n",
    "\n",
    "# [ses.update({'events': pd.read_csv(ses.events_path, sep='\\t')})\n",
    "#  for ses in sessions]\n",
    "\n",
    "# [session.update(Bunch(**dict(tasks=(session.events.trial_type,\n",
    "#                                     session.events.recognition_performance.replace({'Miss':'Fail'}),\n",
    "#                                     session.events.iloc[:, -1]))))\n",
    "#  for session in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [preprocess_events(session=ses).to_csv(ses.events_path, sep='\\t',\n",
    "#                                        encoding='UTF-8-SIG',\n",
    "#                                        index=True)\n",
    "#  for ses in tqdm_(sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{session00.sub_id}_{session00.ses_id}\\n',\n",
    "#       f'{session01.sub_id}_{session01.ses_id}\\n',\n",
    "#       f'{session02.sub_id}_{session02.ses_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foremidbrain_rois = ['thalam', 'pons', 'tegment', 'medulla', 'putamen',\n",
    "                     'fornix', 'cerebell', 'pedunc', 'tect', 'diencep',\n",
    "                     'striatum', 'fluid', 'colli', 'pars', 'nuc', 'reticu',\n",
    "                     'caud', 'ventricle', 'midbrain', 'mamm', 'fossa',\n",
    "                     'caps', 'gang', 'globus', 'claus', 'sinus']\n",
    "\n",
    "wm_rois = ['fasciculus', 'tract' 'forceps', 'callosum',\n",
    "           'cingulum', 'forceps', 'chiasm', 'corticospinal',\n",
    "           'radiation', 'radiata']\n",
    "\n",
    "\n",
    "no_cortex = foremidbrain_rois + wm_rois\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di693 = get_difumo(data_dir=atlases_dir, dimension=1024, resolution_mm=3)\n",
    "\n",
    "not_cortex = di693.labels.set_index('difumo_names').T.filter(regex='|'.join(no_cortex)).columns.tolist()\n",
    "\n",
    "# tmp = di693.labels.set_index('difumo_names').T\n",
    "cortex693_labels = di693.labels.set_index('difumo_names').T.drop(not_cortex, axis=1).T\n",
    "cortex693_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex_693_maps_indexes = di693.labels.reset_index(drop=False).set_index(\n",
    "                              'difumo_names', drop=False).loc[\n",
    "                                  cortex693_labels.index].component\n",
    "\n",
    "cortex_693_maps = nimage.index_img(di693.maps, cortex_693_maps_indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions[0].clean_defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_confounds\n",
    "from inspect import getmembers\n",
    "from nilearn import image as nimage\n",
    "from sklearn.utils import Bunch\n",
    "from pathlib import Path\n",
    "\n",
    "from cimaq_decoding_params import _params\n",
    "from cimaq_decoding_utils import get_t_r, get_frame_times\n",
    "from get_difumo import get_difumo\n",
    "\n",
    "strategy='Minimal'\n",
    "lc_kws={}\n",
    "loader = dict(getmembers(load_confounds))[f'{strategy}']\n",
    "loader\n",
    "# loader = [loader(**lc_kws) if lc_kws is not None\n",
    "#           else loader()][0]\n",
    "# conf = pd.DataFrame(loader.load(session['fmri_path']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(next(ConfLoaderGen(**{})).load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfLoaderGen(strategy: str = 'Minimal',\n",
    "                  **kwargs):\n",
    "    import load_confounds\n",
    "    from inspect import getmembers\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    ConfGen = (dict(getmembers(load_confounds))\n",
    "               [f'{strategy}'](kwargs))\n",
    "    while True:\n",
    "        yield ConfGen\n",
    "# sessions[0].fmri_path\n",
    "next(ConfLoaderGen(**{})).load(['/data/simexp/cimaq_preproc/fmriprep/sub-3025432/ses-V03/func/sub-3025432_ses-V03_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfLoaderGen(strategy: str = 'Minimal',\n",
    "                  **kwargs):\n",
    "\n",
    "    import load_confounds\n",
    "    from inspect import getmembers\n",
    "\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "    ConfGen = (dict(getmembers(load_confounds))\n",
    "               [f'{strategy}'](kwargs))\n",
    "    while True:\n",
    "        yield ConfGen\n",
    "\n",
    "\n",
    "def CleanImgGen(img,\n",
    "                strategy='Minimal',\n",
    "                mask_img=None,\n",
    "                confounds=None,\n",
    "                lc_kws=None,\n",
    "                **kwargs):\n",
    "\n",
    "\n",
    "    from nilearn import image as nimage\n",
    "    from sklearn.utils import Bunch\n",
    "    from pathlib import Path\n",
    "\n",
    "    from cimaq_decoding_params import _params\n",
    "    from cimaq_decoding_utils import get_t_r, get_frame_times\n",
    "    from get_difumo import get_difumo\n",
    "\n",
    "    if lc_kws is None:\n",
    "        lc_kws = {}\n",
    "    loader = next(ConfLoaderGen(strategy=strategy,\n",
    "                                **lc_kws))\n",
    "    conf = pd.DataFrame(loader.load(img))\n",
    "    return conf\n",
    "#     from nilearn.image import clean_img, load_img\n",
    "\n",
    "#     clean_defs = {'detrend': False,\n",
    "#                   'standardize': False,\n",
    "#                   'low_pass': None,\n",
    "#                   'high_pass': None,\n",
    "#                   'ensure_finite': True}\n",
    "#     if kwargs is None:\n",
    "#         kwargs = {}\n",
    "#     clean_defs.update(**kwargs)\n",
    "#     img=load_img(img)\n",
    "#     if mask_img is not None:\n",
    "#         mask_img=load_img(mask_img)\n",
    "\n",
    "#     return clean_img(img, mask_img=mask_img,\n",
    "#                      tr=get_t_r(img),\n",
    "#                      confounds=conf,\n",
    "#                      **clean_defs)\n",
    "\n",
    "CleanImgGen(img=sessions[0].fmri_path,\n",
    "            mask_img=sessions[0].mask_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimage.clean_img(fmri_img, confounds=conf,\n",
    "                                t_r=t_r, mask_img=mask_img,\n",
    "                                **_params.clean_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_signals = [common_maps_masker.fit_transform(nimage.smooth_img(nimage.load_img(sess.fmri_path)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions = [1024, 512, 256, 128, 64]\n",
    "\n",
    "# di1024, di512, di256, d1128, di64 = [get_difumo(dimension, 3, atlases_dir)\n",
    "#                                      for dimension in tqdm_(dimensions)]\n",
    "\n",
    "# atlases = [di1024, di512, di256, d1128, di64]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X00 = session00.computed_.signal_matrix.copy(deep=True)\n",
    "# X01 = session01.computed_.signal_matrix.copy(deep=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             try:\n",
    "                \n",
    "#             except UserWarning:\n",
    "#                 break\n",
    "\n",
    "# agg00 = untangle(X00, session00.tasks[2])\n",
    "# uX00 = X00[agg00.factor_leaders_]\n",
    "# agg_test = untangle(uX00, session00.tasks[2])\n",
    "# uX00b = X00[agg_test.factor_leaders_]\n",
    "# agg00b = untangle(uX00b, session00.tasks[2])\n",
    "# uX00c = X00[agg00b.factor_leaders_]\n",
    "# agg00c = untangle(uX00c, session00.tasks[2])\n",
    "# uX00d = X00[agg00c.factor_leaders_]\n",
    "# agg00d = untangle(uX00d, session00.tasks[2])\n",
    "# uX00e = X00[agg00d.factor_leaders_]\n",
    "# agg00e = untangle(uX00e, session00.tasks[2])\n",
    "# uX00f = X00[agg00e.factor_leaders_]\n",
    "# agg00f = untangle(uX00f, session00.tasks[2])\n",
    "# uX00g = X00[agg00f.factor_leaders_]\n",
    "# agg00g = untangle(uX00g, session00.tasks[2])\n",
    "## Proof of concept\n",
    "\n",
    "# shorter = pairwise_correlates(X)\n",
    "# newcols = [c for c in shorter.columns.tolist()\n",
    "#            if ',' in c]\n",
    "# len(','.join(newcols).split(',')) == len(set(','.join(newcols).split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session00.signal_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_big = pd.concat([ses.computed_.signal_matrix\n",
    "                   for ses in sessions3])\n",
    "[setattr(ses, 'tasks', [ses.events[col] for col in trial_type_cols])\n",
    " for ses in sessions3]\n",
    "tasks_big = [pd.concat([ses.tasks[task[0]] for ses in sessions3])\n",
    "             for task in enumerate(sessions3[0].tasks)]\n",
    "display(X_big.iloc[:divmod(X_big.shape[0], 8)[0],:].shape,\n",
    "        tasks_big[2].iloc[:divmod(X_big.shape[0], 8)[0]].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "[np.mean([validate_model(next(LinearSVCGen(**dict(max_iter=100000))),\n",
    "                sess.computed_.signal_matrix,\n",
    "                sess.tasks[2],\n",
    "                test_size=0.8,\n",
    "                stratify=sess.tasks[2]).accuracy.mean()\n",
    "          for n in tqdm_(range(20))])\n",
    " for sess in [session00, session01, session02]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_decoding_utils import factorGenerator\n",
    "list(factorGenerator(sessions[0].computed_.signal_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts = [recursive_untangle(sess.signal_matrix,\n",
    "                             sess.tasks[2],\n",
    "                             n_clusters=None)#divmod(sess.computed_.signal_matrix.shape[1], 2)[0] - 1)\n",
    "          for sess in tqdm_(sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorts[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "[np.mean([validate_model(next(LinearSVCGen(**dict(max_iter=100000))),\n",
    "                shorts[sess[0]],\n",
    "                sess[1].tasks[2],\n",
    "                test_size=0.8,\n",
    "                stratify=sess[1].tasks[2]).accuracy.mean()\n",
    "          for n in tqdm_(range(20))])\n",
    " for sess in enumerate([session00, session01, session02])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_short = recursive_untangle(X_big, tasks_big[2], n_clusters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(group_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[setattr(ses, 'masker', next(MaskerGen(ses))) for ses in tqdm_(sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sess.update({'smooth_fmri': nimage.smooth_img(\n",
    "    sess.masker.inverse_transform(\n",
    "        sess.signal_matrix.set_index('trial_type')), 8)})\n",
    " for sess in tqdm_(sessions)\n",
    " if 'V03' in sess.fmri_path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(flatten(short.columns.tolist()\n",
    "                   for short in shorts)).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(next(LinearSVCGen(max_iter=100000)),\n",
    "                            X_big.set_axis(tasks_big[2],\n",
    "                                           axis=0).iloc[:divmod(X_big.shape[0], 8)[0],:],\n",
    "                            tasks_big[2].iloc[:divmod(X_big.shape[0], 8)[0]],\n",
    "                            test_size=0.8,\n",
    "                            stratify=tasks_big[2].iloc[:divmod(X_big.shape[0], 8)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_big = \\\n",
    "    np.mean([validate_model(next(LinearSVCGen(max_iter=1000000)),\n",
    "                            X_big.set_axis(tasks_big[2], axis=0),\n",
    "                            tasks_big[2],\n",
    "                            test_size=0.8,\n",
    "                            stratify=tasks_big[2]).accuracy.mean()\n",
    "                       for n in tqdm_(range(20))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "[ses.update({'reduced_signals':\n",
    "             recursive_untangle(ses.signal_matrix,\n",
    "                                ses.tasks[2]).set_index('trial_type')})\n",
    " for ses in tqdm_(sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ses.reduced_signals.set_index('trial_type', inplace=True)\n",
    " for ses in sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(flatten((ses.reduced_signals.set_index('trial_type').columns.tolist()\n",
    "                 for ses in sessions))).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import UserWarning\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "scores = [np.mean([validate_model(next(LinearSVCGen()),\n",
    "                                  ses.reduced_signals,\n",
    "                                  ses.tasks[2], test_size=0.8,\n",
    "                                  stratify=ses.tasks[2]).accuracy.mean()\n",
    "                   for n in range(20)])\n",
    "          for ses in tqdm_(sessions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(flatten(tst.columns.tolist() for tst in test)).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_decoding_pipeline import weightings\n",
    "\n",
    "X00cp = X00.copy(deep=True)\n",
    "X00cp.set_axis(session00.tasks[2]).loc['Ctl'] = X00cp.set_axis(session00.tasks[2]).loc['Ctl']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linsvc00.decision_table_\n",
    "support_vector_indices = list(set(list(np.where(np.abs(linsvc00.decision_table_) <= 1 + 1e-15)[0])))\n",
    "support_vectors = X00.iloc[[ind for ind in range(X00.shape[0])\n",
    "                            if ind not in np.array(support_vector_indices)]]\n",
    "tst_vecs = support_vectors.abs().mean().sort_values(ascending=False).head(40).index\n",
    "sel_ = results01[-2]['selected'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(next(LinearSVCGen()), X01, session01.tasks[2],\n",
    "               test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(next(LinearSVCGen()).fit(X00, session00.tasks[2]).densify().coef_.round(5)[0] ==\n",
    " next(LinearSVCGen()).fit(X00, session00.tasks[2]).coef_.round(5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvc00 = next(LinearSVCGen()).fit(X00, session00.tasks[2])\n",
    "\n",
    "linsvc00.coef_table_ = pd.DataFrame(linsvc00.coef_,\n",
    "                                    index=linsvc00.classes_,\n",
    "                                    columns=linsvc00.feature_names_in_)\n",
    "\n",
    "linsvc00.decision_table_ = pd.DataFrame(linsvc00.decision_function(X00),\n",
    "                                        columns=linsvc00.classes_,\n",
    "                                        index=linsvc00.predict(X00))\n",
    "\n",
    "\n",
    "display(linsvc00.coef_table_, linsvc00.coef_)\n",
    "\n",
    "# linsvc01 = next(LinearSVCGen())\n",
    "\n",
    "# linsvc01.fit(linsvc00.coef_table_,\n",
    "#              linsvc00.coef_table_.index.values)\n",
    "\n",
    "# linsvc01.score(X00, session00.tasks[2])\n",
    "# linsvc01.coef_table_ = pd.DataFrame(linsvc01.coef_, index=linsvc01.classes_,\n",
    "#                                    columns=linsvc01.feature_names_in_)\n",
    "\n",
    "# linsvc01.decision_table_ = pd.DataFrame(linsvc01.decision_function(X),\n",
    "#                                        columns=linsvc01.classes_,\n",
    "#                                        index=linsvc01.predict(X))\n",
    "\n",
    "# display(linsvc01.score(X, tasks[1]),\n",
    "#         [row[1][row[1]==row[1].max()]\n",
    "#          for row in linsvc01.coef_table_.iterrows()],\n",
    "#         linsvc01.decision_table_)\n",
    "\n",
    "\n",
    "# len(list(filter(None, linsvc01.predict(X)==tasks[1].values)))/len(tasks[1].values)\n",
    "# linsvc00.coef_table_\n",
    "# display(linsvc00.score(X,  linsvc00.predict(X)),\n",
    "#         linsvc00.decision_table_, linsvc00.coef_table_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linsvc00.coef_.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X00.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def f_importances(estimator,\n",
    "                  X: Iterable,\n",
    "                  y: Iterable,\n",
    "                  importance_getter: Union[str, callable] ='coef_',\n",
    "                  names=None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from operator import attrgetter\n",
    "    \n",
    "    estimator.fit(X, y)\n",
    "    if hasattr(estimator, 'feature_names_in_'):\n",
    "        feature_names_in_\n",
    "        if names is None:\n",
    "        names = coef.columns\n",
    "#     coef, names = zip(*list(zip(abs(coef),\n",
    "#                                 names[np.argsort(coef)])))\n",
    "    return names[np.argsort(coef.abs())]\n",
    "#     coef, names = coef[:25], names[:25]\n",
    "#     plt.barh(range(len(names)), coef,\n",
    "#              align='center')\n",
    "#     plt.yticks(range(len(names)), names)\n",
    "#     plt.show()\n",
    "#     return pd.DataFrame(zip(coef, names))\n",
    "    \n",
    "test_coef = f_importances(next(LinearSVCGen()),\n",
    "                          X00,\n",
    "                          session00.tasks[2])\n",
    "test_coef[0]\n",
    "#               names=linsvc00.coef_table_.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (linsvc00.coef_table_.loc['Cs'].abs().mul(100).round(4)).sort_values()# - \n",
    "#  linsvc00.coef_table_.loc['Ctl'].abs().mul(100).round(4)).sort_values(ascending=False)\n",
    "subset_ = linsvc00.coef_table_.mean().abs().sort_values(ascending=False).head(30).index\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best10 = [linsvc00.coef_table_.pow(2).T.sort_values(col, ascending=False).index.tolist()[\n",
    "    :divmod(linsvc00.coef_table_.shape[1], 8)[0]\n",
    "]\n",
    " for col in linsvc00.coef_table_.pow(2).T.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flatten(best10))\n",
    "col_set = list(set(flatten(best10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[(row[1].index==cond) for row in results00['selected'].iloc[-1].T.iterrows()]\n",
    "          for cond in results00['selected'].iloc[-1].index.unique()],\n",
    "         dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "ohe_ = pd.DataFrame(OneHotEncoder(sparse=False).fit_transform(session00.tasks[2].values.reshape(-1, 1)))\n",
    "\n",
    "# linsvc00b = LinearSVC(max_iter=10000, class_weight='balanced')\n",
    "# linsvc00b.fit(results00['selected'].iloc[-1],\n",
    "#               session00.tasks[2].values)\n",
    "\n",
    "logreg00 = LogisticRegression(max_iter=10000, multi_class='ovr', n_jobs=11)\n",
    "logreg00.fit(results00['selected'].iloc[-1],\n",
    "             session00.tasks[2])\n",
    "logreg00.coef_table_ = pd.DataFrame(logreg00.coef_,\n",
    "                                    index=logreg00.classes_,\n",
    "                                    columns=logreg00.feature_names_in_)\n",
    "logreg00.predict(results00['selected'].iloc[-1])\n",
    "logreg00.score(results00['selected'].iloc[-1],\n",
    "               session00.tasks[2])\n",
    "\n",
    "\n",
    "importances = pd.DataFrame(data=permutation_importance(logreg00,\n",
    "                                                       results00['selected'].iloc[-1],\n",
    "                                                       session00.tasks[2],\n",
    "                       scoring='accuracy', n_jobs=11).importances,\n",
    "                        index=logreg00.feature_names_in_)\n",
    "importances\n",
    "# logreg00.predict_proba(results00['selected'].iloc[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X00.set_axis(session00.tasks[2], axis=0).drop('Ctl', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session00.tasks[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = X00.set_axis(session00.tasks[2], axis=0).drop('Ctl', axis=0)\n",
    "y = X00.set_axis(session00.tasks[2], axis=0).drop('Ctl', axis=0).index\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.8,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42)\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "plotted = result.importances[perm_sorted_idx].T\n",
    "plotted_labels = X.columns[perm_sorted_idx]\n",
    "\n",
    "plotted.shape\n",
    "\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "# ax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "# ax1.set_yticks(tree_indices)\n",
    "# ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n",
    "# ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "\n",
    "# ax2.boxplot(plotted, vert=False, labels=plotted_labels)\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_cols = [[col[1] for col in enumerate(X.columns) if col[0] == idx][0]\n",
    "                for idx in perm_sorted_idx]\n",
    "\n",
    "X_permutation = X[ordered_cols].iloc[:, :30]\n",
    "X_subset = X[subset_]\n",
    "\n",
    "\n",
    "[col for col in X_subset.columns\n",
    " if col in X_permutation.columns\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "corr = spearmanr(X).correlation\n",
    "\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "# dendro = hierarchy.dendrogram(dist_linkage,\n",
    "#                               labels=X.columns.tolist(),\n",
    "#                               ax=ax1, leaf_rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "X_train_sel = X_train[:, selected_features]\n",
    "X_test_sel = X_test[:, selected_features]\n",
    "\n",
    "clf_sel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_sel.fit(X_train_sel, y_train)\n",
    "print(\n",
    "    \"Accuracy on test data with features removed: {:.2f}\".format(\n",
    "        clf_sel.score(X_test_sel, y_test)\n",
    "    )\n",
    ")\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_svc00 = pd.DataFrame(permutation_importance(linsvc00, X00,\n",
    "                                                  session00.tasks[2],\n",
    "                                                  scoring='accuracy',\n",
    "                                                  n_jobs=11).importances,\n",
    "                        index=linsvc00.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance(linsvc00, X=X00,\n",
    "                                                  y=session00.tasks[2],\n",
    "                                                  scoring='accuracy',\n",
    "                                                  n_jobs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session00.new_roi_indexes = [[col[0] for col in enumerate(X00.columns)\n",
    "                              if col[1] in newcol.split(',')]\n",
    "                             for newcol in new_features.columns]\n",
    "session00.new_maps_img = [nimage.mean_img(nimage.index_img(session00.masker._resampled_maps_img_, idx))\n",
    "                          for idx in session00.new_roi_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session00.new_features = new_features\n",
    "\n",
    "session00.new_masker = NiftiMapsMasker(maps_img=session00.new_maps_img,\n",
    "                                       mask_img=session00.mask_img,\n",
    "                                       t_r=get_t_r(session00.fmri_img),\n",
    "                                       resampling_target='mask',\n",
    "                                       **session00.masker_defs).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session00.new_fmri_img = session00.new_masker.inverse_transform(session00.new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgs = {}\n",
    "[new_imgs.update(itm) for itm in\n",
    " flatten([[{cond: session00.new_masker.inverse_transform(session00.new_features.set_axis(\n",
    "               session00.tasks[task[0]], 0).loc[cond])}\n",
    "                     for cond in session00.tasks[task[0]].unique()]\n",
    "                    for task in enumerate(session00.tasks)])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_imgs = {}\n",
    "[init_imgs.update(itm) for itm in\n",
    " flatten([[{cond: session00.masker.inverse_transform(session00.computed_.signal_matrix.set_axis(\n",
    "               session00.tasks[task[0]], 0).loc[cond])}\n",
    "                     for cond in session00.tasks[task[0]].unique()]\n",
    "                    for task in enumerate(session00.tasks)])]\n",
    "# niplot.plot_connectome(session00.new_features.corr('spearman'),\n",
    "#                        niplot.find_probabilistic_atlas_cut_coords(session00.new_fmri_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session00.computed_.signal_matrix.max().max()\n",
    "sum(np.array([1,2,3,4]), np.array([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[niplot.plot_glass_brain(nimage.mean_img(itm[1]),\n",
    "                         title=f'Before RFE & Clustering: {itm[0]}',\n",
    "                         plot_abs=False,\n",
    "                         colorbar=True,\n",
    "                         black_bg=True)\n",
    " for itm in tuple(init_imgs.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[niplot.plot_glass_brain(nimage.mean_img(itm[1]),\n",
    "                         title=f'After RFE & Clustering: {itm[0]}',\n",
    "                         plot_abs=False,\n",
    "                         colorbar=True,\n",
    "                         black_bg=True)\n",
    " for itm in tuple(new_imgs.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colset = set(flatten([sorted(row[1].sort_values(ascending=False).iloc[:3].index.tolist())\n",
    " for row in linsvc00.coef_table_.pow(2).iterrows()]))\n",
    "# linsvc00.coef_table_.pow(2)\n",
    "display(len(colset),\n",
    "        round(np.mean([validate_model(next(LinearSVCGen()),\n",
    "                                      new_features,#X_subset,#X_permutation,#results00['selected'].iloc[-1],#.filter(regex='cingulate|hippo|occip'),#X01[colset],\n",
    "                                      session00.tasks[2],\n",
    "                                      stratify=session00.tasks[2],\n",
    "                                      test_size=0.8).accuracy.round(2).mean()\n",
    "         for n in tqdm_(range(30))]), 2),\n",
    "        new_features#results00['selected'].iloc[-1]#.filter(regex='cingulate|hippo|occip'),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {cond:linsvc2.coef_table_[linsvc2.coef_table_==linsvc2.coef_table_.loc[cond].max()].loc[cond].dropna()\n",
    "#  for cond in linsvc2.coef_table_.index}\n",
    "from cimaq_decoding_utils import factorGenerator\n",
    "list(factorGenerator(X00.shape[1])), list(factorGenerator(X01.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_decoding_utils import factorGenerator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFdr, SelectFpr\n",
    "from sklearn.feature_selection import SelectFwe, SelectPercentile\n",
    "\n",
    "\n",
    "score_funcs = pd.Series([chi2, SelectFdr, SelectFpr,\n",
    "                     SelectFwe, SelectKBest,\n",
    "                     f_classif, f_regression,\n",
    "                     mutual_info_classif,\n",
    "                     mutual_info_regression,\n",
    "                     GenericUnivariateSelect],\n",
    "                    index=['chi2', 'SelectFdr',\n",
    "                           'SelectFpr', 'SelectFwe',\n",
    "                           'SelectKBest', 'f_classif',\n",
    "                           'f_regression',\n",
    "                           'mutual_info_classif',\n",
    "                           'mutual_info_regression',\n",
    "                           'GenericUnivariateSelect'])\n",
    "\n",
    "score_funcs.loc['GenericUnivariateSelect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pca_new.feature_names_in_, key=np.argsort(pca_new.explained_variance_))\n",
    "# help(sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X00[agg00g.factor_leaders]\n",
    "['supramarginal gyrus postero-superior lh',\n",
    " 'subparietal sulcus anterior rh',\n",
    " 'postcentral gyrus superior lh']\n",
    "['middle frontal gyrus posterior rh',\n",
    " 'superior frontal sulcus anterior lh',\n",
    " 'superior temporal gyrus anterior medial']\n",
    "['lateral occipital cortex postero-inferior rh',\n",
    " 'angular gyrus middle rh',\n",
    " 'cingulate sulcus posterior rh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (session00.tasks[2].tolist()+session01.tasks[2].tolist()).__len__()\n",
    "# X_big = pd.concat([X00[agg00f.factor_leaders],\n",
    "#                    X01[agg00f.factor_leaders]])\n",
    "X_big = pd.concat([X00, X01])\n",
    "y_big = np.array(session00.tasks[2].tolist()+session01.tasks[2].tolist())\n",
    "X_big.shape, y_big.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([validate_model(next(LinearSVCGen()),\n",
    "                        X=X_big[['insula inferior rh',\n",
    "                                 'superior temporal sulcus posterior rh',\n",
    "                                 'frontomarginal gyrus rh',\n",
    "                                 'lateral occipital cortex postero-inferior rh',\n",
    "                                 'angular gyrus middle rh',\n",
    "                                 'cingulate sulcus posterior rh',\n",
    "                                 'middle frontal gyrus posterior rh',\n",
    "                                 'superior frontal sulcus anterior lh',\n",
    "                                 'superior temporal gyrus anterior medial']],\n",
    "#                                  'supramarginal gyrus postero-superior lh',\n",
    "#                                  'subparietal sulcus anterior rh',\n",
    "#                                  'postcentral gyrus superior lh']],\n",
    "#                X01[['lateral occipital cortex postero-inferior rh',\n",
    "#                     'angular gyrus middle rh',\n",
    "#                     'cingulate sulcus posterior rh']],\n",
    "                        y=y_big,\n",
    "                        test_size=0.8,\n",
    "                        stratify=y_big\n",
    "                       ).accuracy.mean()\n",
    "  for n in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import explained_variance_score\n",
    "# y_true = session00.tasks[2]\n",
    "# linsvc = LinearSVC(max_iter=100000,\n",
    "#                    class_weight='balanced')\n",
    "# linsvc.fit(X00, y_true)\n",
    "\n",
    "# RFECV(next(LinearSVCGen(**dict(max_iter=100000,\n",
    "#                                class_weight='balanced'))),\n",
    "#       min_features_to_select=X00.shape[1]-1,\n",
    "#       importance_getter='coef_',\n",
    "#       cv=StratifiedKFold(),\n",
    "#       n_jobs=11).fit(X00, y_true).support_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, RFE\n",
    "from sklearn.linear_model import LinearSVC\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from typing import Iterable, Union\n",
    "\n",
    "def RFECVPCASVCKBest(X: Iterable,\n",
    "                     y: Iterable,\n",
    "                     step: Union[int, float] = 1,\n",
    "                     importance_getter: Unioin[str, callable] = 'coef_'\n",
    "                     **kwargs\n",
    "                     ) -> np.array:\n",
    "    from cimaq_decoding_utils import validate_model\n",
    "    linsvc = LinearSVC(max_iter=10000, class_weight='balanced')\n",
    "    linsvc.fit(X, y)\n",
    "    coef_table_ = pd.DataFrame(linsvc.coef_,\n",
    "                               index=linsvc.classes_\n",
    "                               columns=linsvc.feature_names_in_)\n",
    "    explained_variance_ = \n",
    "    \n",
    "SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_mat = CM(LedoitWolf(),\n",
    "                 kind='correlation',\n",
    "                 discard_diagonal=True).fit_transform([X.values])[0]\n",
    "connect_mat = pd.DataFrame(connect_mat).where(connect_mat!=np.tril(connect_mat), 0)\n",
    "connect_mat.min()\n",
    "# connect_mat.shape, sns.heatmap(connect_mat)\n",
    "# pd.DataFrame()#.describe()\n",
    "# connect_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(factorGenerator(X00.shape[1]))\n",
    "\n",
    "# range(steps[0], steps[-1])\n",
    "connect_mat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "agglo_defs = dict(affinity='euclidean',\n",
    "                  compute_full_tree='auto',\n",
    "                  linkage='ward',\n",
    "                  pooling_func=np.mean,\n",
    "                  distance_threshold=None,\n",
    "                  compute_distances=True)\n",
    "\n",
    "connect_mat = CM(LedoitWolf(),\n",
    "                 kind='covariance').fit_transform([X.values])[0]\n",
    "agg = FeatureAgglomeration(n_clusters=divmod(X00.shape[1], 4)[0],#list(factorGenerator(X00.shape[1]))[-1]-6,\n",
    "                     connectivity=connect_mat,\n",
    "                     **agglo_defs)\n",
    "\n",
    "rfe = RFE(estimator=PCA(), step=1,\n",
    "              n_features_to_select=1,\n",
    "              importance_getter='explained_variance_ratio_')\n",
    "\n",
    "rfe2 = RFE(estimator=PCA(), step=0.05,\n",
    "              n_features_to_select=0.1,\n",
    "              importance_getter='explained_variance_ratio_')\n",
    "\n",
    "agg.fit(X00, session00.tasks[2])\n",
    "\n",
    "sorting = pd.DataFrame(zip(agg.labels_, agg.feature_names_in_),\n",
    "                       columns=['labels_', 'feature_names_in_'])\n",
    "cluster_ids = list(sorting.groupby('labels_').groups.values())\n",
    "\n",
    "cluster_leaders = np.array([rfe.fit(X=X00.iloc[:, clust],\n",
    "                           y=session00.tasks[2]).get_feature_names_out()[0]\n",
    "                   if len(clust) > 1 else X00.iloc[:, clust].columns[0]\n",
    "                   for clust in cluster_ids])\n",
    "\n",
    "rfe2.fit(X00[cluster_leaders], session00.tasks[2])\n",
    "\n",
    "display(X00[rfe2.get_feature_names_out()].shape,\n",
    "        X00[rfe2.get_feature_names_out()],\n",
    "        validate_model(next(LinearSVCGen()),\n",
    "               X00[cluster_leaders], session00.tasks[2],\n",
    "               test_size=0.8,\n",
    "               stratify=session00.tasks[2]).round(2))\n",
    "#     AgglomerateMulticollinearCorrelated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est=PCA()\n",
    "est.fit(X00.corr('spearman'))\n",
    "ev=est.explained_variance_\n",
    "nm=est.feature_names_in_[np.argsort(ev)]\n",
    "# display(tuple(zip(nm, X00.columns)))\n",
    "explained_variance_ = pd.DataFrame(zip(nm, ev))\n",
    "# hierarchy.ward(squareform(1-explained_variance_ratio_.corr('spearman')))\n",
    "# pd.DataFrame(,\n",
    "#              index=PCA().fit(X00.corr('spearman')),\n",
    "#              columns=['explained_variance_ratio_']).sort_values('explained_variance_ratio_',\n",
    "#                                                                 ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X00.cov().round(2), pd.DataFrame(connect_mat,index=X00.columns,\n",
    "                                         columns=X00.columns).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(factorGenerator(X00.shape[1]))\n",
    "from itertools import combinations\n",
    "\n",
    "cond_unique_features_ = dict(tuple((itm, pd.Series(agglo00[itm[0]].new_features_.columns.tolist() +\n",
    "    agglo00[itm[1]].new_features_.columns.tolist()).nunique())\n",
    "                                   for itm in list(combinations(agglo00.keys(), 2))))\n",
    "cond_unique_features_\n",
    "# [len(val) for val in tuple(cond_unique_features_.values())]\n",
    "# .symmetric_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skb = SelectKBest(score_func=score_funcs.loc[score_func], k=k)\n",
    "# estimator_ = RFE(estimator=skb, step=step,\n",
    "#                  n_features_to_select=k,\n",
    "#                  importance_getter='scores_')\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.decomposition import FastICA, PCA\n",
    "\n",
    "score_funcs = pd.Series([chi2,\n",
    "                         f_classif, f_regression,\n",
    "                         mutual_info_classif,\n",
    "                         mutual_info_regression,\n",
    "                         GenericUnivariateSelect],\n",
    "                        index=['chi2',\n",
    "                               'f_classif', 'f_regression',\n",
    "                               'mutual_info_classif',\n",
    "                               'mutual_info_regression',\n",
    "                               'GenericUnivariateSelect'])\n",
    "\n",
    "pca = PCA()\n",
    "skb = SelectKBest(score_func=score_funcs['mutual_info_classif'], k=10)\n",
    "ica = FastICA(n_components=1, max_iter=10000,\n",
    "              algorithm='parallel',\n",
    "              whiten=True, fun='logcosh',\n",
    "              tol=0.0001, w_init=None,\n",
    "              random_state=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters_by_condition = AgglomerateMulticollinearCorrelated(X)\n",
    "\n",
    "X00task = X00.set_axis(session00.tasks[2], axis=0)\n",
    "agglo00 = {}\n",
    "\n",
    "[agglo00.update({cond: AgglomerateMulticollinearCorrelated(X=X00task.loc[cond],\n",
    "                                                           y=X00task.loc[cond].index,\n",
    "                                                           n_clusters=divmod(X00.shape[1], 4)[0])})\n",
    " for cond in session00.tasks[2].unique()]\n",
    "\n",
    "pca00 = {}\n",
    "\n",
    "[pca00.update({key: SortFeaturesByConditionPCA(X=agglo00[key].new_features_,\n",
    "                                   y=agglo00[key].new_features_.index,\n",
    "                                   n_features=4)})\n",
    "                    for key in tuple(agglo00.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFE(PCA(), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca00['Cs'][0].feature_names_in_.shape\n",
    "pca00.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(pca_filtered_[0][0].attributes_.shape, ,\n",
    "#        pca_filtered_[0][0].feature_names_out_.shape)\n",
    "display(*[pca00[key][0].components_table_#attributes_.T.sort_values('explained_variance_',\n",
    "#                                                   ascending=False)\n",
    "          for key in tuple(pca00.keys())])\n",
    "# display(pca_filtered_[0][0].feature_names_out_.shape,\n",
    "#         pca_filtered_[0][0].covariance_table_,\n",
    "#         pca_filtered_[0][0].components_table_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# agglo00[0].feature_names_out_==agglo00[0].feature_names_in_\n",
    "display(*[\n",
    "          \n",
    "          agglo00[key].new_features_ for key in tuple(agglo00.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_mat = CM(LedoitWolf(),\n",
    "                 kind='correlation').fit_transform([X00.values])[0]\n",
    "    \n",
    "# niplot.plot_connectome(session00.new_features.corr('spearman'),\n",
    "#                        niplot.find_probabilistic_atlas_cut_coords(session00.new_fmri_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo.n_leaves_, agglo00.children_\n",
    "agglo.__dict__.keys()\n",
    "agglo.connectivity230,\n",
    " ('Miss', 'Cs'): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "method_list = ['euclidean','spearman', 'pearson']\n",
    "features_by_cond = dict(tuple((method, SortFeaturesByConditionPCA(X00, session00.tasks[2],\n",
    "                                               method=method,\n",
    "                                               n_features = 20))\n",
    "                    for method in tqdm_(method_list)))\n",
    "# pca_new = PCA().fit(X00.set_axis(session00.tasks[2],axis=0).loc['Ctl'].corr('spearman'))\n",
    "# display(pca_new.components_.shape, pca_new.explained_variance_.shape,\n",
    "#  np.argsort(pca_new.explained_variance_),\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_by_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_test = PCA().fit(X00.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_subset = list(set(flatten([v.tolist() for v in tuple(features_by_cond.values())])))\n",
    "print(len(pca_subset))\n",
    "validate_model(next(LinearSVCGen()),\n",
    "               X=X00[pca_subset],#results01[-5]['selected'],#[col].values.reshape(-1,1),\n",
    "               y=session00.tasks[2], test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = maps_cdict.signal_matrix.copy(deep=True)\n",
    "# y = tasks[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols = '|'.join(['hippo', 'cuneus','rhinal',\n",
    "                 'lateral fissure posterior limb lh',\n",
    "                 'retrocalcarine cortex rh'])\n",
    "results00 = RecurviseKBestLinearSVC(X=X00,\n",
    "#                                     [list(set(flatten(X_subset.columns.tolist() +\n",
    "#                                                            X_permutation.columns.tolist())))],\n",
    "                                    y=session00.tasks[2],\n",
    "#                                     half=False,\n",
    "                                    score_func='mutual_info_classif',#'mutual_info_classif',\n",
    "                                    step=0.1)\n",
    "# results01 = RecurviseKBestPCA(X01, session01.tasks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results00['selected'].iloc[]\n",
    "list(factorGenerator(130))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*results00['performance'], results00['selected'].iloc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(next(LinearSVCGen(**{\"class_weight\": \"balanced\"})),\n",
    "               results00['selected'].iloc[-1],\n",
    "               session00.tasks[2],\n",
    "               stratify=session00.tasks[2],\n",
    "               test_size=0.8).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeoff00 = dict(tuple((rez['n_features'], round(rez['performance'], 2))\n",
    "                        for rez in results00))\n",
    "# tradeoff01 = dict(tuple((rez['n_features'], round(rez['performance'], 2))\n",
    "#                         for rez in results01))\n",
    "display(tradeoff00)\n",
    "# results01[-2]['selected'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(estimator=next(LinearSVCGen()),\n",
    "                         step=1,\n",
    "                         n_features_to_select=None,\n",
    "                         importance_getter='coef_')\n",
    "\n",
    "rfe.fit(results01[-5]['selected'],session00.tasks[2]).get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame([[round(validate_model(next(LinearSVCGen()),\n",
    "#                X=results01[-2]['selected'][col].values.reshape(-1,1),\n",
    "#                y=session01.tasks[2], test_size=0.8).accuracy.mean(), 2)\n",
    "#  for col in results01[-2]['selected'].columns]\n",
    "#  for n in range(5)]).mean()\n",
    "\n",
    "validate_model(next(LinearSVCGen()),\n",
    "               X=X00[pca_subset],#results01[-5]['selected'],#[col].values.reshape(-1,1),\n",
    "               y=session00.tasks[2], test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3,\n",
    "            perplexity=len(not_all_conds),\n",
    "            learning_rate='auto', init='pca',\n",
    "            angle=0.3, n_jobs=11,\n",
    "            square_distances=True)\n",
    "tsne.fit(X[not_all_conds])\n",
    "tsne_dims = tsne.fit_transform(X[not_all_conds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.multiclass import OutputCodeClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def rfecv_forest(X, y):\n",
    "    pca_per_cond = Bunch()\n",
    "    for cond in tqdm_(y.unique().tolist()):\n",
    "        X = X.set_axis(y, axis=0)\n",
    "        for n in range(abs((X.shape[1]-1) - 50)):\n",
    "            \n",
    "\n",
    "            estimator_ = RFE(estimator=PCA(),\n",
    "                               step=1,\n",
    "                               n_features_to_select=50,\n",
    "                               importance_getter='explained_variance_',\n",
    "                               n_jobs=11)\n",
    "\n",
    "            pca_per_cond.update({cond: dict(tuple((cond, estimator_.fit(X=X.loc[cond].values,     \n",
    "                                                              y=y[y==cond]\n",
    "                                                              ).get_feature_names_out())))})\n",
    "            X = X[pca_per_cond[cond]]\n",
    "\n",
    "    return pca_per_cond\n",
    "\n",
    "best50 = rfecv_forest(X, y=tasks[2])\n",
    "    # estimator_.__dict__\n",
    "# estimator_.fit(X.loc['Hit'], tasks[1].)\n",
    "# recursive_per_cond = Bunch(**dict(tuple((cond, estimator_.fit(X.loc[cond].values,\n",
    "# #                                                               y.loc[cond].values\n",
    "#                                                               ).get_support(indices=True))\n",
    "#                                         for cond in tasks[1].unique())))\n",
    "\n",
    "\n",
    "# sel = SequentialFeatureSelector(estimator=estimator_,\n",
    "#                                 n_features_to_select=X.shape[1]-1,\n",
    "#                                 scoring='explained_variance',\n",
    "#                                 direction='forward',\n",
    "#                                 n_jobs=11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_per_cond['Miss'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca.components_.T.shape\n",
    "PCA().fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pca.explained_variance_.shape,\n",
    " pca.explained_variance_ratio_.shape,\n",
    " pca.singular_values_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA, PCA, FastICA\n",
    "\n",
    "pca = PCA().fit(X.set_axis(tasks[2], axis=0))\n",
    "#     X.set_axis(tasks[0],axis=0).loc['Ctl'])\n",
    "\n",
    "# ica_ctl = FastICA(max_iter=2000, tol=5e-2).fit(weightings(X.set_axis(tasks[1], axis=0), linsvc.coef_table_).loc['Ctl'])\n",
    "\n",
    "pca.components_attributes_ = pd.DataFrame(zip(pca.explained_variance_,\n",
    "                                          pca.explained_variance_ratio_,\n",
    "                                          pca.singular_values_),\n",
    "                                          index=tasks[2],\n",
    "                                          columns=['explained_variance',\n",
    "                                                   'explained_variance_ratio',\n",
    "                                                   'singular_values'])\n",
    "pca.cov_table_ = pd.DataFrame(pca.get_covariance(),\n",
    "                                  index=pca.feature_names_in_,\n",
    "                                  columns=pca.feature_names_in_)\n",
    "\n",
    "pca.components_table_ = pd.DataFrame(pca.components_,\n",
    "                                         index=tasks[2].values,\n",
    "                                         columns=pca.feature_names_in_)\n",
    "\n",
    "# ica_ctl.components_.shape\n",
    "# pca.components_table_.groupby(pca.components_table_.index).mean()\n",
    "# pca.components_table_\n",
    "pca.components_.shape, pca.explained_variance_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [pca.components_table_.loc['Ctl']==pca.components_table_.loc['Ctl'].max()]\n",
    "# pca.components_table_[pca.components_table_==pca.components_table_.max()]\n",
    "pd.DataFrame([row[1].sort_values(ascending=False)[:50]\n",
    "              for row in pca.components_table_.loc['Ctl'].iterrows()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipca_ctl = IncrementalPCA()\n",
    "\n",
    "ipca_ctl.fit(X.set_axis(tasks[0],axis=0).loc['Ctl'])\n",
    "# [ipca_ctl.partial_fit(row[1].values.reshape(-1, 1))\n",
    "#  for row in X.set_axis(tasks[0],axis=0).loc['Ctl'].iterrows()]\n",
    "\n",
    "ipca_ctl.feature_names_in_ = X.columns.tolist()\n",
    "# ipca_ctl.set_params(**{'feature_names_in_': X.columns.tolist()})\n",
    "ipca_ctl.cov_table_ = pd.DataFrame(ipca_ctl.get_covariance(), index=ipca_ctl.feature_names_in_,\n",
    "                                  columns=ipca_ctl.feature_names_in_)\n",
    "ipca_ctl.components_table_ = pd.DataFrame(ipca_ctl.components_, columns=ipca_ctl.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dir(pca_ctl))\n",
    "# pca_ctl.feature_names_in_\n",
    "#==X.set_axis(tasks[0],axis=0).loc['Ctl']\n",
    "# pca_ctl.cov_table_.pow(-1)\n",
    "# pca_ctl.explained_variance_ratio_.shape, pca_ctl.explained_variance_.shape\n",
    "# pca_ctl.score(pca_ctl.components_table_)\n",
    "ipca_ctl.get_precision()\n",
    "# pca_ctl.components_table_.where(np.isinf(pca_ctl.components_table_.values)).notna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipca_ctl.get_params().keys()\n",
    "pca_ctl.explained_variance_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dir(linsvc))\n",
    "# sorted(dict(inspect.getmembers(linsvc, callable)).keys())\n",
    "# help(linsvc.score)#(tasks[2], linsvc.predict(X))\n",
    "#tasks[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# display(linsvc.coef_table_[linsvc.coef_table_==linsvc.coef_table_.max()].T.head(30),\n",
    "#         linsvc.coef_table_[linsvc.coef_table_==linsvc.coef_table_.min()].T.head(30),\n",
    "#         linsvc.decision_table_.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linsvc.coef_table_[linsvc.coef_table_==linsvc.coef_table_.min()]\n",
    "linsvc.coef_table_.T[linsvc.coef_table_.T==linsvc.coef_table_.T.min()].dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_correlates(linsvc.coef_table_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict(inspect.getmembers(pd.DataFrame.corr)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin, pairwise_distances_argmin_min\n",
    "from scipy.stats import spearmanr\n",
    "# (linsvc.decision_table_, linsvc.coef_table_)\n",
    "# help(pairwise_distances_argmin)#(X.set_axis(tasks[2], axis=0), X.set_axis(tasks[2], axis=0))\n",
    "# pd.Series(pairwise_distances_argmin(X.T,X.T, metric=lambda X, y: scipy.stats.spearmanr(X, y).correlation)).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_corr_mat(X.set_axis(tasks[2], axis=0).loc['Ctl'],\n",
    "                 method='spearman', thresh=0.9, positive=True)\n",
    "\n",
    "trimmed_corr_mat(linsvc.coef_table_.loc['Ctl'],\n",
    "                 method='spearman', thresh=0.9, positive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(scipy.stats.spearmanr(X, X).correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_data = [np.uint8(NiftiMasker().fit_transform(img)).flatten()\n",
    "#             for img in tqdm_(list(nimage.iter_img(session.masker._resampled_maps_img_)))]\n",
    "# maps_vectors = pd.DataFrame(pairwise_distances(np.transpose(np.array([img.get_fdata().flatten()\n",
    "#                          for img in nimage.iter_img(session.masker._resampled_maps_img_)]))),\n",
    "#                 index=X.columns, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "sorted(dict(inspect.getmembers(agglo, callable)).keys())\n",
    "# max(flatten(agglo.children_.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "from sklearn.covariance import GraphicalLassoCV, GraphicalLasso, LedoitWolf\n",
    "# nn = NearestNeighbors(n_neighbors=3)\n",
    "\n",
    "\n",
    "\n",
    "agglo.labels_mapping_ = pd.DataFrame(tuple(zip(agglo.labels_,\n",
    "                                               agglo.feature_names_in_))).groupby(0).groups\n",
    "agglo.features_names_mapping_ = dict(tuple((itm[0], X.iloc[:, itm[1]].columns.tolist())\n",
    "                                     for itm in tuple(agglo.labels_mapping_.items())))\n",
    "agglo.feature_names_out_ = [','.join(list(set(' '.join(val).split()))) for val in\n",
    "                            tuple(agglo.features_names_mapping_.values())]\n",
    "\n",
    "agglo.features_names_mapping_\n",
    "# roi_nn = nn.fit(maps_vectors).kneighbors()\n",
    "\n",
    "# roi_nn = Bunch(**dict(tuple(zip(['neigh_dist', 'neigh_ind'],\n",
    "#                                 roi_nn))))\n",
    "# pd.DataFrame.gt(X.corr('spearman').mask(\n",
    "#     np.triu(np.ones_like(X.corr('spearman'),\n",
    "#                          dtype=bool))), 0.9).any().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=next(LinearSVCGen()),\n",
    "    min_features_to_select=10,\n",
    "    step=5,\n",
    "    n_jobs=11,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "\n",
    "best_clusters = [new_features[rfecv.fit(new_features, task).get_support(indices=True)]\n",
    "                 for task in tasks]\n",
    "\n",
    "display(*best_clusters)\n",
    "# best_of_ = [dict(tuple((itm[0], rfecv.fit(X.iloc[:, itm[1]].values,\n",
    "#                                 task).get_support(indices=True))\n",
    "#              for itm in tuple(agglo.labels_mapping_.items())\n",
    "#              if len(itm[1])>1))\n",
    "#             for task in tasks]\n",
    "\n",
    "# best_indices = rfecv.fit(new_features, tasks[2]).get_support(indices=True)\n",
    "# new_features.iloc[:, rfecv.fit(new_features, tasks[2]).get_support(indices=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_of_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[[pd.Series(dict(tuple((idx,\n",
    "                                 validate_model(next(LinearSVCGen()),\n",
    "                                                X=X.iloc[:, agglo.labels_mapping_[task[0]][idx]],\n",
    "                                                y=task[1], test_size=0.8).accuracy.mean().round(2))\n",
    "                                for idx in best_of_[task[0]]))).sort_values(ascending=False).iloc[[0]]]\n",
    "          for task in enumerate(tasks)])\n",
    "# trimmed_corr_mat(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niplot.view_img(nimage.mean_img(nimage.index_img(session.masker.maps_img, agglo.labels_mapping_[9])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairwise_corrs(X: pd.DataFrame,\n",
    "                        method: str = 'spearman',\n",
    "                        thresh: float = 0.9,\n",
    "                        positive: bool = True\n",
    "                        ) -> list:\n",
    "    \"\"\"\n",
    "    A function to identify highly correlated features.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix with absolute values\n",
    "#     matrix = X.corr(method).abs()\n",
    "    matrix = X.corr(method)\n",
    "\n",
    "    # Create a boolean mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "#     return matrix.mask(mask)\n",
    "    \n",
    "    # Subset the matrix\n",
    "    masked_matrix = matrix.mask(mask)\n",
    "    \n",
    "#     # Find cols that meet the thresh\n",
    "    eq_sign = next(get_corr_sign(thresh))\n",
    "    to_drop = masked_matrix[eq_sign(masked_matrix, thresh)].stack().index.tolist()\n",
    "    return to_drop\n",
    "#     to_drop = [c for c in masked_matrix.columns if \\\n",
    "#               masked_matrix[eq_sign(masked_matrix[c], thresh)].any()]\n",
    "    \n",
    "#     return to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_correlated(X: pd.DataFrame,\n",
    "                        method: str = 'spearman',\n",
    "                        threshold: float = 0.9\n",
    "                        ) -> list:\n",
    "    \"\"\"\n",
    "    A function to identify highly correlated features.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix with absolute values\n",
    "#     matrix = X.corr(method).abs()\n",
    "    matrix = X.corr(method).abs()\n",
    "\n",
    "    # Create a boolean mask\n",
    "    mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "    \n",
    "    # Subset the matrix\n",
    "    masked_matrix = matrix.mask(mask)\n",
    "    \n",
    "    # Find cols that meet the threshold\n",
    "#     to_drop = [c for c in reduced_matrix.columns if \\\n",
    "#               any(reduced_matrix[c] > threshold)]\n",
    "    \n",
    "    return masked_matrix\n",
    "\n",
    "def reduce_matrix(X: pd.DataFrame,\n",
    "                        method: str = 'spearman',\n",
    "                        threshold: float = 0.9\n",
    "                        ) -> pd.DataFrame:\n",
    "    masked_matrix = identify_correlated(X, method=method,\n",
    "                                         threshold=threshold)\n",
    "    to_drop = [c for c in masked_matrix.columns if\n",
    "               any(masked_matrix[c] > threshold)]\n",
    "    reduced_matrix = X.drop(to_drop, axis=1)\n",
    "    return reduced_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfecv_params = dict(step=5,\n",
    "                    cv=5, n_jobs=11,\n",
    "                    importance_getter='auto',\n",
    "                    min_features_to_select=5,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "# reduce_matrix(X).shape\n",
    "# reduced_ = reduce_matrix(X)\n",
    "# reduced_cv = reduced_[RFECV(next(LinearSVCGen()),\n",
    "#                             **rfecv_params).fit(reduced_, tasks[2]).get_feature_names_out()]\n",
    "\n",
    "# reduced_cv\n",
    "# def iter_reduce_matrix(X: pd.DataFrame,\n",
    "#                         method: str = 'spearman',\n",
    "#                         threshold: float = 0.9\n",
    "#                         ) -> pd.DataFrame:\n",
    "    \n",
    "    \n",
    "# rfecv_matrices = [X[RFECV(next(LinearSVCGen()), **rfecv_params).fit(X, tasks[task[0]]).get_feature_names_out()]\n",
    "#                   for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "\n",
    "# print([mat.shape for mat in rfecv_matrices])\n",
    "\n",
    "# fwd_reduced_matrices = [rfecv_matrices[task[0]]\n",
    "#                         [SequentialFeatureSelector(ovrc.fit(\n",
    "#                             X_new, tasks[task[0]]),\n",
    "#                                                    direction='forward',\n",
    "#                                                    **sfs_params).fit(\n",
    "#                             session.computed_.signal_matrix,\n",
    "#                             tasks[task[0]]).get_feature_names_out()]\n",
    "#                         for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "# bwd_reduced_matrices = [fwd_reduced_matrices[task[0]][SequentialFeatureSelector(\n",
    "#                             next(LinearSVCGen()), direction='backward',\n",
    "#                             **sfs_params).fit(fwd_reduced_matrices[task[0]],\n",
    "#                                               tasks[task[0]]).get_feature_names_out()]\n",
    "#                         for task in tqdm_(enumerate(tasks))]\n",
    "# validate_model(next(LinearSVCGen()),\n",
    "#                X=reduce_matrix(X),\n",
    "#                          y=tasks[1],\n",
    "#                          test_size=0.8,\n",
    "#                          stratify=tasks[1],\n",
    "#                          shuffle=True).round(2)\n",
    "# validate_model(estimator=RandomForestRegressor(),\n",
    "#                X=reduce_matrix(X), y=tasks[2], test_size=0.8)\n",
    "# # Build feature/target arrays\n",
    "\n",
    "# def validate_forest(X, y,\n",
    "#                     test_size: float = 0.5,\n",
    "#                     random_state: int = None\n",
    "#                     ) :\n",
    "\n",
    "#     # Generate train/test sets\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                         test_size=.3,\n",
    "#                                                         random_state=1121218)\n",
    "\n",
    "# %%time\n",
    "# # Init, fit, score\n",
    "# forest = RandomForestRegressor()\n",
    "# _ = forest.fit(X_train, y_train)\n",
    "\n",
    "# >>> print(f\"Training score: {forest.score(X_train, y_train)}\")\n",
    "# Training score: 0.9860728454127408\n",
    "\n",
    "# >>> print(f\"Test score: {forest.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusterize(X, y,\n",
    "               method: str = 'spearnan',\n",
    "               thresh: float = 0.9,\n",
    "               positive: bool = True,\n",
    "               n_deci: int = 32\n",
    "               ) -> Bunch:\n",
    "    X = X.set_axis(y, axis=0)\n",
    "    eq_sign = tuple(filter(lambda x: x[0],\n",
    "                           ((positive, pd.DataFrame.gt),\n",
    "                            (not positive, pd.DataFrame.lt))))[0][1]\n",
    "\n",
    "    return Bunch(**dict(tuple((cond, Bunch(**dict(\n",
    "               tuple((row[0], row[1].dropna().to_dict())\n",
    "                     for row in\n",
    "                     X.loc[cond].corr(method).round(n_deci).where(\n",
    "                         X.loc[cond].corr(method).round(n_deci) !=\n",
    "                         np.triu(X.loc[cond].corr(method).round(\n",
    "                             n_deci).values)).where(eq_sign(X.loc[cond].corr(\n",
    "                         method).round(n_deci),\n",
    "                                     thresh)).iterrows()\n",
    "                     if row[1].dropna().to_dict() != {}))))\n",
    "                              for cond in list(set(tuple(y))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, tasks[2], test_size=0.5, random_state=1121218)\n",
    "\n",
    "X_train_std, X_test_std = list(map(StandardScaler().fit_transform, [X_train, X_test]))\n",
    "\n",
    "y_train, y_test =  y_train.values.reshape(-1, 1), y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=next(LinearSVCGen()),\n",
    "    min_features_to_select=50,\n",
    "    step=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "X[rfecv.fit(X, tasks[2]).get_feature_names_out()]\n",
    "\n",
    "\n",
    "# lr = next(LinearSVCGen())\n",
    "# lr.fit(X_train_std, y_train)\n",
    "\n",
    "# print(\"Trainign R-sqaured:\", lr.score(X_train_std, y_train))\n",
    "# print(\"Testing R-squared:\",lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cimaq_decoding_utils import chunks\n",
    "from cimaq_decoding_utils import flatten\n",
    "    \n",
    "X = session.computed_.signal_matrix.copy(deep=True)\n",
    "tasks = [session.events.trial_type,\n",
    "         session.events.recognition_performance.replace({'Miss':'Fail'}),\n",
    "         session.events.iloc[:, -1]]\n",
    "\n",
    "X = X.set_axis(tasks[2], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PairwiseClusterNames(X,\n",
    "                         method: str = 'spearman',\n",
    "                         thresh: float = 0.90\n",
    "                         ):\n",
    "\n",
    "    X_corr = trimmed_corr_mat(X, method=method)\n",
    "    pairs = X_corr[X_corr > thresh].stack()\n",
    "    pairs.sort_values(ascending=False, inplace=True)\n",
    "    return list(map(list, pairs.index.tolist())).__iter__()\n",
    "        \n",
    "    \n",
    "def mknew_(X, y):\n",
    "\n",
    "    return pd.Series(data=X[list(y)].T.mean().values,\n",
    "                     index=X.index, name=','.join(y))\n",
    "\n",
    "\n",
    "# def MakePairwise(X, method: str = 'spearman',\n",
    "#                  thresh=0.90):\n",
    "    \n",
    "#     X = X.copy(deep=True)\n",
    "#     mat = trimmed_corr_mat(X, method=method, thresh=thresh)\n",
    "# #     clusters = [[str(row[0])]+\n",
    "# #     pairs = list(PairwiseCorrelates(X, method, thresh))\n",
    "# #     new_ = pd.concat([mknew_(X, pair) for pair in pairs], axis=1)\n",
    "# #     X.drop(flatten(pairs), axis=1, inplace=True)\n",
    "\n",
    "#     return new_\n",
    "#     while PairwiseClusterNames(new_, method, thresh):\n",
    "        \n",
    "    \n",
    "\n",
    "# def RecursivePairwise(X, method: str = 'spearman',\n",
    "#                       thresh=0.90):\n",
    "    \n",
    "\n",
    "\n",
    "def PairwiseCorrelates(X, method: str = 'spearman',\n",
    "                       thresh=0.90):\n",
    "\n",
    "    X_corr = trimmed_corr_mat(X, method=method)\n",
    "    corr_list = PairwiseClusterNames(X, method=method,\n",
    "                                     thresh=thresh)\n",
    "\n",
    "    yield from corr_list.__iter__()\n",
    "    \n",
    "# trimmed_corr_mat(MakePairwise(X))[trimmed_corr_mat(MakePairwise(X))>0.9].stack().index.tolist()\\\n",
    "# MakePairwise(\n",
    "# [scipy.stats.spearmanr(mknew_(X, PairwiseClusterNames(X)[0]), row[1].values)\n",
    "#  for row in X.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# clust = [[row[0]]+row[1].dropna().index.tolist() for row in trimmed_corr_mat(X).iterrows()]\n",
    "# len(clust), len(flatten(clust)), abs(len(clust) - len(flatten(clust)))\n",
    "\n",
    "sc = X.corr('spearman').where(X.corr('spearman').values\n",
    "                         != np.tril(X.corr('spearman').values))\n",
    "sc = sc.where(sc>0)\n",
    "\n",
    "# trimmed_corr_mat(X)\n",
    "sc.dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FeatureAgglomeration.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proximity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trimmed_corr_mat(, thresh=0.99)#.filter(regex='cingu').T.filter(regex='cingu')\n",
    "\n",
    "a=proximity_matrix[proximity_matrix!=np.tril(proximity_matrix)].filter(\n",
    "    regex='lh')['retrocalcarine cortex lh'].dropna()\n",
    "a['inferior temporal sulcus anterior rh'], a['inferior temporal sulcus anterior lh']\n",
    "#     .filter(regex='posterior').T.filter(\n",
    "#     regex='cingu').filter(regex='posterior').filter(regex='lh').T.sort_index().filter(regex='rh').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from sklearn.cluster import FeatureAgglomeration, ward_tree\n",
    "\n",
    "\n",
    "pos_len = len(trimmed_corr_mat(X, positive=True,\n",
    "                               thresh=0.9).stack().index.tolist())\n",
    "neg_len = len(trimmed_corr_mat(X, positive=False,\n",
    "                               thresh=-0.9).stack().index.tolist())\n",
    "n_c = pos_len+neg_len\n",
    "\n",
    "proximity_matrix = 1 - pd.DataFrame(pairwise_distances(np.array([img.get_fdata().flatten()\n",
    "                                                               for img in\n",
    "                                  nimage.iter_img(session.masker._resampled_maps_img_)])),\n",
    "                                index=X.columns, columns=X.columns)\n",
    "\n",
    "agglo = FeatureAgglomeration(n_clusters=56,\n",
    "                             affinity='euclidean',\n",
    "                             connectivity=proximity_matrix,\n",
    "#                              connectivity=sprm_pw_neg,\n",
    "                             compute_full_tree='auto',\n",
    "                             linkage='ward',\n",
    "                             pooling_func=np.mean,\n",
    "#                              distance_threshold=None,\n",
    "#                              distance_threshold=sprm_pw_neg.quantile(0.75).mean(),\n",
    "                             compute_distances=True)\n",
    "agglo.fit(X)\n",
    "\n",
    "agglo.labels_mapping_ = pd.DataFrame(tuple(zip(agglo.labels_,\n",
    "                                               agglo.feature_names_in_))).groupby(0).groups\n",
    "agglo.features_names_mapping_ = dict(tuple((itm[0], X.iloc[:, itm[1]].columns.tolist())\n",
    "                                     for itm in tuple(agglo.labels_mapping_.items())))\n",
    "agglo.feature_names_out_ = [','.join(list(set(' '.join(val).split()))) for val in\n",
    "                            tuple(agglo.features_names_mapping_.values())]\n",
    "# w_agglo = Bunch(**dict(tuple(zip(['children', 'n_connected_components',\n",
    "#                                   'n_leaves', 'parents', 'distances'],\n",
    "#                                  ward_tree(X.corr('spearman'), connectivity=proximity_matrix,\n",
    "#                                            n_clusters=64, return_distance=True)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature, target arrays\n",
    "# X, y = ansur.iloc[:, :-1], ansur.iloc[:, -1]\n",
    "X = session.computed_.signal_matrix.copy(deep=True)\n",
    "tasks = [session.events.trial_type,\n",
    "         session.events.recognition_performance.replace({'Miss':'Fail'}),\n",
    "         session.events.iloc[:, -1]]\n",
    "y = tasks[2]\n",
    "X.set_axis(y, axis=0, inplace=True)\n",
    "\n",
    "# Train/test set generation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=1121218\n",
    ")\n",
    "\n",
    "# Scale train and test sets with StandardScaler\n",
    "X_train_std = StandardScaler().fit_transform(X_train)\n",
    "X_test_std = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "# Fix the dimensions of the target array\n",
    "y_train = y_train.values.reshape(-1, 1)\n",
    "y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# Init, fit, test Lasso Regressor\n",
    "forest = RandomForestRegressor()\n",
    "_ = forest.fit(X_train_std, y_train)\n",
    "forest_score = forest.score(X_test_std, y_test)\n",
    "\n",
    "print(forest_score)\n",
    "importance_table = pd.DataFrame(zip(X_train.columns,\n",
    "                                    abs(forest.feature_importances_)),\n",
    "                                columns=[\"feature\", \"weight\"]).sort_values(\n",
    "                       \"weight\").reset_index(drop=True)\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Init the transformer\n",
    "rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=10)\n",
    "\n",
    "# Fit to the training data\n",
    "_ = rfe.fit(X_train_std, y_train)\n",
    "\n",
    "# Init, fit, score\n",
    "forest = RandomForestRegressor()\n",
    "_ = forest.fit(rfe.transform(X_train_std), y_train)\n",
    "forest.score(rfe.transform(X_test_std), y_test)\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Init, fit\n",
    "rfecv = RFECV(\n",
    "    estimator=LinearRegression(),\n",
    "    min_features_to_select=5,\n",
    "    step=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "lr = LinearRegression()\n",
    "_ = lr.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"Trainign R-sqaured:\", lr.score(X_train_std, y_train))\n",
    "print(\"Testing R-squared:\",lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo_params = agglo.get_params(deep=True)\n",
    "X_reduced = pd.DataFrame(agglo.transform(X), index=X.index,\n",
    "                         columns=agglo.feature_names_out_)\n",
    "X_restored = agglo.inverse_transform(X_reduced.values)\n",
    "\n",
    "X_reduced4 = X_reduced\n",
    "X_reduced4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(all(X_reduced0 == X_reduced2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_classes = [task.value_counts()[task.value_counts()==task.value_counts(ascending=True)[0]]\n",
    " for task in tasks]\n",
    "\n",
    "min_classes = [c for c in min_classes if c.values == 1]\n",
    "min_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(next(LinearSVCGen()),\n",
    "               X=X_reduced.values,\n",
    "                         y=tasks[1],\n",
    "                         test_size=0.8,\n",
    "                         stratify=tasks[1],\n",
    "                         shuffle=True).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprm_pw = pd.DataFrame(pairwise_distances(X.corr('spearman')),\n",
    "             columns=X.columns, index=X.columns)\n",
    "sprm = X.corr('spearman')\n",
    "sprm_neg = 1-X.corr('spearman')\n",
    "sprm_pw_neg = pd.DataFrame(pairwise_distances(sprm_neg),\n",
    "                           columns=X.columns,\n",
    "                           index=X.columns)\n",
    "display(sprm_pw.describe(), sprm.describe(), sprm_neg.describe(), sprm_pw_neg.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprm_pw_neg.quantile(0.75).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimmed_corr_mat(X)[trimmed_corr_mat(X)==trimmed_corr_mat(X).max()]#.sort_values(ascending=False)\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X = session.computed_.signal_matrix.copy(deep=True)\n",
    "y = tasks[2]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "squareform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42)\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n",
    "ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "ax2.boxplot(\n",
    "    result.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=data.feature_names[perm_sorted_idx],\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [session.events.trial_type,\n",
    "         session.events.recognition_performance.replace({'Miss':'Fail'}),\n",
    "         session.events.iloc[:, -1]]\n",
    "\n",
    "def clusterize(X, y,\n",
    "               method: str = 'spearnan',\n",
    "               thresh: float = 0.9,\n",
    "               positive: bool = True,\n",
    "               n_deci: int = 32\n",
    "               ) -> Bunch:\n",
    "    X = X.set_axis(y, axis=0)\n",
    "    eq_sign = tuple(filter(lambda x: x[0],\n",
    "                           ((positive, pd.DataFrame.gt),\n",
    "                            (not positive, pd.DataFrame.lt))))[0][1]\n",
    "\n",
    "    return Bunch(**dict(tuple((cond, Bunch(**dict(\n",
    "               tuple((row[0], row[1].dropna().to_dict())\n",
    "                     for row in\n",
    "                     X.loc[cond].corr(method).round(n_deci).where(\n",
    "                         X.loc[cond].corr(method).round(n_deci) !=\n",
    "                         np.triu(X.loc[cond].corr(method).round(\n",
    "                             n_deci).values)).where(eq_sign(X.loc[cond].corr(\n",
    "                         method).round(n_deci),\n",
    "                                     thresh)).iterrows()\n",
    "                     if row[1].dropna().to_dict() != {}))))\n",
    "                              for cond in list(set(tuple(y))))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rfecv_params = dict(step=0.1, cv=None, n_jobs=11,\n",
    "                    min_features_to_select=1,\n",
    "                    scoring='accuracy')\n",
    "\n",
    "sfs_params = dict(n_jobs=11, n_features_to_select=0.25,\n",
    "                  scoring='accuracy')\n",
    "\n",
    "\n",
    "ovrc = OneVsRestClassifier(next(LinearSVCGen()), n_jobs=11)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recursive_best(X: Iterable,\n",
    "                       y: Iterable = None,\n",
    "                       estimator=None,\n",
    "                       rfecv_params: Union[dict, Bunch] = None,\n",
    "                       ) -> Iterable:\n",
    "    rfecv_defs = dict(cv=None, step=0.01,  n_jobs=11,\n",
    "                      min_features_to_select=1,\n",
    "                      scoring='accuracy')\n",
    "    if rfecv_params is None:\n",
    "        rfecv_params = {}\n",
    "    rfecv_defs.update(rfecv_params)\n",
    "    if estimator is None:\n",
    "        estimator = next(LinearSVCGen())\n",
    "#     rfecv = RFECV(estimator, **rfecv_defs)\n",
    "#     rez = yield X[rfecv.fit(X, y).get_feature_names_out()]\n",
    "#     for n in range(2):\n",
    "#         X = rez.send(rez[rez.columns], y)\n",
    "#     yie\n",
    "    rfecv = RFECV(estimator, **rfecv_defs).fit(X, y)\n",
    "    selected = tuple(zip(rfecv.support_, rfecv.feature_names_in_))\n",
    "    rez = yield [itm[1] for itm in list(filter(lambda x: x[0], selected))]\n",
    "    yield from (rez.send(X) for n in range(4))\n",
    "\n",
    "next(get_recursive_best(X, tasks[2]))\n",
    "# X[get_recursive_best(X[get_recursive_best(X[get_recursive_best(X[get_recursive_best(X[get_recursive_best(X=X, y=tasks[2])], tasks[2])],\n",
    "#                    tasks[2])], tasks[2])], tasks[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector, SelectFromModel\n",
    "\n",
    "sfm = SelectFromModel(next(LinearSVCGen()),\n",
    "                      threshold=-np.inf, max_features=8)\n",
    "\n",
    "bsfs = SequentialFeatureSelector(next(LinearSVCGen()),\n",
    "                                 direction='backward',\n",
    "                                 scoring='accuracy',\n",
    "                                 n_jobs=11)\n",
    "\n",
    "test = bsfs.fit(X, X.index).get_feature_names_out()\n",
    "test\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method='spearman'\n",
    "n_deci=2\n",
    "\n",
    "tuple(X.corr(method).round(n_deci\n",
    "                          ).where(\n",
    "     X.corr(method).round(n_deci) !=\n",
    "     np.triu(X.corr(method).round(\n",
    "         n_deci).values)).where(X.corr(\n",
    "     method).round(n_deci)>thresh).iterrows()\n",
    "      for row in X.corr(method).iterrows()\n",
    " if row[1].dropna().to_dict() != {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "sel0, sel1 = [RFECV(next(LinearSVCGen()),\n",
    "                   **rfecv_params)] * n\n",
    "\n",
    "X0 = X[sel0.fit(X, y).get_feature_names_out()]\n",
    "sel2 = RFECV(next(LinearSVCGen()),\n",
    "                   **rfecv_params)\n",
    "sel2.fit(X0, y)\n",
    "sel2.get_feature_names_out().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix\n",
    "y = tasks[2]\n",
    "n = 4\n",
    "\n",
    "selectors = [RFECV(next(LinearSVCGen()),\n",
    "                   **rfecv_params)] * n\n",
    "X = X[selectors[0].fit(X, y).get_feature_names_out()]\n",
    "# X = X[selectors[1].fit(X, y).get_feature_names_out()]\n",
    "X\n",
    "# for sel in selectors:\n",
    "#     sel.fit(X, y)\n",
    "#     outnames = sel.get_feature_names_out()\n",
    "#     sel.fit(X[outnames], y)\n",
    "#     print(len(sel.get_feature_names_out()))\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv = RFECV(next(LinearSVCGen()),\n",
    "#               **rfecv_params).fit(X, tasks[2])\n",
    "\n",
    "def selector(X, y):\n",
    "    rfecv_params = dict(step=0.2, cv=None, n_jobs=11,\n",
    "                        min_features_to_select=1,\n",
    "                        scoring='accuracy')\n",
    "    selectors = RFECV(next(LinearSVCGen()),\n",
    "                       **rfecv_params)\n",
    "    return X[sel.fit(X, y).get_feature_names_out()]\n",
    "\n",
    "X_new = selector(X, tasks[2])\n",
    "#     [X[sel.fit(X, y).get_feature_names_out()]\n",
    "#      >> ]\n",
    "# rfecv = (RFECV(next(LinearSVCGen()),\n",
    "#               **rfecv_params).fit(X, tasks[2]))\n",
    "# selected = tuple(zip(rfecv.support_, rfecv.feature_names_in_))\n",
    "# X[[itm[1] for itm in\n",
    "#    list(filter(lambda x: x[0], selected))]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tasks[2]\n",
    "estimator = next(LinearSVCGen())\n",
    "estimator.fit(X, y)\n",
    "print(estimator.__dict__.keys())\n",
    "\n",
    "coef_table = pd.DataFrame(estimator.coef_,\n",
    "                          index=estimator.classes_,\n",
    "                          columns=estimator.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "renamer = dict(tuple((itm[1], itm[0])\n",
    "                     for itm in enumerate(y.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(coef_table.T*coef_table.T.std()).corr('spearman').plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "coef_table.min()\n",
    "# scaled_corr = MinMaxScaler().fit_transform(coef_table.corr('spearman'))\n",
    "# sns.heatmap(1 - scaled_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = tuple(zip(rfecv.support_, rfecv.feature_names_in_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[itm[1] for itm in list(filter(lambda x: x[0], selected))].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfecv_matrices = [X[RFECV(next(LinearSVCGen()), **rfecv_params).fit(X, tasks[task[0]]).get_feature_names_out()]\n",
    "#                   for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "\n",
    "# print([mat.shape for mat in rfecv_matrices])\n",
    "\n",
    "# fwd_reduced_matrices = [rfecv_matrices[task[0]]\n",
    "#                         [SequentialFeatureSelector(ovrc.fit(\n",
    "#                             X_new, tasks[task[0]]),\n",
    "#                                                    direction='forward',\n",
    "#                                                    **sfs_params).fit(\n",
    "#                             session.computed_.signal_matrix,\n",
    "#                             tasks[task[0]]).get_feature_names_out()]\n",
    "#                         for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "# bwd_reduced_matrices = [fwd_reduced_matrices[task[0]][SequentialFeatureSelector(\n",
    "#                             next(LinearSVCGen()), direction='backward',\n",
    "#                             **sfs_params).fit(fwd_reduced_matrices[task[0]],\n",
    "#                                               tasks[task[0]]).get_feature_names_out()]\n",
    "#                         for task in tqdm_(enumerate(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fwd_reduced_matrices = [rfecv_matrices[task[0]]\n",
    "                        [SequentialFeatureSelector(ovrc.fit(\n",
    "                            X_new, tasks[task[0]]),\n",
    "                                                   direction='forward',\n",
    "                                                   **sfs_params).fit(\n",
    "                            session.computed_.signal_matrix,\n",
    "                            tasks[task[0]]).get_feature_names_out()]\n",
    "                        for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "bwd_reduced_matrices = [fwd_reduced_matrices[task[0]][SequentialFeatureSelector(\n",
    "                            next(LinearSVCGen()), direction='backward',\n",
    "                            **sfs_params).fit(fwd_reduced_matrices[task[0]],\n",
    "                                              tasks[task[0]]).get_feature_names_out()]\n",
    "                        for task in tqdm_(enumerate(tasks))]\n",
    "\n",
    "print([mat.shape for mat in fwd_reduced_matrices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions import img_to_signals_maps, signals_to_img_maps, _compute_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # session.masker.maps_img.shape\n",
    "fmri4d = signals_to_img_maps(region_signals=session.computed_.signal_matrix,\n",
    "                             maps_img=session.masker._resampled_maps_img_,\n",
    "                             mask_img=session.masker._resampled_mask_img_)\n",
    "# session.masker.__dict__.keys()\n",
    "#.maps_img.shape, session.masker.mask_img.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_matrices = [rfecv_matrices[task[0]][RFECV(next(LinearSVCGen()), **rfecv_params).fit(\n",
    "                      session.computed_.signal_matrix, tasks[task[0]]).get_feature_names_out()]\n",
    "                  for task in tqdm_(enumerate(tasks))]\n",
    "print([mat.shape for mat in rfecv_matrices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_reduced_matrices = [rfecv_matrices[task[0]][SequentialFeatureSelector(\n",
    "                            next(LinearSVCGen()), direction='forward',\n",
    "                            **sfs_params).fit(rfecv_matrices[task[0]],\n",
    "                                              tasks[task[0]]).get_feature_names_out()]\n",
    "                        for task in tqdm_(enumerate(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "bwd_reduced_matrices = [fwd_reduced_matrices[task[0]][SequentialFeatureSelector(\n",
    "                            next(LinearSVCGen()), direction='backward',\n",
    "                            **sfs_params).fit(fwd_reduced_matrices[task[0]],\n",
    "                                              tasks[task[0]]).get_feature_names_out()]\n",
    "                        for task in tqdm_(enumerate(tasks))]\n",
    "display(*bwd_reduced_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_savepath(data_dir, dimension, resolution_mm):\n",
    "    dim, res = str(dimension), str(resolution_mm)+'mm'\n",
    "    suffix = f'difumo_{dim}_{res}_cortex_labels.txt'\n",
    "    while True:\n",
    "        yield (os.path.join(data_dir, suffix))\n",
    "\n",
    "\n",
    "def get_cortex_atlas(atlas):\n",
    "    from nilearn.plotting import find_probabilistic_atlas_cut_coords\n",
    "    not_cortex = (atlas.labels.reset_index(drop=False).set_index(\n",
    "                      'difumo_names').T.filter(\n",
    "                          regex='|'.join(no_cortex)).columns.tolist())\n",
    "    cortex = atlas.labels.reset_index(drop=False).set_index(\n",
    "                 'difumo_names').drop(list(not_cortex), axis=0)\n",
    "    \n",
    "    new_map = nilearn.image.index_img(atlas.maps, cortex.component)\n",
    "    cortex = cortex.drop(['component'], axis=1).reset_index(drop=False)\n",
    "    cortex['component'] = range(cortex.shape[0])\n",
    "    cortex = cortex.set_index('component')\n",
    "    cortex[['x', 'y' 'z']] = find_probabilistic_atlas_cut_coords(new_map)\n",
    "    return Bunch(**dict(maps=new_map, labels=cortex))\n",
    "\n",
    "# cortex_atlases_dir = '/data/simexp/fnadeau/nilearn_atlases/'\n",
    "# cortex_atlases = [get_cortex_atlas(atlas) for atlas in atlases]\n",
    "# cortex_atlases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di1024_cortex_coords = niplot.find_probabilistic_atlas_cut_coords(cortex_atlases[0].maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_roi0 = nimage.smooth_img(nimage.mean_img(nimage.index_img(cortex_atlases[0].maps, [0])), 8 )\n",
    "roi_data = np.array([img.get_fdata().flatten() for img in\n",
    "                     tqdm_(list(nimage.iter_img(cortex_atlases[0].maps)))])\n",
    "distances = pairwise_distances(roi_data)\n",
    "distances=pd.DataFrame(distances,\n",
    "                       index=cortex_atlases[0].labels.difumo_names,\n",
    "                       columns=cortex_atlases[0].labels.difumo_names)\n",
    "distances = distances.where(distances.values!=np.tril(distances.values))\n",
    "nearest_rois = pd.DataFrame(distances[distances==distances.min()].stack().index.tolist())\n",
    "# neighbors = [nearest_rois.groupby(0).get_group(grp) for grp in nearest_rois.groupby(0).groups]\n",
    "X = session.computed_.signal_matrix\n",
    "corr_mat = X.corr('spearman')\n",
    "corr_mat = corr_mat.where(corr_mat.values!=np.tril(corr_mat.values))\n",
    "constants = pd.DataFrame(corr_mat[corr_mat > 0.9].stack().index.tolist())\n",
    "region_clusters = pd.DataFrame([val for val in constants.values\n",
    "                                if val in nearest_rois.values])\n",
    "cluster_dict = dict(tuple((grp, [v for v in\n",
    "                                 flatten(region_clusters.groupby(0).get_group(grp).values.tolist())\n",
    "                                 if v != grp])\n",
    "                          for grp in region_clusters.groupby(0).groups))\n",
    "cluster_names = [flatten(item) for item in tuple(cluster_dict.items())]\n",
    "cluster_ids = [cortex_atlases[0].labels.loc[[name[0] for name in\n",
    "                                            enumerate(cortex_atlases[0].labels.difumo_names.tolist())\n",
    "                          if name[1] in cluster]].index.tolist()\n",
    "               for cluster in cluster_names]\n",
    "\n",
    "cluster_maps = [nimage.mean_img(nimage.index_img(cortex_atlases[0].maps, cluster_id))\n",
    "                for cluster_id in cluster_ids]\n",
    "name_enum = enumerate(cortex_atlases[0].labels.difumo_names.tolist())\n",
    "unclustered = cortex_atlases[0].labels.loc[[name[0] for name in name_enum\n",
    "                                            if name[1] not in flatten(cluster_names)]]\n",
    "unclustered_names, unclustered_ids = unclustered.difumo_names.tolist(), unclustered.index.tolist()\n",
    "\n",
    "unclustered_maps = nimage.index_img(cortex_atlases[0].maps, unclustered_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "not_cortex = di1024.labels.reset_index(drop=False).set_index('difumo_names').T.filter(\n",
    "                 regex='|'.join(no_cortex))\n",
    "cortex = di1024.labels.reset_index(drop=False).set_index(\n",
    "             'difumo_names').T.drop(not_cortex.columns, axis=1).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_masker512 = NiftiMapsMasker(maps_img=cortex_atlases[1].maps,\n",
    "                              mask_img=session.mask_img,\n",
    "                              t_r=get_t_r(session.fmri_img),\n",
    "                              resampling_target='mask',\n",
    "                              **session.masker_defs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_cols=['trial_type', 'recognition_performance',\n",
    "                 'ctl_miss_ws_cs']\n",
    "session.glm_defs.update({'signal_scaling': False})\n",
    "session.computed_ = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                               events=session.events,\n",
    "                               masker=session.masker,\n",
    "                               ctl_id='Ctl',\n",
    "                               output_type='effect_size',\n",
    "                               trial_type_cols=trial_type_cols,\n",
    "                               standardize=True,\n",
    "                               scale=False,\n",
    "                               maximize=False,\n",
    "                               glm_kws=session.glm_defs,\n",
    "                               design_kws=session.design_defs,\n",
    "                               feature_labels=)\n",
    "\n",
    "\n",
    "print(maps_masker512.signal_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "linear_svc = next(LinearSVCGen())\n",
    "tasks = [session.events.trial_type,\n",
    "         session.events.recognition_performance.replace({'Miss':'Fail'}),\n",
    "         session.events.iloc[:, -1]]\n",
    "\n",
    "display(*[validate_model(linear_svc,\n",
    "                         X=session.computed_.signal_matrix,\n",
    "                         y=tasks[task[0]],\n",
    "                         test_size=0.8,\n",
    "                         stratify=tasks[task[0]],\n",
    "                         shuffle=True).round(2)\n",
    "         for task in enumerate(tasks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dir(linear_svc))\n",
    "# help(linear_svc.score)\n",
    "# linear_svc.predict(X)\n",
    "# linear_svc.__dict__.keys()\n",
    "help(explained_variance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(explained_variance_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = session.computed_.signal_matrix.copy(deep=True)\n",
    "y = tasks[2]\n",
    "X = X.set_axis(y, axis=0)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X.loc['Cs'])\n",
    "\n",
    "pca.explained_variance_ratio_, \n",
    "\n",
    "var_table = pd.DataFrame(pca.components_, columns=pca.feature_names_in_)#.max().sort_values(ascending=False)\n",
    "# pca.singular_values_.shape\n",
    "# linear_svc.fit(X, y)\n",
    "\n",
    "# # y_pred = cross_val_predict(linear_svc, X, y, groups=y)\n",
    "# y_pred = linear_svc.predict(X)\n",
    "# evs = explained_variance_score(y_true=y, y_pred=y_pred)\n",
    "# evs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.var().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_classif(estimator,\n",
    "                    X: Iterable, y: Iterable,\n",
    "                    test_size: float = 0.8,\n",
    "                    cv: Union[int, callable] = None,\n",
    "                    stratify: Iterable = None,\n",
    "                    random_state: int = None,\n",
    "                    **kwargs\n",
    "                    ) -> float:\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "    validation_params = dict(test_size=test_size, shuffle=True,\n",
    "                             stratify=stratify, random_state=None)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        **validation_params)\n",
    "\n",
    "#     estimator.fit(X_train, y_train)\n",
    "    cv_score = cross_val_predict(estimator.fit(X_train, y_train),\n",
    "                                 X_test, y_test, groups=y_test, cv=cv)\n",
    "    return round((len(list(filter(None, cv_score == y_test))) /\n",
    "                  len(y_test)), 2)\n",
    "#     cr_test = pd.DataFrame(classification_report(y_pred=cv_score,\n",
    "#                                                  y_true=y_test,\n",
    "#                                                  output_dict=True,\n",
    "#                                                  zero_division=0)).accuracy.mean()\n",
    "    return cr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions.rena_clustering import recursive_neighbor_agglomeration, ReNA\n",
    "from nilearn.regions import Parcellations\n",
    "# help(ReNA)\n",
    "\n",
    "rena = Parcellations(method='rena', n_parcels=X.shape[1]/2,\n",
    "                     standardize=False, smoothing_fwhm=2.,\n",
    "                     scaling=True)\n",
    "\n",
    "\n",
    "\n",
    "# rena = ReNA(session.masker._resampled_mask_img_, n_clusters=3, scaling=False,\n",
    "#             n_iter=10, threshold=1e-07)\n",
    "\n",
    "rena.fit(list(nimage.iter_img(fmri4d)))\n",
    "# help(rena.fit)\n",
    "# rena_clusters = recursive_neighbor_agglomeration(X.T.values,\n",
    "#                                               session.mask_img, 3)\n",
    "# session.masker.__dict__\n",
    "# help(rena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl_(X, method: str = 'spearman', thresh: float = 0.9):\n",
    "    mat = trimmed_corr_mat(X, method=method, thresh=0.9)\n",
    "    clusters = []\n",
    "    for row in mat.iterrows():\n",
    "        names_ = [row[0]]+[str(ind) for ind \n",
    "                           in row[1].dropna().index]\n",
    "        feature_ = pd.Series(data=X[names_].T.mean().values,\n",
    "                             index=X.index, name=' '.join(names_))\n",
    "        mat.drop(names_, axis=1, inplace=True)\n",
    "        clusters.append(feature_)\n",
    "    return clusters, mat\n",
    "\n",
    "clusters, mat = cl_(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_ = yield pd.concat([mknew_(X, flatten((row[0], list(row[1].dropna().to_dict()))))\n",
    "                       for row in trimmed_corr_mat(X, method=method, thresh=thresh).iterrows()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure as CM\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from cimaq_decoding_utils import factorGenerator\n",
    "\n",
    "cm = CM(kind='correlation').fit_transform([X.values])\n",
    "# sorted(cm.__dict__.keys())\n",
    "aggc = AgglomerativeClustering(n_clusters=None,\n",
    "                               connectivity=cm[0],\n",
    "                               compute_full_tree=True,\n",
    "                               linkage='ward',\n",
    "                               distance_threshold=-np.inf,\n",
    "                               compute_distances=True).fit(X.corr('spearman'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggc.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_ = pd.concat([mknew_(X, flatten((row[0], list(row[1].dropna().to_dict()))))\n",
    "                       for row in trimmed_corr_mat(X).iterrows()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "# help(spearmanr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trimmed_corr_mat(clusters_)\n",
    "\n",
    "clusters_2 = pd.concat([mknew_(clusters_, flatten((row[0], list(row[1].dropna().to_dict()))))\n",
    "                       for row in trimmed_corr_mat(clusters_).iterrows()], axis=1)\n",
    "\n",
    "clusters_3 = pd.concat([mknew_(clusters_2, flatten((row[0], list(row[1].dropna().to_dict()))))\n",
    "                       for row in trimmed_corr_mat(clusters_2).iterrows()], axis=1)\n",
    "\n",
    "clusters_4 = pd.concat([mknew_(clusters_3, flatten((row[0], list(row[1].dropna().to_dict()))))\n",
    "                       for row in trimmed_corr_mat(clusters_3).iterrows()], axis=1)\n",
    "clusters_4\n",
    "# X.[spearmanr(X).correlation>0.9]\n",
    "# help(pairwise_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([len(c) for c in PairwiseClusterNames(X)])\n",
    "\n",
    "clusters_ = [pwc+[row[0] for row in X.T.iterrows() if\n",
    "             scipy.stats.spearmanr(row[1], mknew_(X, pwc)).correlation > 0.9]\n",
    "             for pwc in tqdm_(PairwiseClusterNames(X))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "len(tuple(dict(Counter(flatten(clusters_)).most_common()).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy.ward(trimmed_corr_mat(X)[trimmed_corr_mat(X)>0.9].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dir(hierarchy))\n",
    "# help(hierarchy.ClusterNode)\n",
    "def make_clusters(X, thresh:float=0.9):\n",
    "    idx = pd.DataFrame(trimmed_corr_mat(X, thresh=thresh).stack().index.tolist()).groupby(0).groups\n",
    "    return [pd.Series(data=X[flatten([itm[0], X.iloc[:, itm[1]].columns.tolist()])].T.mean(),\n",
    "                      index=X.index, name=)\n",
    "             for itm in tuple(idx.items())]\n",
    "\n",
    "make_clusters(X)\n",
    "# leaves_list, leaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# dist_linkage = hierarchy.ward(X.corr('spearman'))\n",
    "cluster_ids = hierarchy.fcluster(hierarchy.ward(trimmed_corr_mat(X)[trimmed_corr_mat(X)>0.9].fillna(0)), 1)\n",
    "dict(tuple((grp,\n",
    "            flatten(pd.DataFrame(tuple(zip(cluster_ids, X.columns))).groupby(0).get_group(\n",
    "                grp).set_index(0).values.tolist()))\n",
    "           for grp in pd.DataFrame(tuple(zip(cluster_ids, X.columns))).groupby(0).groups))\n",
    "# hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cimaq_decoding_pipeline import get_optimal_features\n",
    "def get_optimal_features(X: [np.ndarray, pd.DataFrame]\n",
    "                         ) -> list:\n",
    "    \"\"\"\n",
    "    Return optimal features using hierarchical clustering.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.cluster import hierarchy\n",
    "    from scipy.spatial.distance import squareform\n",
    "    from collections import defaultdict\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        corr = X.corr(method='spearman').fillna(0).values\n",
    "        np.fill_diagonal(corr, 1)\n",
    "    else:\n",
    "        corr = spearmanr(X).correlation\n",
    "        # Ensure the correlation matrix is symmetric\n",
    "        corr = (corr + corr.T) / 2\n",
    "        np.fill_diagonal(corr, 1)\n",
    "        np.nan_to_num(corr, 0)\n",
    "    # Converting the correlation matrix to a distance matrix\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    # hierarchical clustering using Ward's linkage\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "\n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "    cluster_id_to_feature_ids = defaultdict(list)\n",
    "\n",
    "    for idx, cluster_id in enumerate(cluster_ids):\n",
    "        cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "    return selected_features\n",
    "\n",
    "get_optimal_features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PairwiseClusters(X=MakePairwise(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def PairwiseClusters(X, method: str = 'spearman',\n",
    "                     thresh=0.90):\n",
    "    X = X.copy(deep=True)\n",
    "    pairs = list(PairwiseCorrelates(X, method, thresh))\n",
    "    new_ = pd.concat([mknew_(X, pair)\n",
    "             for pair in pairs], axis=1)\n",
    "    X.drop(flatten(pairs), axis=1, inplace=True)\n",
    "    X = pd.concat([new_, X], axis=1)\n",
    "#     new_pairs = list(PairwiseCorrelates(X_new, method, thresh))\n",
    "#     new_ = pd.concat((new_, pd.concat([mknew_(X_new, pair)\n",
    "#                       for pair in new_pairs], axis=1)), axis=1)\n",
    "    return X\n",
    "\n",
    "X = session.computed_.signal_matrix.copy(deep=True)\n",
    "PairwiseClusters(X=PairwiseClusters(X))\n",
    "#     names_ = next(pairs)\n",
    "#     new_ = mknew_(X, names_)\n",
    "#     X.drop(names_, axis=1, inplace=True)\n",
    "    \n",
    "#     names_ = list(next(pairs) for n in range(step))\n",
    "#     new_ = pd.concat([mknew_(Xcp, pair) for pair in pairs], axis=1)\n",
    "#     Xcp.drop(flatten(pairs), axis=1, inplace=True)\n",
    "#     return pd.concat([new_, Xcp], axis=1)\n",
    "\n",
    "\n",
    "# def IterativePairwiseClusters(X, method: str = 'spearman',\n",
    "#                               thresh=0.90, step=1):\n",
    "    \n",
    "# #     gen = yield PairwiseClusters(X, method, thresh)\n",
    "#     for n in tqdm_(range(step)):\n",
    "#         X = PairwiseClusters(method=method, thresh=thresh)\n",
    "#     return X\n",
    "# \n",
    "# X = X.set_axis(tasks[2])\n",
    "# step = 3\n",
    "# for _ in tqdm_(range(step)):\n",
    "#     X_new = PairwiseClusters(X)\n",
    "#     X = PairwiseClusters(X_new)\n",
    "# X\n",
    "# pairs = list(PairwiseCorrelates(X_new))\n",
    "# # flatten(pairs)\n",
    "# # any([name in X_new.columns for name in flatten(pairs)])\n",
    "# X_new.drop(flatten(pairs), axis=1)\n",
    "# new_ = pd.concat([mknew_(X_new, pair) for pair in pairs], axis=1)\n",
    "# new_.shape, X_new.drop(flatten(pairs), axis=1).shape\n",
    "# list(PairwiseCorrelates(X_new)).__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trim = trimmed_corr_mat(X)[trimmed_corr_mat(X)>0.9].dropna(\n",
    "             how='all', axis=1).dropna(how='all', axis=0)\n",
    "to_group = [flatten((row[1].name, row[1].dropna().round(2).index.tolist()))\n",
    "            for row in X_trim.iterrows()]\n",
    "new_ = pd.concat([mknew_(X, g) for g in to_group], axis=1)\n",
    "\n",
    "\n",
    "# pd.concat((new_, X.drop(flatten(to_group), axis=1)), axis=1)\n",
    "# # [len(g) for g in to_group]\n",
    "X_trim_new = trimmed_corr_mat(new_)[trimmed_corr_mat(new_)>0.9].dropna(\n",
    "             how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "to_group_new = [flatten((row[1].name, row[1].dropna().round(2).index.tolist()))\n",
    "                for row in X_trim_new.iterrows()]\n",
    "new_new = pd.concat([mknew_(new_, g) for g in to_group_new], axis=1)\n",
    "PairwiseClusterNames(new_new)\n",
    "# new_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3from itertools import combinations\n",
    "triplets = list(map(list, list(combinations(maps_masker512.signal_matrix.columns, 3))))\n",
    "len(triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_roi = pd.Series(((validate_classif(next(LinearSVCGen()),\n",
    "                                     X=maps_masker512.signal_matrix[col].values.reshape(-1, 1),\n",
    "                                     y=tasks[2], test_size=0.4,\n",
    "                                     stratify=tasks[2],\n",
    "                                     shuffle=True)\n",
    "                    for col in tqdm_(maps_masker512.signal_matrix.columns))),\n",
    "                     index = maps_masker512.signal_matrix.columns)\n",
    "best_roi[best_roi==best_roi.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(pairs), pd.Series(tuple(map(tuple, pairs))).nunique()\n",
    "\n",
    "roi0 = Counter(itm[0] for itm in\n",
    "        [pd.Series(pairs).loc[pairwise_results[\n",
    "            pairwise_results>0.99].index].values.tolist()][0]).most_common(1)[0][0]\n",
    "roi1 = Counter(itm[1] for itm in [pd.Series(pairs).loc[pairwise_results[\n",
    "    pairwise_results>0.99].index].values.tolist()][0]\n",
    "        if itm[0] == 'precentral sulcus mid-inferior rh').most_common(1)[0][0]\n",
    "\n",
    "(roi0, roi1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "\n",
    "\n",
    "# ovrc = LogisticRegression(max_iter=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di1024 = get_difumo(1024, 3, atlases_dir)\n",
    "\n",
    "\n",
    "not_cortex = di1024.labels.reset_index(drop=False).set_index('difumo_names').T.filter(\n",
    "                 regex='|'.join(no_cortex))\n",
    "cortex = di1024.labels.reset_index(drop=False).set_index(\n",
    "             'difumo_names').T.drop(not_cortex.columns, axis=1).T\n",
    "\n",
    "cortex_atlas = nimage.index_img(di1024.maps, cortex.component.tolist())\n",
    "cortex_names = cortex.index.tolist()\n",
    "Path('/data/simexp/fnadeau/cortex-difumo-693-labels.txt').write_text('\\n'.join(cortex_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_id, ses_id, task, space = os.path.basename(session.fmri_path).split('_')[:-2]\n",
    "\n",
    "prefix = '_'.join(os.path.basename(session.fmri_path).split('_')[:-2])\n",
    "\n",
    "sorted(Path(masker_dir).rglob(f'{sub_id}_{ses_id}*.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix\n",
    "corr_mat = X.corr('spearman')\n",
    "corr_mat = corr_mat.where(corr_mat.values!=np.tril(corr_mat.values))\n",
    "constants = pd.DataFrame(corr_mat[corr_mat > 0.9].stack().index.tolist())\n",
    "activation_neighbors = [constants.groupby(0).get_group(grp)\n",
    "                        for grp in constants.groupby(0).groups]\n",
    "\n",
    "len(activation_neighbors), max([len(n) for n in activation_neighbors])\n",
    "# pd.DataFrame(distances[distances==distances.min()].stack().index.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n in activation_neighbors if n.shape[0] == 9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_ntable = pd.concat(activation_neighbors)#.set_index(0).sort_index()\n",
    "# activation_ntable[activation_ntable[0]==activation_ntable[1]]\n",
    "neighbortable = pd.concat(neighbors)#.set_index(0).sort_index()\n",
    "# display(activation_ntable, neighbortable)\n",
    "# neighbortable.where(neighbortable[0] != neighbortable[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "common_atable = activation_ntable.loc[activation_ntable.index.intersection(neighbortable.index)]\n",
    "comon_ntable = neighbortable.loc[neighbortable.index.intersection(activation_ntable.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session.computed_.signal_matrix.drop(unclustered_names, axis=1)\n",
    "# (X<0).any().any()\n",
    "# X.min().min()\n",
    "# session.computed_.whole.contrast_img.get_fdata().min().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(flatten(cluster_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps_masker = NiftiMapsMasker(maps_img=cortex_atlas,\n",
    "#                               mask_img=session.mask_img,\n",
    "#                               t_r=get_t_r(session.fmri_img),\n",
    "#                               resampling_target='mask',\n",
    "#                               **session.masker_defs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nilearn.connectome import ConnectivityMeasure as CM\n",
    "# from sklearn.manifold import MDS\n",
    "# from  sklearn.decomposition import PCA\n",
    "# from cimaq_decoding_utils import factorGenerator\n",
    "\n",
    "# X = session.computed_.signal_matrix\n",
    "# X_factors = list(factorGenerator(X.shape[1]))\n",
    "\n",
    "# cm = CM(kind='correlation')\n",
    "# connect_mat = cm.fit_transform([session.computed_.signal_matrix.values])[0]\n",
    "\n",
    "# mds = MDS(n_components=connect_mat.shape[1],\n",
    "#           metric=True,\n",
    "#           max_iter=X.shape[0], eps=1e-12,\n",
    "#           random_state=sample(range(10), 1)[0],\n",
    "#           dissimilarity=\"precomputed\",\n",
    "#           n_jobs=11)\n",
    "\n",
    "# mds2 = MDS(n_components=X.shape[1],\n",
    "#            metric=True,\n",
    "#            max_iter=X.shape[0], eps=1e-12,\n",
    "#            random_state=sample(range(10), 1)[0],\n",
    "#            dissimilarity=\"euclidean\",\n",
    "#            n_jobs=11)\n",
    "\n",
    "# # X_factors\n",
    "# # connect_mat.shape\n",
    "# mds.fit(connect_mat)\n",
    "# mds2.fit(X)\n",
    "\n",
    "# embedding = pd.DataFrame(mds.embedding_, index=X.columns,\n",
    "#                          columns=X.columns)\n",
    "\n",
    "# dsm = pd.DataFrame(mds.dissimilarity_matrix_,\n",
    "#                    index=X.columns, columns=X.columns)\n",
    "\n",
    "\n",
    "# embedding2, dsm2 = mds2.embedding_, mds2.dissimilarity_matrix_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_inputs(step=3):\n",
    "    _ = 0\n",
    "    while _ < step:\n",
    "        x = yield\n",
    "        yield x * 2\n",
    "    _ += 1\n",
    "        \n",
    "gen = double_inputs(step=4)\n",
    "next(gen)\n",
    "gen.send(2)\n",
    "# next(gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "[p for p in sorted(Path(os.path.dirname(pd.__file__)).rglob('*.py'))\n",
    " if re.match('yield', p.read_text())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from itertools import chain, takewhile, tee, repeat\n",
    "# sorted(dir(itertools))\n",
    "help(repeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IterativePairwiseClusters(X, *,method: str = 'spearman',\n",
    "                              thresh=0.90, step=1):\n",
    "    rez = {}\n",
    "    for _ in range(step):\n",
    "        pairs = list(PairwiseCorrelates(X, method=method, thresh=thresh))\n",
    "#         [rez.update(mknew_(X, pair))  for pair in pairs]\n",
    "# #         new_ = pd.concat([mknew_(X, pair) for pair in pairs], axis=1)\n",
    "#         X.drop(flatten(pairs), axis=1, inplace=True)\n",
    "#         [rez.update(item) for item in X.iteritems()]\n",
    "#     return rez\n",
    "#         X = pd.concat([new_, X], axis=1)\n",
    "#     return pd.DataFrame.from_dict(rez, orient='columns')\n",
    "\n",
    "#     for _ in range(step):\n",
    "        \n",
    "#         X = PairwiseClusters(X, method=method, thresh=thresh)\n",
    "#     return X\n",
    "#     X = PairwiseClusters(X=X, method=method, thresh=thresh)\n",
    "#     gen = (PairwiseClusters(X, method=method, thresh=thresh)\n",
    "#            for _ in range(step))\n",
    "      \n",
    "    \n",
    "#     X_new = PairwiseClusters(X=X, method=method, thresh=thresh)\n",
    "#     next(gen)\n",
    "#     (gen.send(PairwiseClusters(X_new, method=method, thresh=thresh))\n",
    "#      for _ in range(step))\n",
    "#     return X_new\n",
    "#     for _ in tqdm_(range(step)):\n",
    "#         next(gen).send(gen)\n",
    "#     X = PairwiseClusters(X, method, thresh)\n",
    "#     :\n",
    "#         X = PairwiseClusters(X, method, thresh)\n",
    "#     return X\n",
    "\n",
    "gen = IterativePairwiseClusters(X, step=3)\n",
    "# display(*tuple(gen))\n",
    "# tuple(gen)\n",
    "gen\n",
    "\n",
    "# help(gen.throw)\n",
    "# display(*[next(gen).shape for i in range(3)])\n",
    "# gen.send(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import defer\n",
    "\n",
    "@defer.inlineCallbacks\n",
    "def doStuff():\n",
    "    result = yield takesTwoSeconds()\n",
    "    nextResult = yield takesTenSeconds(result * 10)\n",
    "    defer.returnValue(nextResult / 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.reporting import get_clusters_table\n",
    "\n",
    "cluster_table = [get_clusters_table(img, 0.05, cluster_threshold=None,\n",
    "                   two_sided=False, min_distance=8.0)\n",
    "                 for img in nimage.iter_img(fmri4d)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_table = list(filter(lambda x: x.shape[0] != 0, cluster_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csgraph, coo_matrix, dia_matrix\n",
    "# help(coo_matrix)\n",
    "from nilearn.regions.rena_clustering import recursive_neighbor_agglomeration as rnagg\n",
    "help(rnagg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.9\n",
    "\n",
    "X_new = PaiwiseClusters(X)\n",
    "# X_corr = trimmed_corr_mat(X)\n",
    "\n",
    "\n",
    "# X_drop = X_corr[X_corr > thresh].dropna(\n",
    "#              how='all', axis=0).dropna(how='all', axis=1)\n",
    "# # X_drop.iloc[4]\n",
    "# # X_drop.loc[X_drop.T.notna().sum().sort_values(ascending=False).index[0]].dropna().index\n",
    "\n",
    "# to_combine = [row[1].dropna().index.tolist() for row in X_drop.iterrows()]\n",
    "# to_combine\n",
    "# # 'lateral occipital cortex inferior lh' in X_drop.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "#         gen = yield gen.send(gen)\n",
    "#     return gen\n",
    "#     yield\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "@defer.inlineCallbacks\n",
    "def doStuff():\n",
    "    result = yield takesTwoSeconds()\n",
    "    nextResult = yield takesTenSeconds(result * 10)\n",
    "    defer.returnValue(nextResult / 10)\n",
    "\"\"\"\n",
    "#     clusters_ = (PairwiseClusters(X, method, thresh))\n",
    "#     test = clusters.send\n",
    "#     new_features_ = mknew_\n",
    "#     new_feature = pd.Series(data=X[corr_list].T.mean().values,\n",
    "#                             index=X.index, name=' '.join(test00))\n",
    "    \n",
    "#     new_features = pd.concat([pd.Series(data=X[list(pair)].T.mean().values,\n",
    "#                               index=X.index, name=' '.join(pair))\n",
    "#                     for pair in corr_list], axis=1)\n",
    "#     yield pd.concat([new_features, X.drop(flatten(corr_list), axis=1)], axis=1) \n",
    "    \n",
    "#     to_combine = (flatten(chunk) for chunk in chunks(corr_list, step))\n",
    "#     new_feature = next(pd.Series(data=X[flatten(chunk)].T.mean().values,\n",
    "#                                   index=X.index, name=' '.join(chunk))\n",
    "#                         for chunk in chunks(corr_list, step))\n",
    "#     X_new = X.drop(to_combine, axis=1)\n",
    "#     to_combine = flatten(corr_list[:step])\n",
    "#     new_feature = pd.Series(data=X[to_combine].T.mean().values,\n",
    "#                             index=X.index, name=' '.join(to_combine))\n",
    "#     X_new = X.drop(to_combine, axis=1)\n",
    "#     return new_feature\n",
    "\n",
    "# X.shape[1] - \n",
    "PairwiseClusters(PairwiseClusters(X))\n",
    "# test02 = PairwiseClusters(X, thresh=0.90, step=6)\n",
    "# trimmed_corr_mat(test02)[test02.iloc[:, :5].columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = PairwiseClusters(X, thresh=0.90, step=3)\n",
    "\n",
    "test01 = [mknew_(X, pair) for pair in pairs]\n",
    "new_ = pd.concat(test01, axis=1)\n",
    "X_tmp = X.drop(flatten(pairs), axis=1)\n",
    "X_new01 = pd.concat([new_, X_tmp], axis=1)\n",
    "list(PairwiseCorrelates(X_new01))\n",
    "# any([col in flatten(list(PairwiseCorrelates(X_new01))) for col in new_.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_cs = X.set_axis(tasks[2], axis=0).loc['Cs'].corr('spearman').where(\n",
    "                  (X.set_axis(tasks[2], axis=0).loc['Cs'].corr('spearman').values !=\n",
    "                   np.triu(X.set_axis(tasks[2], axis=0).loc['Cs'].corr('spearman').values)))\n",
    "\n",
    "const_pos_cs = trimmed_cs[trimmed_cs > 0.9].stack().index.tolist()\n",
    "const_neg_cs = trimmed_cs[trimmed_cs < -0.9].stack().index.tolist()\n",
    "\n",
    "trimmed_ws = X.set_axis(tasks[2], axis=0).loc['Ws'].corr('spearman').where(\n",
    "                 (X.set_axis(tasks[2], axis=0).loc['Ws'].corr('spearman').values\n",
    "                  != np.triu(X.set_axis(tasks[2], axis=0).loc['Ws'].corr('spearman').values)))\n",
    "    \n",
    "const_pos_ws = trimmed_cs[trimmed_ws > 0.9].stack().index.tolist()\n",
    "const_neg_ws = trimmed_cs[trimmed_ws < -0.9].stack().index.tolist()\n",
    "\n",
    "\n",
    "len(const_pos_cs), len(const_neg_ws)\n",
    "# [c for c in const_pos_cs if c[0] in [c2[0] for c2 in const_neg_ws]]\n",
    "# ('postcentral sulcus medial', 1, 'superior temporal gyrus anterior medial rh')\n",
    "# ('postcentral sulcus medial', -1, 'middle frontal gyrus mid-posterior superior rh')\n",
    "\n",
    "# \n",
    "# const_neg_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos00 = clusterize(X, tasks[0], 'spearman', 0.9,\n",
    "                                positive=True, n_deci=2)\n",
    "neg00 = clusterize(X, tasks[0], 'spearman', -0.9,\n",
    "                                positive=False, n_deci=2)\n",
    "pos01 = clusterize(X, tasks[1], 'spearman', 0.9,\n",
    "                                positive=True, n_deci=2)\n",
    "neg01 = clusterize(X, tasks[1], 'spearman', -0.9,\n",
    "                                positive=False, n_deci=2)\n",
    "pos02 = clusterize(X, tasks[2], 'spearman', 0.9,\n",
    "                                positive=True, n_deci=2)\n",
    "neg02 = clusterize(X, tasks[2], 'spearman', -0.9,\n",
    "                                positive=False, n_deci=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix.drop(constants, axis=1)\n",
    "\n",
    "\n",
    "# conditions = X.index.unique()\n",
    "pos_clusters = Bunch()\n",
    "[pos_clusters.update(clusterize(X, task[1], 'spearman', 0.9,\n",
    "                                positive=True, n_deci=2))\n",
    " for task in enumerate(tasks)]\n",
    "\n",
    "neg_clusters = Bunch()\n",
    "[neg_clusters.update(clusterize(X, task[1], 'spearman', -0.9,\n",
    "                                positive=False, n_deci=2))\n",
    " for task in enumerate(tasks)]\n",
    "\n",
    "neg_rois = flatten([flatten([itm[0] for itm in tuple(item[1].items())]\n",
    "                 for item in tuple(neg_clusters[key].items()))\n",
    "         for key in tuple(neg_clusters.keys())])\n",
    "\n",
    "pos_rois = flatten([flatten([itm[0] for itm in tuple(item[1].items())]\n",
    "                 for item in tuple(pos_clusters[key].items()))\n",
    "         for key in tuple(pos_clusters.keys())])\n",
    "\n",
    "all_rois = set(neg_rois+pos_rois)\n",
    "\n",
    "len(all_rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clusterize2ways(X, y,\n",
    "                    method: str = 'spearnan',\n",
    "                    thresh: float = 0.9,\n",
    "                    n_deci: int = 32\n",
    "                    ) -> Bunch:\n",
    "                    \n",
    "    pos_clusters = Bunch()\n",
    "    [pos_clusters.update(clusterize(X, y,\n",
    "                                    method, thresh,\n",
    "                                    positive=True,\n",
    "                                    n_deci=n_deci))\n",
    "     for task in enumerate(tasks)]\n",
    "\n",
    "    neg_clusters2 = Bunch()\n",
    "    [neg_clusters2.update(clusterize(X[all_rois], task[1],\n",
    "                                     'spearman', -0.9,\n",
    "                                     positive=False, n_deci=2))\n",
    "     for task in enumerate(tasks)]\n",
    "\n",
    "    neg_rois2 = flatten([flatten([itm[0] for itm in tuple(item[1].items())]\n",
    "                                 for item in tuple(neg_clusters2[key].items()))\n",
    "             for key in tuple(neg_clusters2.keys())])\n",
    "\n",
    "    pos_rois2 = flatten([flatten([itm[0] for itm in tuple(item[1].items())]\n",
    "                                 for item in tuple(pos_clusters2[key].items()))\n",
    "             for key in tuple(pos_clusters2.keys())])\n",
    "\n",
    "    all_rois2 = set(neg_rois2+pos_rois2)\n",
    "len(all_rois2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(neg_rois)), len(set(pos_rois)), len(set(all_rois))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(*[validate_model(ovrc,\n",
    "                         X=session.computed_.signal_matrix[all_rois],\n",
    "                         y=tasks[task[0]], test_size=0.2,\n",
    "                         stratify=tasks[task[0]],\n",
    "                         shuffle=True)\n",
    "         for task in enumerate(tasks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.computed_.signal_matrix[all_rois].shape[1]*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Convert to function ###\n",
    "\n",
    "# tasks = (session.events.trial_type,\n",
    "#          session.events.recognition_performance.replace({'Miss': 'Fail'}),\n",
    "#          session.events.iloc[:, -1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*rfecv_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*bwd_reduced_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfecv_matrices[1].columns==rfecv_matrices[2].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [session.events.trial_type, session.events.recognition_performance,\n",
    "         session.events.iloc[:, -1]]\n",
    "\n",
    "display(*[validate_model(ovrc,\n",
    "                         X=bwd_reduced_matrices[task[0]],\n",
    "                        y=tasks[task[0]],\n",
    "                        test_size=0.8,\n",
    "                        stratify=tasks[task[0]],\n",
    "                        shuffle=True)\n",
    "         for task in enumerate(tasks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks[-1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings.shape\n",
    "# help(PCA)\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "pca.__dict__\n",
    "\n",
    "# help(np.randint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "EPSILON = np.finfo(np.float32).eps\n",
    "\n",
    "seed = np.random.RandomState(seed=3)\n",
    "# X_true = seed.randint(0, 20, 2 * n_samples).astype(float)\n",
    "X_true = session.computed_.signal_matrix.values\n",
    "n_samples = X_true.shape[0]\n",
    "# X_true = X_true.reshape((n_samples, 2))\n",
    "# Center the data\n",
    "X_true -= X_true.mean()\n",
    "\n",
    "similarities = euclidean_distances(X_true)\n",
    "\n",
    "# Add noise to the similarities\n",
    "noise = np.random.rand(n_samples, n_samples)\n",
    "noise = noise + noise.T\n",
    "noise[np.arange(noise.shape[0]),\n",
    "      np.arange(noise.shape[0])] = 0\n",
    "similarities += noise\n",
    "\n",
    "mds = manifold.MDS(n_components=2, max_iter=3000,\n",
    "                   eps=1e-12, random_state=seed,\n",
    "                   dissimilarity=\"precomputed\",\n",
    "                   n_jobs=1)\n",
    "\n",
    "pos = mds.fit(similarities).embedding_\n",
    "\n",
    "nmds = manifold.MDS(n_components=2, metric=False,\n",
    "                    max_iter=3000, eps=1e-12,\n",
    "                    dissimilarity=\"precomputed\",\n",
    "                    random_state=seed,\n",
    "                    n_jobs=1, n_init=1)\n",
    "\n",
    "npos = nmds.fit_transform(similarities, init=pos)\n",
    "\n",
    "# Rescale the data\n",
    "pos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((pos ** 2).sum())\n",
    "npos *= np.sqrt((X_true ** 2).sum()) / np.sqrt((npos ** 2).sum())\n",
    "\n",
    "# Rotate the data\n",
    "clf = PCA(n_components=2)\n",
    "X_true = clf.fit_transform(X_true)\n",
    "\n",
    "pos = clf.fit_transform(pos)\n",
    "\n",
    "npos = clf.fit_transform(npos)\n",
    "\n",
    "fig = plt.figure(1)\n",
    "ax = plt.axes([0.0, 0.0, 1.0, 1.0])\n",
    "\n",
    "s = 100\n",
    "plt.scatter(X_true[:, 0], X_true[:, 1], color=\"navy\",\n",
    "            s=s, lw=0, label=\"True Position\")\n",
    "plt.scatter(pos[:, 0], pos[:, 1], color=\"turquoise\", s=s, lw=0, label=\"MDS\")\n",
    "plt.scatter(npos[:, 0], npos[:, 1], color=\"darkorange\",\n",
    "            s=s, lw=0, label=\"NMDS\")\n",
    "plt.legend(scatterpoints=1, loc=\"best\", shadow=False)\n",
    "\n",
    "similarities = similarities.max() / (similarities + EPSILON) * 100\n",
    "np.fill_diagonal(similarities, 0)\n",
    "# Plot the edges\n",
    "start_idx, end_idx = np.where(pos)\n",
    "# a sequence of (*line0*, *line1*, *line2*), where::\n",
    "#            linen = (x0, y0), (x1, y1), ... (xm, ym)\n",
    "segments = [[X_true[i, :], X_true[j, :]]\n",
    "            for i in range(len(pos))\n",
    "            for j in range(len(pos))\n",
    "            ]\n",
    "values = np.abs(similarities)\n",
    "lc = LineCollection(segments, zorder=0, cmap=plt.cm.Blues,\n",
    "                    norm=plt.Normalize(0, values.max()))\n",
    "\n",
    "lc.set_array(similarities.flatten())\n",
    "lc.set_linewidths(np.full(len(segments), 0.5))\n",
    "ax.add_collection(lc)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix\n",
    "X.values.reshape(X.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divmod(session.fmri_img.shape[-1], 2)\n",
    "# session.computed_.keys()\n",
    "# session.masker.__dict__.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sns.heatmap(\n",
    "# submap = session.masker._resampled_maps_img_\n",
    "# submap.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_cut_coords = niplot.find_probabilistic_atlas_cut_coords(submap)\n",
    "# connectome_plot = niplot.plot_connectome(connect_mat[0], sub_cut_coords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cpath = '/home/fnadeau/.linuxbrew/Cellar/isl/0.24/include/isl/hmap_templ.c'\n",
    "\n",
    "# print(Path(cpath).read_bytes().splitlines()[2:])\n",
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.masker._resampled_mask_img_.shape, session.masker._resampled_maps_img_.shape\n",
    "# # session.mask_path\n",
    "# 693/7\n",
    "# \n",
    "# session.computed_.keys()\n",
    "# inv_test = session.masker.inverse_transform(session.computed_.signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions import Parcellations\n",
    "\n",
    "parcells = Parcellations('ward', n_parcels=231, random_state=0,\n",
    "                         mask=session.mask_img,\n",
    "                         smoothing_fwhm=None, standardize=False, detrend=False,\n",
    "                         low_pass=None, high_pass=None, t_r=session.t_r,\n",
    "                         target_affine=session.masker._resampled_maps_img_.affine,\n",
    "                         target_shape=session.mask_img.shape,\n",
    "                         mask_strategy='whole-brain-template',\n",
    "                         scaling=False, n_iter=10, n_jobs=11)\n",
    "parcells.fit(inv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sorted(dir(parcells))\n",
    "# help(parcells)\n",
    "sig_test = parcells.transform(inv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niplot.view_img(nimage.mean_img(parcells.components_img_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "dict(inspect.getmembers(parcells))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(connect_mat[0],\n",
    "             index=session.computed_.signal_matrix.columns,\n",
    "             columns=session.computed_.signal_matrix.columns).round(1)==session.computed_.signal_matrix.corr().round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_test = CM(kind='correlation', discard_diagonal=True).fit([session.computed_.signal_matrix.values]).inverse_transform([session.computed_.signal_matrix.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testb = CM(kind='correlation').fit([cm_test[0]]).inverse_transform([cm_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix\n",
    "dsm0 = X.corr('spearman')\n",
    "dsm1 = pairwise_distances(X)\n",
    "dsm2 = pairwise_distances(dsm0)\n",
    "\n",
    "# pd.DataFrame(dsm0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROI-Based Classification\n",
    "\n",
    "\n",
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "validation_params = dict(test_size=0.4, shuffle=True,\n",
    "                         random_state=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(session.computed_.signal_matrix.T.values,\n",
    "                                                    session.computed_.signal_matrix.columns,\n",
    "#                                                     session.computed_.signal_matrix.in.tolist(),\n",
    "                                                    **validation_params)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# len(list(filter(None, ovrc.predict(X_test)==y_test)))/len(y_test)\n",
    "logreg.predict(X_test) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.regions import Parcellations, ReNA\n",
    "\n",
    "\"\"\"\n",
    "score(self, imgs, confounds=None, per_component=False)\n",
    "     Score function based on explained variance on imgs.\n",
    "\"\"\"\n",
    "\n",
    "# rena = ReNA(mask_img=session.mask_img,\n",
    "#             n_clusters=divmod(session.fmri_img.shape[-1], 2)[0],\n",
    "#             scaling=False, n_iter=10, threshold=1e-07)\n",
    "\n",
    "# new_clusters = \n",
    "# rena.fit(session.computed_.signal_matrix.corr('spearman'))\n",
    "# parcels = Parcellations(method='ward', n_parcels=divmod(session.computed_.signal_matrix.shape[1], 2)[0],\n",
    "#                         smoothing_fwhm=None, mask=session.masker._resampled_mask_img_,\n",
    "#                         mask_strategy='gm-template')\n",
    "# parcels.fit(session.masker._resampled_maps_img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_contrasts(fmai_img,\n",
    "                           events=None,\n",
    "                           design_matrices=None\n",
    "                           ) -> Bunch:\n",
    "    list(itertools.combinations(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cbptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg.__dict__['coef_'].shape\n",
    "display(pd.DataFrame(logreg.predict_log_proba(X), columns=logreg.classes_,\n",
    "             index=tasks[2]).describe(),\n",
    "        pd.DataFrame(logreg.predict_proba(X), columns=logreg.classes_,\n",
    "             index=tasks[2]),\n",
    "        pd.DataFrame(logreg.decision_function(X),\n",
    "                     index=tasks[2], columns=logreg.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[row[1].mean() for row in session.computed_.signal_matrix.iterrows()].__len__()\n",
    "from sklearn.metrics import explained_variance_score\n",
    "# help(explained_variance_score)\n",
    "# from sklearn.decomposition import PCA\n",
    "# help(PCA)\n",
    "ovrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(session.computed_.signal_matrix.T.corr('spearman'))\n",
    "\n",
    "# \n",
    "\n",
    "# sns.heatmap(session.computed_.signal_matrix.set_axis(\n",
    "#     tasks[0], axis=0).sort_index().T.set_axis(tasks[2],axis=0).sort_index()\n",
    "\n",
    "sns.heatmap(bwd_reduced_matrices[2].set_axis(tasks[1],axis=0).sort_index().corr('spearman'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcels.variance_.shape\n",
    "# niplot.view_img(parcels.labels_img_)\n",
    "# help(parcels.connectivity_)\n",
    "# parcels.nifti_maps_masker_\n",
    "test = parcels.inverse_transform(session.computed_.signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.shape\n",
    "# parcels.components_img_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(*[validate_model(ovrc, rfecv_matrices[task[0]], tasks[task[0]])\n",
    "          for task in enumerate(tasks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*rfecv_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.computed_.signal_matrix.corr('spearman')\n",
    "from collections import Counter\n",
    "\n",
    "def clusterize(X: Union[np.ndarray, pd.DataFrame],\n",
    "               thresh: float = 0.,\n",
    "               ) -> pd.DataFrame:\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    corrmat = X.corr('spearman').where(X.corr('spearman').values !=\n",
    "                                       np.tril(X.corr('spearman').values))\n",
    "\n",
    "    cluster_names = dict([itm for itm in [(row[0], row[1].where(\n",
    "                        row[1].values >= thresh).dropna().index.tolist())\n",
    "                                for row in corrmat.iterrows()]\n",
    "                          if itm[1] != []])\n",
    "#     return [pd.Series(itm[1]).value_counts(ascending=False)]\n",
    "    return cluster_names\n",
    "\n",
    "# [session.computed_.signal_matrix[]\n",
    "clusters = clusterize(session.computed_.signal_matrix)\n",
    "clusters\n",
    "# cluster_counts = [pd.Series(data=item[1], name=item[0]).value_counts(ascending=False)\n",
    "#                   for item in tuple(clusters.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any([itm.__contains__('superior occipital gyrus inferior rh') for itm in tuple(clusters.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = session.computed_.signal_matrix\n",
    "\n",
    "# Spearman Rank Correlation\n",
    "Xcorr = X.corr('spearman')\n",
    "# Remove null or near-zero variance features\n",
    "Xcorr.where(Xcorr.var().between(-0.05, 0.05)).dropna(axis=0, how='all')\n",
    "\n",
    "# Xcorr.where(Xcorr.corr('spearman').values !=\n",
    "#                                 np.tril(Xcorr.corr('spearman').values))\n",
    "\n",
    "# sns.heatmap(pairwise_distances(Xcorr.fillna(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from nilearn.regions import Parcellations, ReNA, RegionExtractor\n",
    "\n",
    "\n",
    "# cluster_lists = [cortex.loc[[itm[0]]+[itm[1]]] for itm in cluster_list]\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 12), dpi=80)\n",
    "\n",
    "from cimaq_decoding_utils import flatten\n",
    "cortex['component']=range(cortex.shape[0])\n",
    "\n",
    "cluster_maps = [nimage.index_img(cortex_atlas,\n",
    "                        cortex.loc[flatten(cluster)].component.tolist())\n",
    "                for cluster in cluster_list]\n",
    "# niplot.plot_prob_atlas(cl03, display_mode='mosaic')\n",
    "# plt.show()\n",
    "# help(nilearn.regions.ReNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niplot.plot_roi(nimage.index_img(session.masker.maps_img, [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = session.computed_.signal_matrix\n",
    "y = session.events.iloc[:, -1]\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "def classify_kbest_clustering(X: Union[np.ndarray, pd.DataFrame],\n",
    "                              y: Iterable = None,\n",
    "                              thresh: float = 0.75,\n",
    "                              metric: str = 'spearman',\n",
    "                              maps_img=cortex_atlas,\n",
    "                              labels=cortex,\n",
    "                              dst: Union[str, PathLike,\n",
    "                                         PosixPath] = None,\n",
    "                              ) -> dict:\n",
    "\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "    from sklearn.metrics import pairwise_distances\n",
    "    from scipy.spatial.distance import correlation\n",
    "    from scipy.stats import spearmanr\n",
    "    from cimaq_decoding_utils import flatten\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    cluster_names = clusterize(X, thresh)\n",
    "    \n",
    "    if dst is not None:\n",
    "        with open(dst, mode='w') as jfile:\n",
    "            json.dump(cluster_names, jfile)\n",
    "            jfile.close()\n",
    "\n",
    "    cluster_names_ = [flatten(item) for item in\n",
    "                     tuple(cluster_names.items())]\n",
    "\n",
    "    cluster_means_ = pd.concat([X[clust].mean(axis=1)\n",
    "                                for clust in cluster_names_],\n",
    "                               axis=1).values\n",
    "    \n",
    "    cluster_means = pd.DataFrame(cluster_means_,\n",
    "                                 index=X.index,\n",
    "                                 columns=[clust[0] for clust\n",
    "                                          in cluster_names_])    \n",
    "\n",
    "    unclustered = X.copy(deep=True)\n",
    "    unclustered.drop(flatten(cluster_names_),\n",
    "                     axis=1, inplace=True)        \n",
    "\n",
    "    new_signals = pd.concat([cluster_means, unclustered], axis=1)\n",
    "\n",
    "    \n",
    "    new_atlas = cluster_maps_labels(maps_img=maps_img, labels=labels,\n",
    "                                    cluster_names=cluster_names_)\n",
    "\n",
    "    return {'cluster_matrix': new_signals,\n",
    "            'cluster_maps': new_atlas,\n",
    "            'cluster_names': cluster_names}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_maps_labels(maps_img: Nifti1Image,\n",
    "                        labels: pd.DataFrame,\n",
    "                        cluster_names: list,\n",
    "                        ) -> Bunch:\n",
    "\n",
    "    from nilearn.image import concat_imgs, index_img, mean_img\n",
    "    from cimaq_decoding_utils import flatten\n",
    "\n",
    "    no_cluster = [label for label in labels\n",
    "                  if label not in flatten(cluster_names)]\n",
    "\n",
    "    no_cluster_idx = [itm[0] for itm in enumerate(labels)\n",
    "                      if itm[1] in no_cluster]\n",
    "    no_cluster_names = no_cluster\n",
    "    no_cluster_maps = [nimage.index_img(maps_img, idx)\n",
    "                       for idx in no_cluster_idx]\n",
    "    no_cluster_values = list(zip(no_cluster_idx,\n",
    "                                 no_cluster_names,\n",
    "                                 no_cluster_maps))\n",
    "\n",
    "    cluster_idx = [itm for itm in range(labels.shape[0])\n",
    "                   if itm not in no_cluster_idx]\n",
    "    cluster_names_ = [itm[0] for itm in cluster_names]\n",
    "    cluster_maps = [mean_img(index_img(maps_img, clust_idx))\n",
    "                    for clust_idx in cluster_idx]\n",
    "    cluster_values = list(zip(cluster_idx, cluster_names_,\n",
    "                              cluster_maps))\n",
    "\n",
    "    whole_values = no_cluster_values + cluster_values\n",
    "    whole_values = sorted(whole_values)\n",
    "\n",
    "    new_labels = pd.DataFrame((((itm[0], itm[1]))\n",
    "                               for itm in whole_values))\n",
    "    new_maps_img = concat_imgs([itm[2] for itm in whole_values])\n",
    "\n",
    "    return Bunch(**dict(maps=new_maps_img, labels=new_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.update(**classify_kbest_clustering(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_reduced_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_masker = NiftiMapsMasker(maps_img=session.cluster_maps.maps,\n",
    "                                mask_img=session.mask_img,\n",
    "                                resampling_target='mask',\n",
    "                                **session.masker_defs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_fmri = cluster_masker.inverse_transform(session.cluster_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# svc = SVC(kernel='linear',\n",
    "#           cache_size=5,\n",
    "#           decision_function_shape='ovr',\n",
    "#           class_weight='balanced',\n",
    "#           probability=True)\n",
    "# linsvc = LinearSVC(class_weight='balanced')\n",
    "# linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "# ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "# rfecv_params = dict(estimator=ovrc, step=1,\n",
    "#                     cv=None, n_jobs=11,\n",
    "#                     importance_getter='auto',\n",
    "#                     min_features_to_select=1,\n",
    "#                     scoring='accuracy')\n",
    "\n",
    "# # nftselect = lambda n: next((n+1 if n==1 else)) \n",
    "# mds = Pipeline([('rfecv', RFECV(**rfecv_params)),\n",
    "#                 ('sfsf', SequentialFeatureSelector(ovrc, n_jobs=11,\n",
    "#                                                    n_features_to_select=0.80,\n",
    "#                                                    scoring='accuracy',\n",
    "#                                                    direction='forward')),\n",
    "#                 ('sfsb', SequentialFeatureSelector(ovrc, n_jobs=11,\n",
    "#                                                    n_features_to_select=0.80,\n",
    "#                                                    scoring='accuracy',\n",
    "#                                                    direction='backward'))])\n",
    "\n",
    "# pipeline_matrices = [session.cluster_matrix[mds.fit(session.cluster_matrix,\n",
    "#                                                     tasks[task[0]]).get_feature_names_out()]\n",
    "#                      for task in tqdm_(enumerate(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "rfecv_params = dict(step=1,\n",
    "                    cv=None, n_jobs=11,\n",
    "                    importance_getter='auto',\n",
    "                    min_features_to_select=4,\n",
    "                    scoring='accuracy')\n",
    "sfs_params = dict(n_jobs=11, n_features_to_select=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redux_matrix = [session.cluster_matrix[SequentialFeatureSelector(\n",
    "                    ovrc, direction='backward', **sfs_params).fit(\n",
    "                        session.cluster_matrix[SequentialFeatureSelector(\n",
    "                            ovrc, direction='forward', **sfs_params).fit(\n",
    "                                session.cluster_matrix[RFECV(\n",
    "                                    ovrc, **rfecv_params).fit(\n",
    "                                         session.cluster_matrix,\n",
    "                                    tasks[task[0]]).get_feature_names_out()],\n",
    "                            tasks[task[0]]).get_feature_names_out()],\n",
    "                tasks[task[0]]).get_feature_names_out()]\n",
    "                for task in tqdm_(enumerate(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*redux_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.cluster_maps.maps.shape\n",
    "# tuple(session.computed_.keys())\n",
    "base_contrasts = [session.computed_['whole']['contrast_img'],\n",
    "                  session.computed_['trial_type']['contrast_img'],\n",
    "                  session.computed_['recognition_performance']['contrast_img'],\n",
    "                  session.computed_['ctl_miss_ws_cs']['contrast_img']]\n",
    "[img.shape for img in base_contrasts]\n",
    "\n",
    "# [type(session.computed_[key]) for key in tuple(session.computed_.keys())]\n",
    "# [(key, session.computed_[key]['contrast_img']) for key in tuple(session.computed_.keys())[:3]\n",
    "#  if 'contrast_img' in tuple(session.computed_[key].keys())\n",
    "#  and isinstance(session.computed_[key], dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display([mat.shape for mat in rfecv_matrices],\n",
    "        [mat.shape for mat in fwd_reduced_matrices],\n",
    "        [mat.shape for mat in bwd_reduced_matrices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# import sklearn\n",
    "\n",
    "# SVCGen = get_class(sklearn.svm,'SVC', cls_kws=dict(kernel='linear',\n",
    "#                                      cache_size=5,\n",
    "#                                     decision_function_shape='ovr',\n",
    "#                                     class_weight='balanced',\n",
    "#                                     probability=True))\n",
    "\n",
    "# help(SVCGen.send)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # next(get_class('','OneVsRestClassifier'))\n",
    "# # # sorted(dir(sklearn.multiclass))\n",
    "# def get_class(module_name, class_name, cls_kws=None):\n",
    "#     if cls_kws is None:\n",
    "#         cls_kws = {}\n",
    "#     yield dict(inspect.getmembers(module_name))[class_name](**cls_kws)\n",
    "# # \n",
    "# def gen_svc():\n",
    "#     while True:\n",
    "#         yield next(get_class(sklearn.svm,'SVC', cls_kws=dict(kernel='linear',\n",
    "#                                      cache_size=5,\n",
    "#                                     decision_function_shape='ovr',\n",
    "#                                     class_weight='balanced',\n",
    "#                                     probability=True)))\n",
    "# next(gen_svc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mat.shape for mat in fwd_reduced_matrices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwd_reduced_matrices = [fwd_reduced_matrices[task[0]][SequentialFeatureSelector(ovrc, n_jobs=11,\n",
    "                                                                                direction='backward').fit(\n",
    "                            fwd_reduced_matrices[task[0]], tasks[task[0]]).get_feature_names_out()]\n",
    "                        for task in tqdm_(enumerate(tasks))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*fwd_reduced_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sp_cor = pairwise_distances(X.corr('spearman'))\n",
    "pd.DataFrame(sp_cor)\n",
    "tests = [X.corr('spearman'),\n",
    "        (1-X.corr('spearman')),\n",
    "        pd.DataFrame(pairwise_distances(X.T),\n",
    "                     index=X.columns,columns=X.columns),\n",
    "        pd.DataFrame(pairwise_distances(X.corr('spearman')),\n",
    "                     index=X.columns,columns=X.columns),\n",
    "        pd.DataFrame(1-pairwise_distances(X.corr('spearman')),\n",
    "                     index=X.columns,columns=X.columns)]\n",
    "\n",
    "cluster_names_test = [dict([itm for itm in [(row[0], row[1].where(\n",
    "                    row[1].values>=thresh).dropna().index.tolist())\n",
    "                            for row in test.iterrows()]\n",
    "                      if itm[1] != []])\n",
    "                 for test in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cluster_maps = list(zip(cortex.loc[no_cluster].component.values,\n",
    "                           nimage.index_img(cortex_atlas,\n",
    "                                            cortex.loc[no_cluster].component.values)))\n",
    "# cortex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_maps_masker = NiftiMapsMasker(maps_img=cortex_atlas3,\n",
    "                                  mask_img=session.mask_img,\n",
    "                                  t_r=session.t_r,\n",
    "                                  resampling_target='mask',\n",
    "                                  **session.masker_defs).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_brain_map = alt_maps_masker.inverse_transform(newsignals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_cluster_maps = list(in)\n",
    "cluster_maps = [nimage.mean_img(nimage.index_img(cortex_atlas, cortex.loc[cluster].component))\n",
    "                for cluster in cluster_names]\n",
    "\n",
    "\n",
    "whole_idx = no_cluster_idx+list(zip(cluster_maps_idx, cluster_maps))\n",
    "whole_idx = sorted(whole_idx)\n",
    "newsignals = pd.concat([pd.concat([X[clust].mean(axis=1) for clust in\n",
    "                        cluster_names], axis=1),\n",
    "                         X.copy(deep=True).drop(flatten(cluster_names), axis=1)],axis=1)\n",
    "newsignals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_maps_masker = NiftiMapsMasker(maps_img=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selkbest_multi(k, x, y):\n",
    "    yield from (selkbest(k, X, task[1]) for task in enumerate(y))\n",
    "\n",
    "\n",
    "def get_kbest_multilabels(estimator, k, X, y, thresh=0.95, nbootstrap=10):\n",
    "    cond = lambda e, k, mod, y, thresh: (validate_model(e, mod, y).accuracy <= thresh).all()\n",
    "    while True:\n",
    "        rfe = RFE(estimator, n_features_to_select=k)\n",
    "        \n",
    "list(get_kbest_multilabels(ovrc, 4, X, tasks, thresh=0.75, nbootstrap=100))\n",
    "# [x[0] for x in scores if x[1] == max(list(x[1] for x in scores))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intgen(estimator, k, cond: callable = lambda e,x,y: validate_model(e, X, y).accuracy.all() >= thresh):\n",
    "    X_new = SelectKBest\n",
    "    while cond(estimator, X, y):\n",
    "        print(cond.accuracy.mean())\n",
    "        k +=1\n",
    "        \n",
    "        continue\n",
    "        break\n",
    "intgen(ovrc, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "y = tasks\n",
    "estimator = ovrc\n",
    "thresh = 0.95\n",
    "condi = lambda e, X, y, t: validate_model(e, X, y) <= t\n",
    "models = ([validate_model(selkbest(k, X, task[1]))\n",
    "           for task in enumerate(y)]\n",
    "          for k in tqdm_(range(k, X.shape[1])))\n",
    "# X_new = next(models)\n",
    "# while condi(estimator, X_new, y, ):\n",
    "#     continue\n",
    "    \n",
    "# while validate_model(estimator, X, y).accuracy.all() <= thresh:\n",
    "#     next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = [(X.iloc[:, SelectKBest(k=k).fit(X, task[1]).get_support(indices=True)]\n",
    "#            for k in range(k, X.shape[1]))\n",
    "#           for task in enumerate(y)]\n",
    "# scores = [validate_model(estimator, next(result[task[0]]), task[1]) <= thresh\n",
    "#         for task in enumerate(y)]\n",
    "# while [score.any()<=thresh for score in scores]:\n",
    "#     k+=1\n",
    "#     print(k)\n",
    "#     continue\n",
    "#     break\n",
    "    \n",
    "\n",
    "    \n",
    "def ordertest(A):\n",
    "    for i in range( len(A) - 1 ):\n",
    "        if A[i] < A[i+1]:\n",
    "            return i\n",
    "        return True\n",
    "\n",
    "    \n",
    "def val_gen(estimator, k, X, y):\n",
    "    scores = [[]*len(y)]\n",
    "    nk = []\n",
    "    for task in enumerate(y):\n",
    "        validity = (validate_model(estimator,\n",
    "                                    next(selkbest(k, X, y[task[0]])),\n",
    "                                    y[task[0]]).accuracy.mean()\n",
    "                    for k in range(k, X.shape[1]))\n",
    "    \n",
    "        for idx in range(1, X.shape[1]):\n",
    "            scores.append(next(validity))\n",
    "            if k > 2:\n",
    "                if scores[task[0]][idx-1]<scores[idx]:\n",
    "                    continue\n",
    "                else:\n",
    "                    nk.append(k)\n",
    "\n",
    "    return nk\n",
    "#     scores.append(next(val_gen))\n",
    "#     if \n",
    "#     for task in enumerate(y):\n",
    "        \n",
    "#     if [v < thresh for v in validity]:\n",
    "#         k+=1\n",
    "#         continue\n",
    "#     else:\n",
    "#         return validity\n",
    "    \n",
    "tst=val_gen(ovrc, 2, X, tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_gen(estimator, k, X, y, thresh):\n",
    "\n",
    "    scores = [val_gen(estimator, k=k, X=X, y=task[1], thresh=thresh)\n",
    "              for task in tqdm_(enumerate(y))]\n",
    "    return scores\n",
    "\n",
    "\n",
    "# conseq_decrease = lambda l: [l[i] <= l[i+1] for i in range(len(l))]\n",
    "\n",
    "conseq_decrease(list(score_gen(estimator=ovrc, k=2, X=X, y=tasks, thresh=0.9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb(estimator, k, X, y, thresh=0.9):\n",
    "    k=k\n",
    "    acc = [validate_model(estimator=estimator,\n",
    "                                    X=next(selkbest(k, X, task[1])),\n",
    "                          y=task[1]).accuracy.mean()\n",
    "           for task in tqdm_(enumerate(y))]\n",
    "    while  np.mean(acc) < thresh:\n",
    "        k+=1\n",
    "        continue\n",
    "\n",
    "        break\n",
    "    scores.append(k)\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "#             <= thresh).any():\n",
    "#         for k in tqdm_(range(k, X.shape[1])):\n",
    "#             if cond.any():\n",
    "#             k+=1\n",
    "#             continue\n",
    "#         else:\n",
    "#             return k\n",
    "\n",
    "kb(estimator=ovrc, k=2, X=X, y=tasks, thresh=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from cimaq_decoding_utils import flatten\n",
    "\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "def selkbest(k, X, y):\n",
    "    yield X.iloc[:, SelectKBest(k=k).fit(X, y).get_support(indices=True)]\n",
    "    \n",
    "def get_kbest(estimator, k, X, y, thresh=0.95, nbootstrap=10):\n",
    "    \n",
    "    cond = lambda e, k, mod, y, thresh: (validate_model(e, mod, y).accuracy <= thresh).all()\n",
    "    scores = []\n",
    "    for nboot in tqdm_(range(nbootstrap)):\n",
    "        while True:\n",
    "            mod = next(selkbest(k, X, y))\n",
    "            k += 1\n",
    "            if not cond(estimator, k, mod, y, thresh):\n",
    "                break\n",
    "        scores.append((mod, validate_model(estimator, mod, y).accuracy.mean()))\n",
    "    common_shape = Counter(list(x[0].shape[1] for x in scores)).most_common(1)[0][0]\n",
    "    return common_shape\n",
    "#     scores = [x for x in scores if x[0].shape[1] == common_shape]\n",
    "#     return scores\n",
    "#     return X.iloc[:, RFE(estimator).fit([x[0] for x in scores\n",
    "#             if x[1] == max(x[1] for x in scores)][0], y).get_support(indices=True)]\n",
    "#     feature_names = flatten([mod.columns.tolist() for mod in scores])\n",
    "#     \n",
    "    \n",
    "#     return X[[x[0] for x in Counter(feature_names).most_common(min_shape+1)]]\n",
    "#     return [score for score in scores if\n",
    "#             score.shape == min(list(x.shape for x in scores))][0]\n",
    "\n",
    "def get_kbest_bootstrap(estimator, k, X, y, thresh=0.95, nboot=10):\n",
    "    from collections import Counter\n",
    "    nk = Counter((get_kbest(estimator, k, X, y, thresh)\n",
    "                    for n in tqdm_(range(nboot)))).most_common(1)[0][0]\n",
    "    return X.iloc[:, SelectKBest(k=nk).fit(X, y).get_support(indices=True)]\n",
    "#     cond = (validate_model(estimator, next(selkbest(k, X, y)), y).accuracy <=0.95).all()\n",
    "#     if cond:\n",
    "#         k += 1\n",
    "#     else:\n",
    "#         return k\n",
    "get_kbest(svc, k=2, X=session.computed_.signal_matrix,\n",
    "                    y=session.events.iloc[:, -1],#recognition_performance,#iloc[:, -1],#recognition_performance,\n",
    "                    thresh=0.90, nbootstrap=100)\n",
    "#           nboot=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     if cv_kws is None:\n",
    "#         cv_kws = {}\n",
    "#     if cv is not None:\n",
    "#         cv = cv(**cv_kws)\n",
    "\n",
    "#         X_00 = min_err(X, y)\n",
    "#         skb = SelectKBest(k=divmod(X_00.shape[1], 2)[0]).fit(X_00, y)\n",
    "#         _X_new_ = X.iloc[:, skb.get_support(indices=True)]\n",
    "\n",
    "#         X_new = X.iloc[:, rfe.get_support(indices=True)]\n",
    "\n",
    "        \n",
    "#         rfecv = RFECV(estimator, step=step,\n",
    "#                       cv=cv, n_jobs=n_jobs,\n",
    "#                       importance_getter='auto',\n",
    "#                       min_features_to_select=min_features_to_select,\n",
    "#                       scoring=scoring).fit(X.copy(deep=True), task[1])\n",
    "\n",
    "#     return X.iloc[:, rfecv.get_support(indices=True)]\n",
    "\n",
    "#     rfe = RFE(estimator, step=1,\n",
    "#               importance_getter='auto',\n",
    "#               n_features_to_select=0.5).fit(X_best, y)\n",
    "#     return X_best.iloc[:, rfe.get_support(indices=True)]\n",
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "# sdgc = SGDClassifier(class_weight='balanced', n_jobs=11)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "recursive_best = [get_optimal_features_recursive(ovrc, kbest_by_task[task[0]],\n",
    "                                                 task[1], step=1, n_jobs=11)\n",
    "                  for task in enumerate(tasks)]\n",
    "\n",
    "display(*recursive_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_names = list(set(flatten(list(rbest.columns.tolist() for rbest in recursive_best))))\n",
    "recursive_idx = cortex.loc[recursive_names].component.values.tolist()\n",
    "recursive_maps = nimage.index_img(cortex_atlas, recursive_idx)\n",
    "# recursive_maps.shape\n",
    "\n",
    "new_trial_imgs = maps_masker.fit_transform(signal_matrix, sample_mask=recursive_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst00 = [maps_masker.inverse_transform(rbest) for rbest in recursive_best]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_cluter2 = [classify_kbest_clustering(recursive_best[task[0]], task[1])\n",
    "#              for task in enumerate(tasks)]\n",
    "[x.columns for x in recursive_best]\n",
    "rbest0, rbest1, rbest2 = recursive_best\n",
    "intersect = (rbest0.columns.intersection(rbest1.columns).tolist() +\n",
    "             rbest1.columns.intersection(rbest2.columns).tolist() +\n",
    "             rbest0.columns.intersection(rbest2.columns).tolist())\n",
    "\n",
    "rcopy = [rbest.copy(deep=True) for rbest in recursive_best]\n",
    "\n",
    "[[rbest.drop(col, axis=1, inplace=True)\n",
    "  for col in intersect\n",
    "  if col in rbest.columns]\n",
    " for rbest in rcopy]\n",
    "\n",
    "display(*rcopy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[validate_model(ovrc, rcopy[task[0]], task[1])\n",
    "          for task in enumerate(tasks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builtins import FutureWaning, UserWarning\n",
    "from cimaq_decoding_utils import validate_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=FutureWaning,\n",
    "#                         module='sklearn.utils')\n",
    "# Set y values (label) for each classification task\n",
    "tasks = (session.events.trial_type,\n",
    "         session.events.recognition_performance,\n",
    "         session.events.iloc[:, -1])\n",
    "\n",
    "# Initialize estimators\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "sdgc = SGDClassifier(class_weight='balanced', n_jobs=11)\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "linovrc = OneVsRestClassifier(linsvc, n_jobs=11) \n",
    "ovrc = OneVsRestClassifier(svc, n_jobs=11)\n",
    "\n",
    "# Run cross-validation for each classification task\n",
    "validation_results = [validate_model(ovrc,\n",
    "                                     X=session.computed_.signal_matrix.values,\n",
    "                                     y=task.values,\n",
    "                                     test_size=0.2,\n",
    "                                     cv=StratifiedKFold(shuffle=True))\n",
    "                      for task in tasks]\n",
    "\n",
    "\n",
    "test_opt = [get_optimal_features_recursive(ovrc,\n",
    "                                           X=session.computed_.signal_matrix,\n",
    "                                           y=task,\n",
    "                                           min_features_to_select=1,\n",
    "                                           n_jobs=11,\n",
    "                                           scoring='accuracy',\n",
    "                                           step=1,\n",
    "                                           cv=StratifiedKFold(shuffle=True))\n",
    "            for task in tasks]\n",
    "\n",
    "# Run cross-validation for each classification task\n",
    "validation_results_new = [validate_model(ovrc,\n",
    "                                         X=test_opt[task[0]].values,\n",
    "                                         y=task[1].values,\n",
    "                                         test_size=0.2,\n",
    "                                         cv=StratifiedKFold(shuffle=True))\n",
    "                          for task in enumerate(tasks)]\n",
    "\n",
    "# Show classification results\n",
    "display(*test_opt, *validation_results, *validation_results_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (validate_model(svc, session.computed_.signal_matrix, session.events.trial_type).accuracy ==1).all()\n",
    "# np.array([['a','b',['c', 'd']]]).flatten()\n",
    "# from collections import Counter\n",
    "# session.computed_.signal_matrix\n",
    "# display(*kbest_by_task)\n",
    "display(*[validate_model(ovrc, kbest_by_task[task[0]], task[1])\n",
    "          for task in enumerate(tasks)])\n",
    "# [x[0] for x in Counter(list('asdfghhjj')).most_common(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst=[session.computed_.signal_matrix.iloc[:, SelectKBest(k=20).fit(session.computed_.signal_matrix,\n",
    "                                                            task).get_support(indices=True)]\n",
    "     for task in tasks]\n",
    "display(*[validate_model(ovrc, X=tst[task[0]], y=tasks[task[0]]) for task in enumerate(tst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(svc, X=get_kbest(svc, k=1, X=session.computed_.signal_matrix,\n",
    "                    y=session.events.iloc[:, -1],#recognition_performance,\n",
    "                    thresh=0.91, nbootstrap=400),\n",
    "               y=session.events.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([(itm.to_dict(),) for itm in session.computed_.matrices])\n",
    "# (session.computed_.matrices[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cortex.T.filter(regex='pedunc|chiasm|corticospinal').loc['component'].values\n",
    "# \n",
    "# list(factorGenerator(693))\n",
    "# divmod(693, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [name for name in cortex_names if 'tract' in name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# di1024 = get_difumo(1024, 3, '/data/simexp/fnadeau/nilearn_atlases/difumo_atlases/')\n",
    "# v03 = get_fmri_sessions(topdir=fmriprep_dir, events_dir=events_dir, ses_id=\"V03\")\n",
    "# sessions = [fetch_fmriprep_session(session=sess) for sess in tqdm_(v03)]\n",
    "# from cimaq_decoding_utils import save_masker\n",
    "# from builtins import FutureWarning\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "# [(sess.update({'masker': NiftiMapsMasker(maps_img=di1024.maps,\n",
    "#                                         mask_img=sess.mask_img,\n",
    "#                                         resampling_target='mask',\n",
    "#                                         t_r=get_t_r(sess.fmri_img),\n",
    "#                                         **sess.masker_defs).fit()}),\n",
    "# save_masker(masker_dir, session=sess)) for sess in tqdm_(sessions)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Audio\n",
    "# sound_file = './sound/beep.wav'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from cimaq_decoding_utils import get_masker, save_masker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masker_prefix = '_'.join([session.sub_id, session.ses_id,\n",
    "#                           f'task-{session.task}',\n",
    "#                           f'space-{session.space}'])\n",
    "# next(Path(masker_dir).rglob(f'*{masker_prefix}*.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "tasks = (session.events.trial_type,\n",
    "         session.events.recognition_performance,\n",
    "         session.events.iloc[:, -1])\n",
    "\n",
    "validation_results = [validate_model(ovrc,\n",
    "                                     X=session.computed_.regressed_matrix.values,\n",
    "                                     y=task.values,\n",
    "                                     test_size=0.2)\n",
    "#                                      cv=StratifiedKFold())\n",
    "                      for task in tasks]\n",
    "display(*validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.computed_.matrices[2].where(session.computed_.matrices[0].values<=0).isna().sum()\n",
    "# len(np.array(tasks[0]).shape)\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())\n",
    "\n",
    "# sklearn.metrics.SCORERS['accuracy']\n",
    "# help(sklearn.metrics.SCORERS['top_k_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.computed_.signal_matrix.where(session.computed_.signal_matrix.values<=0).isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(*[weight.where(weight.values<=0).isna().sum() for weight in session.computed_.weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(pd.concat([mat.reset_index(drop=False).iloc[:, 0:3] for mat in session.computed_.matrices], axis=1))\n",
    "# display(session.computed_.matrices[0],\n",
    "#         session.computed_.signal_matrix)\n",
    "#         session.computed_.weights)\n",
    "#         session.computed_.signal_matrix)\n",
    "help(warnings.filterwarnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# warnings.filterwarnings(action='ignore', category=UserWarning,\n",
    "#                         module='sklearn.model_selection')\n",
    "# warnings.filterwarnings(action='ignore', category=FutureWarning,\n",
    "#                         module='sklearn.utils')\n",
    "# y = session.events.iloc[:, -1]\n",
    "\n",
    "tasks = (session.events.trial_type,\n",
    "         session.events.recognition_performance,\n",
    "         session.events.iloc[:, -1])\n",
    "\n",
    "linsvc = LinearSVC(class_weight='balanced')\n",
    "\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "ovrc = OneVsRestClassifier(linsvc, n_jobs=11)\n",
    "\n",
    "# .fit(X=X, y=task)\n",
    "# from cimaq_decoding_utils import factorGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(*test_opt)\n",
    "# display(*[np.sum(np.mean(mat.loc['Ctl'].values)) for mat in session.computed_.weighted_matrices[1:]])\n",
    "ctl_weight = pd.concat([mat.loc[['Ctl']] for mat in session.computed_.weights]).mean()\n",
    "X_test = session.computed_.signal_matrix.set_axis(session.events.iloc[:, -1], axis=0)\n",
    "\n",
    "X_test = pd.DataFrame(((((row[1].values - ctl_weight)) if row[0]!='Ctl'\n",
    "               else row[1].values for row in X_test.iterrows())),\n",
    "             index=X_test.index, columns=X_test.columns)\n",
    "# X_test.where(X_test.loc['Ctl'].index=='Ctl')\n",
    "# X_test.where(X_test.loc['Ctl'].index=='Ctl')\n",
    "\n",
    "#                                         session.computed_.signal_matrix - ctl_weight)\n",
    "# X_test\n",
    "# ctl_weight.shape, session.computed_.signal_matrix.shape\n",
    "# session.computed_.signal_matrix.shape[1]/4\n",
    "# [idx for idx in enumerate(weigh.index)\n",
    "#  if idx[1] == 'Ctl']\n",
    "\n",
    "session.computed_.matrices[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builtins import FutureWarning\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from sklearn.feature_selection import RFECV, RFE\n",
    "# from sklearn.preprocessing import Binarizer\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# X = session.computed_.signal_matrix\n",
    "# # y = session.events.iloc[:, -1]\n",
    "\n",
    "# # tasks = (session.events.trial_type,\n",
    "# #          session.events.recognition_performance,\n",
    "# #          session.events.iloc[:, -1])\n",
    "\n",
    "# # .fit(X=X, y=task)\n",
    "\n",
    "# rec = RFECV(ovrc, step=3,\n",
    "#             min_features_to_select=2,\n",
    "#             scoring='accuracy')\n",
    "# X_best = [X.iloc[:, rec.fit(X, y).get_support(indices=True)]\n",
    "#           for y in tqdm_(tasks, desc='Recursively searching optimal features')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(*X_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_of = Bunch(**dict(tuple((task[1].name, test_opt[task[0]])\n",
    "                             for task in enumerate(tasks))))\n",
    "session.computed_.best_rois = best_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cimaq_decoding_utils import flatten\n",
    "cortex2 = cortex.reset_index().reset_index().set_index('difumo_names')\n",
    "# di1024.labels = di1024.labels.reset_index().reset_index().set_index('difumo_names')\n",
    "# cortex.where(cortex.index.tolist() ==\n",
    "#              flatten([x.columns.tolist() for x in X_best])).reset_index()\n",
    "# cortex.loc[flatten([x.columns.tolist() for x in X_best])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_atlases = [nimage.index_img(cortex_atlas,\n",
    "                                  cortex2.loc[tst.columns]['index'].values.tolist())\n",
    "                 for tst in tqdm_(test_opt)]\n",
    "\n",
    "trial_atlases_dict = Bunch(**dict(tuple((task[1].name, trial_atlases[task[0]])\n",
    "                                        for task in enumerate(tasks))))\n",
    "session.computed_.trial_best_features_imgs = trial_atlases_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_trial_imgs = Bunch(**dict(tuple((f'Condition Type {trial[0]}', Bunch(**dict(\n",
    "                            tuple((cond, maps_masker.inverse_transform(\n",
    "                                session.computed_.signal_matrix.set_axis(\n",
    "                                    tasks[trial[0]].values, axis=0).loc[cond].values)))\n",
    "                                  for cond in tqdm_(np.unique(tasks[trial[0]].values)))))\n",
    "                               for trial in tqdm_(enumerate(trial_atlases)))))\n",
    "session.computed_.wholebrain_trial_imgs = wholebrain_trial_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_ctl_imgs = Bunch(**dict(tuple((key, wholebrain_trial_imgs[key].Ctl)\n",
    "                                         for key in tuple(wholebrain_trial_imgs.keys()))))\n",
    "ctl_id = 'Ctl'\n",
    "\n",
    "[wholebrain_trial_imgs[key].pop(ctl_id)\n",
    " for key in tuple(wholebrain_trial_imgs.keys())]\n",
    "session.computed_.wholebrain_trial_imgs = wholebrain_trial_imgs\n",
    "session.computed_.wholebrain_ctl_imgs = wholebrain_ctl_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rois_trial_imgs = Bunch(**dict(tuple((f'Condition Type {trial[0]}', Bunch(**dict(\n",
    "                 tuple((cond, NiftiMapsMasker(maps_img=trial_atlases[trial[0]],\n",
    "                                              mask_img=session.mask_img,\n",
    "                                              resampling_target='mask',\n",
    "                                              t_r=get_t_r(session.fmri_img),\n",
    "                                              **session.masker_defs).fit().inverse_transform(\n",
    "                         test_opt[trial[0]].set_index(tasks[trial[0]].values).loc[cond].values)))\n",
    "                       for cond in tqdm_(np.unique(tasks[trial[0]].values)))))\n",
    "                               for trial in tqdm_(enumerate(trial_atlases)))))\n",
    "\n",
    "best_rois_ctl_imgs = Bunch(**dict(tuple((key, best_rois_trial_imgs[key].Ctl)\n",
    "                                         for key in tuple(best_rois_trial_imgs.keys()))))\n",
    "ctl_id = 'Ctl'\n",
    "\n",
    "[best_rois_trial_imgs[key].pop(ctl_id)\n",
    " for key in tuple(best_rois_trial_imgs.keys())]\n",
    "session.computed_.best_rois_trial_imgs = best_rois_trial_imgs\n",
    "session.computed_.best_rois_ctl_imgs = best_rois_ctl_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.computed_.best_rois_trial_imgs['Condition Type 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_decoding_utils import flatten\n",
    "condition_imgs = flatten([[session.computed_.best_rois_trial_imgs[key[1]][cond]\n",
    "                           for cond in tuple(session.computed_.best_rois_trial_imgs[key[1]].keys())]\n",
    "                          for key in enumerate(tuple(session.computed_.best_rois_trial_imgs.keys()))])\n",
    "\n",
    "condition_keys = flatten([[cond for cond in tuple(session.computed_.best_rois_trial_imgs[key[1]].keys())]\n",
    "                          for key in enumerate(tuple(session.computed_.best_rois_trial_imgs.keys()))])\n",
    "\n",
    "\n",
    "best_roi_condition_imgs = Bunch(**dict(tuple(zip(condition_keys, condition_imgs))))\n",
    "session.computed_.best_roi_condition_imgs = best_roi_condition_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(session.computed_.best_roi_condition_imgs[key[1]]),\n",
    "                          title=f'Task Condition {key[1]}',\n",
    "#                           display_type='mosaic',\n",
    "                          dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in enumerate(tuple(session.computed_.best_roi_condition_imgs.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img.shape for img in tuple(session.computed_.wholebrain_ctl_imgs.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(session.computed_.wholebrain_ctl_imgs[key[1]]),\n",
    "                          title=f'Control Condition {key[0]}',\n",
    "#                           display_type='mosaic',\n",
    "                          dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in enumerate(tuple(session.computed_.wholebrain_ctl_imgs.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(session.computed_.best_rois_ctl_imgs[key[1]]),\n",
    "                          title=f'Control Condition {key[0]}',\n",
    "#                           display_type='mosaic',\n",
    "                          dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in enumerate(tuple(session.computed_.best_rois_ctl_imgs.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_trial_imgs = [trial_imgs[key].Ctl for key in tuple(trial_imgs.keys())]\n",
    "[ctl_img.shape for ctl_img in control_trial_imgs], len(control_trial_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dict_keys(['whole', 'trial_type', 'recognition_performance',\n",
    "           'ctl_miss_ws_cs', 'matrices', 'weights',\n",
    "           'weighted_matrices', 'signal_matrix'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display(*[mat.iloc[:, 1:4] for mat in session.computed_.matrices])\n",
    "display(*session.computed_.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_trial_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_trial_imgs.ctl_trial_imgs = wholebrain_ctl_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholebrain_trial_imgs['Condition Type 1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_minus_ctl = \\\n",
    "    NiftiMapsMasker(maps_img=cortex_atlas,\n",
    "                    mask_img=session.mask_img,\n",
    "                    resampling_target='mask',\n",
    "                    t_r=get_t_r(session.fmri_img),\n",
    "                    **masker_defs).fit().inverse_transform(NiftiMapsMasker(maps_img=cortex_atlas,\n",
    "                    mask_img=session.mask_img,\n",
    "                    resampling_target='mask',\n",
    "                    t_r=get_t_r(session.fmri_img),\n",
    "                    **masker_defs).fit().transform_single_imgs(nimage.mean_img(wholebrain_trial_imgs['Condition Type 0'].Enc)) -\n",
    "     NiftiMapsMasker(maps_img=cortex_atlas,\n",
    "                    mask_img=session.mask_img,\n",
    "                    resampling_target='mask',\n",
    "                    t_r=get_t_r(session.fmri_img),\n",
    "                    **masker_defs).fit().transform_single_imgs(nimage.mean_img(wholebrain_trial_imgs['Condition Type 0'].Ctl)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(niplot.view_img(enc_minus_ctl, title=f'Encoding Minus Control',\n",
    "                        dim=-1, opacity=0.8, bg_img=session.anat_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(trial_imgs[key[1]].Ctl),\n",
    "                          title=f'Control Condition {key[0]}',\n",
    "                          display_type='mosaic',\n",
    "                          dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in enumerate(tuple(trial_imgs.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(trial_imgs[key[1]].Ctl),\n",
    "                          title=f'Control Condition {key[0]}',\n",
    "                          display_type='mosaic',\n",
    "                          dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in enumerate(tuple(trial_imgs.keys()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "control_mean_img = nimage.mean_img(nimage.concat_imgs([trial_imgs[key].Ctl\n",
    "                                                      for key in tuple(trial_imgs.keys())]))\n",
    "control_img = nimage.concat_imgs([trial_imgs[key].Ctl\n",
    "                                  for key in tuple(trial_imgs.keys())])\n",
    "\n",
    "niplot.view_img(nimage.mean_img(control_img),\n",
    "                opacity=0.8, \n",
    "                title='Control Trial Activity',\n",
    "                dim=-1, display_mode='mosaic',\n",
    "                bg_img=session.anat_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(*X_best)\n",
    "# sorted(dir(niplot))\n",
    "# help(niplot.plot_prob_atlas)\n",
    "control_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_trial_imgs = flatten([[nimage.mean_img(trial_imgs[key][trial])\n",
    "                            for trial in trial_imgs[key] if trial != 'Ctl']\n",
    "                   for key in tuple(trial_imgs.keys())])\n",
    "\n",
    "trial_keys = flatten([[f'{key}_{trial}' for trial in trial_imgs[key]\n",
    "              if trial != 'Ctl']\n",
    "              for key in tuple(trial_imgs.keys())])\n",
    "mean_trials = dict(tuple(zip(trial_keys, mean_trial_imgs)))\n",
    "\n",
    "display(*[niplot.view_img(mean_trials[key],\n",
    "                          title=key, dim=-1, opacity=0.8,\n",
    "                          bg_img=session.anat_img)\n",
    "         for key in tuple(mean_trials.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "# from sklearn.feature_selection import SelectFpr, SelectFdr, SelectKBest\n",
    "# from sklearn.feature_selection import VarianceThreshold, SelectFwe\n",
    "# ########################################################################\n",
    "# # from sklearn.feature_selection import GenericUnivariateSelect\n",
    "\n",
    "# X, y = session.computed_.signal_matrix, session.events.trial_type.tolist()\n",
    "\n",
    "\n",
    "# def select_low_error_features(X: Iterable, y: Iterable,\n",
    "#                               score_func: Union[callable,\n",
    "#                                                 str] = 'f_classif',\n",
    "#                               percentile: int = None,\n",
    "#                               alpha: float = 5e-2,\n",
    "#                               k: int = None,\n",
    "#                               **kwargs\n",
    "#                               ) -> np.ndarray:\n",
    "#     \"\"\"\n",
    "#     Return the common features from different error correction methods.\n",
    "    \n",
    "    \n",
    "#     The methods used control for FDR, FPR and FWE.\n",
    "#     \"\"\"\n",
    "\n",
    "#     from inspect import getmembers\n",
    "#     from sklearn import feature_selection\n",
    "#     if isinstance(score_func, str):\n",
    "#         score_func = dict(getmembers(feature_selection))[score_func]\n",
    "\n",
    "#     sfpr = SelectFpr(score_func, alpha=alpha)\n",
    "#     sfdr = SelectFdr(score_func, alpha=alpha)\n",
    "#     sfwe = SelectFwe(score_func, alpha=alpha)\n",
    "#     opt = []\n",
    "#     ests = (sfpr, sfdr, sfwe)\n",
    "#     if isinstance(X, np.ndarray):\n",
    "#         X = pd.DataFrame(X)\n",
    "#     [est.fit(X=X, y=y) for est in ests]\n",
    "#     [opt.extend(est.get_support(indices=True).tolist()) for est in ests]\n",
    "#     opt = list(set(opt))\n",
    "#     X_new = X.iloc[:, opt]\n",
    "# #     return X_new\n",
    "\n",
    "#     if percentile is not None:\n",
    "#         meta = SelectPercentile(score_func, percentile)\n",
    "#     if k is not None:\n",
    "#         meta = SelectKBest(score_func=score_func, k=k)\n",
    "#     opt_new = meta.fit(X_new, y).get_support(indices=True).tolist()\n",
    "#     if isinstance(X, np.ndarray):\n",
    "#         return X_new.iloc[:, opt_new].values\n",
    "#     else:\n",
    "#         return X_new.iloc[:, opt_new]\n",
    "\n",
    "#     # gus.fit(X, y).get_feature_names_out().shape\n",
    "# #     sfpr.fit(X=X, y=y), sfdr.fit(X=X, y=y), sfwe.fit(X=X, y=y)\n",
    "# #     best_idx, best_names = [], []\n",
    "# # #     if hasattr(sfpr, 'feature_names_in_'):\n",
    "# #     [best_names.extend(est.get_feature_names_out().tolist())\n",
    "# #      for est in ests]\n",
    "# #     best_names = list(set(flatten(best_names)))\n",
    "# #     return np.argsort(best_names), best_names\n",
    "\n",
    "# #     return X.iloc[:, np.argsort(best_names)], X[best_names]\n",
    "# #     X_new = sp.fit(X[best_names], y).get_feature_names_out()\n",
    "# #     return X_new\n",
    "\n",
    "# # #     else:\n",
    "# #     [best_idx.extend(np.argsort(est.transform(X)).tolist())\n",
    "# #      for est in (sfpr, sfdr, sfwe)]\n",
    "# #     best_idx = list(set(flatten(best_idx)))\n",
    "# #     X_new = sp.fit(pd.DataFrame(X).iloc[best_idx],\n",
    "# #                    y).transform(X[best_idx])\n",
    "# #     return X_new\n",
    "    \n",
    "# #     if K is not None:\n",
    "# #         \n",
    "# #         X_new = \n",
    "                    \n",
    "\n",
    "# a = select_low_error_features(X, y, k=2)\n",
    "# display(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # help(RFECV)\n",
    "# # y = Binarizer().fit_transform(session.events.iloc[:, -1])\n",
    "\n",
    "# def get_score_after_permutation(model, X, y, curr_feat):\n",
    "#     \"\"\" return the score of model when curr_feat is permuted \"\"\"\n",
    "\n",
    "#     X_permuted = X.copy()\n",
    "#     col_idx = list(X.columns).index(curr_feat)\n",
    "#     # permute one column\n",
    "#     X_permuted.iloc[:, col_idx] = np.random.permutation(\n",
    "#         X_permuted[curr_feat].values)\n",
    "\n",
    "#     permuted_score = model.score(X_permuted, y)\n",
    "#     return permuted_score\n",
    "\n",
    "\n",
    "# def get_feature_importance(model, X, y, curr_feat):\n",
    "#     \"\"\" compare the score when curr_feat is permuted \"\"\"\n",
    "\n",
    "#     baseline_score_train = model.score(X, y)\n",
    "#     permuted_score_train = get_score_after_permutation(model, X, y, curr_feat)\n",
    "\n",
    "#     # feature importance is the difference between the two scores\n",
    "#     feature_importance = baseline_score_train - permuted_score_train\n",
    "#     return feature_importance\n",
    "\n",
    "\n",
    "# # curr_feat = 'MedInc'\n",
    "\n",
    "# # feature_importance = get_feature_importance(model, X_train, y_train, curr_feat)\n",
    "# # print(f'feature importance of \"{curr_feat}\" on train set is'\n",
    "# #       f'{feature_importance:.3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # nk = 2\n",
    "# estimator = ovrc\n",
    "# X = session.computed_.signal_matrix\n",
    "# y = session.events.iloc[:, -1]\n",
    "# metric = 'precision'\n",
    "\n",
    "# def kbest_iter(estimator,\n",
    "#                X: Iterable, y: Iterable,\n",
    "#                metric: str = 'precision',\n",
    "#                nk: int = 2,\n",
    "#                test_size: float = 0.8,\n",
    "#                nbootstrap: int = 20,\n",
    "#                cv: Union[int, callable] = None,\n",
    "#                stratify: Iterable = None,\n",
    "#                random_state: int = None,\n",
    "#                **kwargs\n",
    "#                ) -> Iterable:\n",
    "#     data = []\n",
    "#     validation_params = dict(test_size=test_size, cv=cv,\n",
    "#                              stratify=stratify,\n",
    "#                              random_state=random_state)    \n",
    "# #     while True:\n",
    "# #         new = SelectKBest(k=nk).fit(X, y).get_support(indices=True)\n",
    "# #     for n in range(nbootstrap):\n",
    "#     for n in tqdm_(range(nbootstrap)):\n",
    "#         new = SelectKBest(k=nk).fit(X, y).get_support(indices=True)\n",
    "#         while (validate_model(estimator,\n",
    "#                               X.iloc[:, new], y,\n",
    "#                                        **validation_params).T.loc[list(set(y))][metric] <\n",
    "#                validate_model(estimator, X, y,\n",
    "#                              **validation_params).T.loc[list(set(y))][metric]).any():\n",
    "#             nk += 1\n",
    "#             continue\n",
    "#         else:\n",
    "#             data.append(np.array((len(new), new)))\n",
    "#     return (data)\n",
    "\n",
    "\n",
    "#     yield from data.__iter__() \n",
    "#         yield from (SelectKBest(k=nk).fit(X, y).get_support(indices=True)\n",
    "#                     for n in range(nbootstrap))\n",
    "#         score = validate_model(estimator, X, y,\n",
    "#                                **validation_params).T.loc[list(set(y))][metric]\n",
    "#         new_score = validate_model(estimator, X.iloc[\n",
    "#                         SelectKBest(k=nk).fit(X, y).get_support(indices=True):, new], y,\n",
    "#                                    **validation_params).T.loc[list(set(y))][metric]\n",
    "#         if (new_score < score).any():\n",
    "#         nk += 1\n",
    "#         continue\n",
    "#     else:\n",
    "#         yield SelectKBest(k=nk).fit(X, y).get_support(indices=True)\n",
    "#     else:\n",
    "#         break\n",
    "#     return X.iloc[:, SelectKBest(k=nk).fit(X, y).get_support(indices=True)]\n",
    "\n",
    "# test = [kbest_iter(ovrc, X.set_axis(y, axis=0), y,\n",
    "#                      nk=2, nbootstrap=10)\n",
    "#         for y in (session.events.trial_type,\n",
    "#                   session.events.recognition_performance,\n",
    "#                   session.events.iloc[:, -1])]\n",
    "\n",
    "#     (X_new for nboot in nbootstrap)\n",
    "#kbest_iter(estimator, X, y, metric, nk, test_size, cv,stratify, random_state, nbootstrap)\n",
    "#\n",
    "# def kbest_iter_bootstrap(estimator,\n",
    "#                          X: Iterable, y: Iterable,\n",
    "#                          metric: str = 'precision',\n",
    "#                          nk: int = 2,\n",
    "#                          test_size: float = 0.8,\n",
    "#                          cv: Union[int, callable] = None,\n",
    "#                          stratify: Iterable = None,\n",
    "#                          random_state: int = None,\n",
    "#                          nbootstrap: int = 20,\n",
    "#                          **kwargs\n",
    "#                          ) -> Iterable:\n",
    "    \n",
    "#     validation_params = dict(test_size=test_size, cv=cv,\n",
    "#                              stratify=stratify,\n",
    "#                              random_state=random_state)\n",
    "# #     @consumer\n",
    "#     yield from (kbest_iter(estimator, X, y, metric, nk, **validation_params)\n",
    "#                 for n in tqdm_(range(nbootstrap)))\n",
    "#     @consumer\n",
    "#     yield from (pd.DataFrame(kbest_iter(estimator, X, y, metric, nk, **validation_params))\n",
    "#                 for n in tqdm_(range(nbootstrap)))\n",
    "#     data = pd.DataFrame(((next(kbest_iter(estimator, X, y, metric, nk,\n",
    "#                                      **validation_params))\n",
    "#                           for n in tqdm_(range(nbootstrap)))))\n",
    "#     most_comm = data[0].value_counts(ascending=False).index.tolist()[0]\n",
    "#     comm_features = list(set(flatten(idx.tolist() for idx in\n",
    "#                                      data.set_index(0).loc[most_comm][1].tolist())))\n",
    "#     better = SelectKBest(k=most_comm).fit(X.iloc[:, comm_features],\n",
    "#                                           y).get_support(indices=True)\n",
    "#     return X.iloc[:, better]\n",
    "\n",
    "\n",
    "\n",
    "# Xnew = flatten(kbest_iter_bootstrap(ovrc, X.set_axis(y, axis=0), y,\n",
    "#                      nk=2, nbootstrap=10))\n",
    "# bootstrap_test = [kbest_iter_bootstrap(ovrc, X.set_axis(y, axis=0), y,\n",
    "#                                        nk=4,\n",
    "#                                        nbootstrap=20)\n",
    "#                   for y in (session.events.trial_type,\n",
    "#                             session.events.recognition_performance,\n",
    "#                             session.events.iloc[:, -1])]\n",
    "# display(*bootstrap_test)\n",
    "# test = [kbest_iter(estimator, X.set_axis(trials, axis=0), y=trials, nk=2)\n",
    "#         for trials in (session.events.trial_type, session.events.recognition_performance,\n",
    "#                        session.events.iloc[:, -1])]\n",
    "#  for nk in range(2, 14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(niplot.view_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "\n",
    "ovrc = OneVsRestClassifier(svc)\n",
    "\n",
    "def validate_conditions(estimator, X, y, k=2, metric='precision'):\n",
    "    return validate_model(estimator, X, y, test_sixe=0.8).T.loc[pd.Series(y).unique()][metric]\n",
    "\n",
    "def init_vs_new(estimator, X, y, k, metric='precision'):\n",
    "    skb = SelectKBest(k=k)\n",
    "    X_new = X.iloc[:, skb.fit(X, y).get_support(indices=True)]\n",
    "    init_scores = validate_conditions(estimator, X, y, metric)\n",
    "    k_scores = validate_conditions(estimator, X_new, y, metric)\n",
    "    return (init_scores, k_scores)\n",
    "\n",
    "# def validate_kbest(estimator, X, y, k=2, metric='precision'):\n",
    "#     init_scores, k_scores = bool_kbest(estimator, X, y, k, metric)\n",
    "#     if (init_scores >= k_scores).any():\n",
    "#         k+=1\n",
    "#         init_scores, k_scores = bool_kbest(estimator, X, y, k, metric)\n",
    "#     else:\n",
    "#         return X_new\n",
    "# validate_kbest(ovrc, session.computed_.signal_matrix, session.events.trial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import takewhile\n",
    "\n",
    "def kbest_per_condition(estimator, X, y, k=2):\n",
    "#     skb = SelectKBest(k=k)\n",
    "#     conds = [events[col].tolist() for col in trial_type_cols]\n",
    "#     k_cond = [k]*len(trial_type_cols)\n",
    "    \n",
    "    X_new = X.iloc[:, SelectKBest(k=k).fit(X, y).get_support(indices=True).tolist()]\n",
    "    takewhile((validate_conditions(estimator, X, y) <\n",
    "               validate_conditions(estimator, X_new, y)).any(),\n",
    "              list(range(X.shape[1])))\n",
    "#         k += 1\n",
    "#     else:\n",
    "    return X_new\n",
    "        \n",
    "        \n",
    "#     return init_scores\n",
    "#     init_scores = [validate_model(estimator, X=X.iloc[:, SelectKBest(k=k,\n",
    "#                              y=cond, test_size=0.9,\n",
    "#                              cv=min(Counter(cond).values())\n",
    "#                              ).T.loc[list(set(cond))].precision\n",
    "#                    for cond in conds]\n",
    "#     cond_names = [list(set(cond)) for cond in conds]\n",
    "#     scores = [validate_model(estimator,\n",
    "#                              X=X.iloc[:, SelectKBest(k=k_cond[cond[0]]).fit(X, cond[1]).get_support(\n",
    "#                                              indices=True).tolist()],\n",
    "#                              y=cond[1], test_size=0.9, stratify=cond[1],\n",
    "#                              cv=min(Counter(cond[1]).values())\n",
    "#                                       ).T.loc[list(set(cond[1]))].precision\n",
    "#              for cond in enumerate(conds)]\n",
    "#     return X_new\n",
    "kbest_per_condition(ovrc, X=session.computed_.signal_matrix,\n",
    "                    y=session.events.recognition_performance)\n",
    "#                     trial_type_cols=trial_type_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize estimator\n",
    "import pingouin as pg\n",
    "svc = SVC(kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "\n",
    "ovrc = OneVsRestClassifier(svc)\n",
    "# ovrc.predict(session.computed_.signal_matrix)\n",
    "X= session.computed_.signal_matrix\n",
    "k = 2\n",
    "\n",
    "X_new = SelectKBest(k=2).fit\n",
    "result = [validate_model(estimator=ovrc,\n",
    "                         X=a,\n",
    "                         y=session.events[col], cv=2,\n",
    "                         test_size=0.9,\n",
    "                         stratify=session.events[col],\n",
    "                         random_state=1).T\n",
    "          for col in trial_type_cols]\n",
    "# while \n",
    "display(*result)\n",
    "# best_of = list(set(flatten([SelectKBest(k=2).fit(session.computed_.signal_matrix,\n",
    "#                                                  session.events[col].tolist()\n",
    "#                                                 ).get_support(indices=True).tolist()\n",
    "#                             for col in trial_type_cols])))\n",
    "\n",
    "# display(X.iloc[:, best_of])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_cols = ['trial_type', 'recognition_performance', session.events.iloc[:, -1].name]\n",
    "\n",
    "best_rois = Bunch(**dict(tuple((trial_type_col, [col for col in tqdm_(X.columns) if\n",
    "                (validate_model(ovrc, X[col].values.reshape(-1, 1),\n",
    "                               session.events[trial_type_col]).accuracy>0.95).all()])\n",
    "                       for trial_type_col in trial_type_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_rois = Bunch(**best_rois)\n",
    "\n",
    "[len(best_rois[key]) for key in list(best_rois.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (pd.DataFrame(MinMaxScaler().fit_transform(StandardScaler().fit_transform(pairwise_distances(X))),\n",
    "                  index=session.events.recognition_performance,\n",
    "                  columns=session.events.recognition_performance).corr('spearman').sort_index().T.sort_index())\n",
    "sns.heatmap(a)\n",
    "# b = a.copy(deep=True)\n",
    "\n",
    "# a.corrwith(method='spearman', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_model(X.iloc[SelectKBest().fit(X, )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.filter(regex='hippo')\n",
    "sns.heatmap(pd.DataFrame(pd.DataFrame(pairwise_distances(X.T)).corr('spearman').values,\n",
    "             index=grouped_clusters.cluster_ids,\n",
    "             columns=grouped_clusters.cluster_ids).sort_index().T.sort_index())\n",
    "\n",
    "#.set_axis(grouped_clusters.cluster_ids, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(X.corr)\n",
    "\n",
    "crit = 1\n",
    "Xcorr = 1 - np.abs(X.corr('spearman', min_periods=X.shape[0]).values)\n",
    "np.fill_diagonal(Xcorr, 0)\n",
    "dist_linkage = hierarchy.ward(squareform(Xcorr))\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, crit, criterion=\"distance\")\n",
    "cluster_ids.shape, np.unique(cluster_ids).shape\n",
    "\n",
    "grouped_clusters = pd.DataFrame(tuple(zip(cluster_ids, X.columns)),\n",
    "                                columns=['cluster_ids', 'rois'])#.groupby('cluster_ids')\n",
    "\n",
    "# clusters = [grouped_clusters.get_group(grp).rois.values for grp in grouped_clusters.groups]\n",
    "\n",
    "# cluster_ids\n",
    "# validate_model(ovrc, X=X.filter(regex='hippo'),\n",
    "#                X=pd.concat([X[cluster].T.mean() for cluster in clusters], axis=1),\n",
    "#                y=session.events.iloc[:, -1]).accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "# scipy.spatial.distance.pdist\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = cdict2.signal_matrix, session.events.iloc[:, -1].values\n",
    "\n",
    "def get_optimal_features(X):\n",
    "    corr = spearmanr(X).correlation\n",
    "    # Ensure the correlation matrix is symmetric\n",
    "    corr = (corr + corr.T) / 2\n",
    "    np.fill_diagonal(corr, 1)\n",
    "\n",
    "    # We convert the correlation matrix to a distance matrix before performing\n",
    "    # hierarchical clustering using Ward's linkage.\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "\n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "    return list(enumerate(cluster_ids))\n",
    "#     cluster_id_to_feature_ids = list#defaultdict(list)\n",
    "\n",
    "#     for idx, cluster_id in enumerate(cluster_ids):\n",
    "#         cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "#     selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "#     slected_ids = [v[1] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# spheres_optimal_features = get_optimal_features(spheres_cdict.signal_matrix)\n",
    "maps_optimal_features = get_optimal_features(session.computed_.signal_matrix)\n",
    "maps_optimal_features\n",
    "# X.iloc[:, maps_optimal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*session.computed_.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_decoding_utils import flatten\n",
    "flatten([[{1, 'a'}, 'b', {'c':3}]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# c3 = (session.events[col]\n",
    "\n",
    "# display(*session.computed_.weighted_matrices)\n",
    "# best = session.computed_.signal_matrix.iloc[:, maps_optimal_features]\n",
    "\n",
    "# best\n",
    "# dist_mats = Bunch(**dict(tuple(col, Bunch(**dict(tuple(\n",
    "#     (cond, pairwise_distances(best.set_axis(session.events[col], axis=0).loc[cond].T))\n",
    "#                        for cond in list(set(col))))))\n",
    "#                    for col in trial_type_cols))\n",
    "\n",
    "ctl_idx = 'Ctl'\n",
    "\n",
    "combo_mat = pd.concat([mat for mat in session.computed_.weighted_matrices[1:]])\n",
    "cond_signals = Bunch(**dict(tuple((cond, combo_mat.loc[cond])\n",
    "                                  for cond in combo_mat.index.unique())))\n",
    "cond_imgs = cond_signals.copy()\n",
    "[cond_imgs.update({item[0]: nimage.mean_img(maps_masker.inverse_transform(item[1]))})\n",
    " for item in tqdm_(tuple(cond_imgs.items()),\n",
    "                   desc='Converting trial type signals back to 4D')]\n",
    "\n",
    "# cond_imgs = dict(tuple((cond, nimage.mean_img(maps_masker.inverse_transform(combo_mat.loc[cond])))\n",
    "#                        for cond in tqdm_(combo_mat.index.unique(),\n",
    "#                                          desc='Converting trial type signals back to 4D')))\n",
    "# subtracted_imgs = \n",
    "# cond_imgs = flatten([[{cond, maps_masker.inverse_transform(session.computed_.weighted_matrices[mat[0]].loc[cond])}\n",
    "#                       for cond in session.computed_.weighted_matrices[mat[0]].index.unique()]\n",
    "#                      for mat in tqdm_(enumerate(session.computed_.weighted_matrices[1:], 1),\n",
    "#                                       desc='Converting trial type signals back to 4D')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtracted_signals = dict(tuple((key, cond_signals[key].mean() - cond_signals[ctl_idx].mean())\n",
    "                                for key in tuple(key for key in cond_signals\n",
    "                                                 if key != ctl_idx)))\n",
    "subtracted_imgs = subtracted_signals.copy()\n",
    "[subtracted_imgs.update({item[0]: nimage.mean_img(maps_masker.inverse_transform(item[1]))})\n",
    " for item in tqdm_(tuple(subtracted_imgs.items()),\n",
    "                   desc='Converting subtracted signals back to 4D')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_signals['Enc'].mean()# - cond_si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(item[1]), dim=-1, opacity=0.8,\n",
    "                          title=item[0], bg_img=session.anat_img)\n",
    "          for item in subtracted_imgs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(nimage.mean_img(item[1]), dim=-1, opacity=0.8,\n",
    "                          title=item[0], bg_img=session.anat_img)\n",
    "          for item in cond_imgs.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(list(dist_mats.values())[0])\n",
    "sns.heatmap(pd.DataFrame(((pd.DataFrame(val).mean() for val in\n",
    "                           list(dist_mats.values()))),\n",
    "                         index=list(dist_mats.keys())).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndims = 1024\n",
    "# difumo_cut_coords = pd.read_csv(f'/data/simexp/fnadeau/difumo_{ndims}_dims_3mm_cut_coords.tsv',\n",
    "#                                 sep='\\t', index_col='component')\n",
    "# # difumo_cut_coords\n",
    "# most_pred = '|'.join(['retrocalcarine cortex rh', 'precentral sulcus mid-inferior rh',\n",
    "#                       'paracingulate gyrus posterior', 'inferior temporal sulcus anterior rh',\n",
    "#                       'superior frontal sulcus mid-posterior lh', 'calcarine cortex mid-posterior rh',\n",
    "#                       'middle frontal gyrus superior rh', 'superior parietal lobule anterior lh',\n",
    "#                       'hippocampal fissure', 'cingulate anterior', 'subcentral gyrus rh',\n",
    "#                       'frontal pole lateral rh', 'postcentral sulcus middle lh',\n",
    "#                       'lateral fissure posterior limb lh', 'posterior cingulate cortex anterior',\n",
    "#                       'middle frontal gyrus mid-posterior inferior rh',\n",
    "#                       'intracalcarine cortex', 'fusiform gyrus anterior rh'])\n",
    "\n",
    "# maps_path = '/data/simexp/fnadeau/nilearn_atlases/difumo_atlases/1024/3mm/maps.nii.gz'\n",
    "# most_pred_idx = difumo_cut_coords.reset_index().set_index(\n",
    "#                     'difumo_names').loc[most_pred.split('|')].component\n",
    "# feature_names = difumo_cut_coords.loc[most_pred_idx].difumo_names\n",
    "# maps_indexes = [idx -1 for idx in most_pred_idx]\n",
    "\n",
    "# new_difumo = nimage.index_img(nimage.load_img(maps_path), most_pred_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.plot_prob_atlas(nimage.index_img(di1024.maps, [0])),\n",
    "          niplot.view_img(nimage.mean_img(nimage.index_img(di1024.maps, [0])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# di2 = difumo_cut_coords.copy(deep=True).set_index('difumo_names').T\n",
    "\n",
    "# wm =  di2.filter(wm_rois).columns.tolist()\n",
    "# not_cortex = di2.filter(regex='|'.join([foremidbrain_rois, wm_rois])).columns.tolist()\n",
    "# di2 = di2.drop(not_cortex, axis=1)\n",
    "\n",
    "# cortex_rois = difumo_cut_coords.reset_index(drop=False).set_index(\n",
    "#                   'difumo_names', drop=False).loc[di2.columns].component.values\n",
    "\n",
    "\n",
    "\n",
    "# maps_masker = NiftiMapsMasker(maps_img=new_difumo,\n",
    "#                               mask_img=session.mask_img,\n",
    "#                               resampling_target='mask',\n",
    "#                               t_r=get_t_r(session.fmri_img),\n",
    "#                               **masker_defs).fit()\n",
    "\n",
    "# from nilearn import plotting as niplot\n",
    "\n",
    "# new_seeds = niplot.find_probabilistic_atlas_cut_coords(maps_masker.maps_img_)\n",
    "\n",
    "# spheres_masker = NiftiSpheresMasker(new_seeds,\n",
    "#                                     mask_img=session.mask_img,\n",
    "#                                     t_r=get_t_r(session.fmri_img),\n",
    "#                                     **masker_defs).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NiftiMapsMasker Vs NiftiSpheresMasker Resampling Fit Test\n",
    "\n",
    "\n",
    "\n",
    "# spheres_cdict = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "#                                   events=session.events,\n",
    "#                                   masker=spheres_masker,\n",
    "#                                   output_type='effect_size',\n",
    "#                                   trial_type_cols=trial_type_cols,\n",
    "#                                   glm_kws=session.glm_defs,\n",
    "#                                   design_kws=session.design_defs,\n",
    "#                                   feature_labels=di2.columns.tolist())\n",
    "\n",
    "# session.computed_ = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "#                                events=session.events,\n",
    "#                                masker=maps_masker,\n",
    "#                                output_type='effect_size',\n",
    "#                                trial_type_cols=trial_type_cols,\n",
    "#                                glm_kws=session.glm_defs,\n",
    "#                                design_kws=session.design_defs,\n",
    "#                                feature_labels=di2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names.iloc[maps_optimal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_seeds = niplot.find_probabilistic_atlas_cut_coords(\n",
    "                nimage.index_img(new_difumo, spheres_optimal_features))\n",
    "opt_maps = nimage.index_img(new_difumo, maps_optimal_features)\n",
    "\n",
    "opt_spheres_masker = NiftiSpheresMasker(opt_seeds,\n",
    "                                        mask_img=session.mask_img,\n",
    "                                        t_r=get_t_r(session.fmri_img),\n",
    "                                        **masker_defs).fit()\n",
    "\n",
    "opt_maps_masker = NiftiMapsMasker(maps_img=opt_maps,\n",
    "                                  mask_img=session.mask_img,\n",
    "                                  resampling_target='mask',\n",
    "                                  t_r=get_t_r(session.fmri_img),\n",
    "                                  **masker_defs).fit()\n",
    "\n",
    "opt_spheres_cdict = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                                  events=session.events,\n",
    "                                  masker=spheres_masker,\n",
    "                                  output_type='effect_size',\n",
    "                                  trial_type_cols=trial_type_cols,\n",
    "                                  glm_kws=session.glm_defs,\n",
    "                                  design_kws=session.design_defs,\n",
    "                                  feature_labels=di2.columns.tolist())\n",
    "\n",
    "opt_session.computed_ = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                               events=session.events,\n",
    "                               masker=maps_masker,\n",
    "                               output_type='effect_size',\n",
    "                               trial_type_cols=trial_type_cols,\n",
    "                               glm_kws=session.glm_defs,\n",
    "                               design_kws=session.design_defs,\n",
    "                               feature_labels=di2.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_ovrc = OneVsRestClassifier(SVC(tol=0.0001, kernel='linear',\n",
    "                                   cache_size=5,\n",
    "                                   decision_function_shape='ovr',\n",
    "                                   class_weight='balanced',\n",
    "                                   probability=True))\n",
    "\n",
    "opt_spheres_acc = validate_model(opt_ovrc, X=opt_spheres_cdict.signal_matrix,\n",
    "                                 y=session.events.iloc[:, -1],\n",
    "                                 test_size=0.5,\n",
    "                                 stratify=session.events.iloc[:, -1])\n",
    "\n",
    "opt_maps_acc = validate_model(opt_ovrc, X=opt_session.computed_.signal_matrix,\n",
    "                                  y=session.events.iloc[:, -1],\n",
    "                                  test_size=0.5,\n",
    "                                  stratify=session.events.iloc[:, -1])\n",
    "\n",
    "display(opt_spheres_acc, opt_maps_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spheres_optimal_features), len(maps_optimal_features)\n",
    "\n",
    "common_opt_features = list(set(spheres_optimal_features).intersection(set(maps_optimal_features)))\n",
    "\n",
    "common_seeds = niplot.find_probabilistic_atlas_cut_coords(\n",
    "                nimage.index_img(new_difumo, common_opt_features))\n",
    "common_maps = nimage.index_img(new_difumo, common_opt_features)\n",
    "\n",
    "print(len(common_opt_features), common_seeds.shape, common_maps.shape)\n",
    "\n",
    "common_spheres_masker = NiftiSpheresMasker(common_seeds,\n",
    "                                           mask_img=session.mask_img,\n",
    "                                           t_r=get_t_r(session.fmri_img),\n",
    "                                           **masker_defs).fit()\n",
    "\n",
    "common_maps_masker = NiftiMapsMasker(maps_img=common_maps,\n",
    "                                     mask_img=session.mask_img,\n",
    "                                     resampling_target='mask',\n",
    "                                     t_r=get_t_r(session.fmri_img),\n",
    "                                     **masker_defs).fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_roi_labels = di2.iloc[:, common_opt_features].columns.tolist()\n",
    "\n",
    "common_spheres_cdict = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                                  events=session.events,\n",
    "                                  masker=common_spheres_masker,\n",
    "                                  output_type='effect_size',\n",
    "                                  trial_type_cols=trial_type_cols,\n",
    "                                  glm_kws=session.glm_defs,\n",
    "                                  design_kws=session.design_defs,\n",
    "                                  feature_labels=common_roi_labels)\n",
    "\n",
    "\n",
    "common_session.computed_ = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                               events=session.events,\n",
    "                               masker=common_maps_masker,\n",
    "                               output_type='effect_size',\n",
    "                               trial_type_cols=trial_type_cols,\n",
    "                               glm_kws=session.glm_defs,\n",
    "                               design_kws=session.design_defs,\n",
    "                               feature_labels=common_roi_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "common_ovrc = OneVsRestClassifier(SVC(tol=0.0001, kernel='linear',\n",
    "                                   cache_size=5,\n",
    "                                   decision_function_shape='ovr',\n",
    "                                   class_weight='balanced',\n",
    "                                   probability=True))\n",
    "\n",
    "common_spheres_acc = validate_model(common_ovrc, X=common_spheres_cdict.signal_matrix,\n",
    "                                 y=session.events.iloc[:, -1],\n",
    "                                 test_size=0.5,\n",
    "                                 stratify=session.events.iloc[:, -1])\n",
    "\n",
    "common_maps_acc = validate_model(common_ovrc, X=common_session.computed_.signal_matrix,\n",
    "                                  y=session.events.iloc[:, -1],\n",
    "                                  test_size=0.5,\n",
    "                                  stratify=session.events.iloc[:, -1])\n",
    "\n",
    "display(common_spheres_acc, common_maps_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "common_maps_inv = common_maps_masker.inverse_transform(MinMaxScaler().fit_transform(common_session.computed_.signal_matrix))\n",
    "# common_spheres_inv = common_spheres_masker.inverse_transform(common_spheres_cdict.signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treshold = nimage.mean_img(common_maps_inv).get_fdata().min()\n",
    "treshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niplot.view_img(nimage.mean_img(common_maps_inv), title='18 Most Predictive ROIs',\n",
    "                display_mode='mosaic', opacity=0.8,\n",
    "#                 treshold=treshold,\n",
    "                symmetric_cmap=False,\n",
    "                bg_img=nimage.load_img(sessions[0].anat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# niplot.view_img()\n",
    "niplot.plot_prob_atlas(common_maps_inv, title='18 Most Predictive ROIs',\n",
    "                       display_mode='mosaic',\n",
    "#                        opacity=0.8,\n",
    "                       bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_roi_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[niplot.view_img(inverse_imgs.ctl_miss_ws_cs[trial_type],\n",
    "                                    title=trial_type, dim=-1,\n",
    "                                    symmetric_cmap=False, opacity=0.8,\n",
    "                                    cut_coords=(42, 19, -8),\n",
    "                                    bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "          for trial_type in inverse_imgs.ctl_miss_ws_cs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_spheres_optimal_features = get_optimal_features(common_spheres_cdict.signal_matrix)\n",
    "common_maps_optimal_features = get_optimal_features(common_session.computed_.signal_matrix)\n",
    "\n",
    "n_opt_sphres = len(common_spheres_optimal_features)\n",
    "n_opt_maps = len(common_maps_optimal_features)\n",
    "\n",
    "opt_common_features = set(common_spheres_optimal_features).intersection(\n",
    "                          set(common_maps_optimal_features))\n",
    "n_opt_common = len(opt_common_features)\n",
    "\n",
    "display(n_opt_sphres, n_opt_maps, n_opt_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_maps_optimal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     clf = RandomForestClassifier(n_estimators=X_train.shape[0], random_state=42)\n",
    "#     clf.fit(X_train, y_train)\n",
    "\n",
    "#     result = permutation_importance(clf, X_train, y_train, n_repeats=2, random_state=42)\n",
    "#     perm_sorted_idx = np.argsort(result.importances_mean)\n",
    "\n",
    "#     tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "#     tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "#     # print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "#     feature_importance_sorted = clf.feature_importances_[tree_importance_sorted_idx]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                         test_size=,\n",
    "#                                                         random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spheres_cdict.signal_matrix.isna().any().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, pearsonr, spearmanr\n",
    "\n",
    "X = spheres_cdict.signal_matrix\n",
    "\n",
    "corr = spearmanr(X).correlation\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "np.nan_to_num(corr, 0)\n",
    "# corr = corr.fillna(0)\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "# help(permutation_importance)\n",
    "\n",
    "permutation_importance(estimator=ovrc, X=X, y=session.events.iloc[:,-1],\n",
    "                       scoring='accuracy')\n",
    "# pd_corr = pd.DataFrame(X.values, index=None).corr(method='spearman')\n",
    "\n",
    "# methods = {'kendall': kendalltau, 'pearson': pearsonr, 'spearman': spearmanr}\n",
    "# method = 'spearman'\n",
    "\n",
    "# (X.corr('spearman') + X.corr('spearman').T) /2\n",
    "# == pd.DataFrame(X.values, index=None).corrwith(\n",
    "#     X.values.T ,method='spearman')\n",
    "# corr = spearmanr(X).correlation\n",
    "# # corr.shape\n",
    "# \n",
    "# corr.shape\n",
    "# \n",
    "# \n",
    "# # help(X.corr)\n",
    "# help(scipy.stats.spearmanr)\n",
    "# corr.shape\n",
    "# \n",
    "# distance_matrix.shape\n",
    "# \n",
    "# cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "\n",
    "# display(corr.shape, distance_matrix.shape, dist_linkage.shape, cluster_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spheres_difumo = \n",
    "spheres_optimal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([idx for idx in spheres_optimal_features if idx not in maps_optimal_features])\n",
    "from nilearn import plotting as niplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_img = maps_masker.inverse_transform(cdict2.signal_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(niplot.view_img(nimage.mean_img(nimage.index_img(end_img,\n",
    "                                                         maps_optimal_features)),\n",
    "                        title='Maps Masker ROIs',\n",
    "                        dim=-1, symmetric_cmap=False, opacity=0.8,\n",
    "                        bg_img=nimage.load_img(sessions[0].anat_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_difumo = nimage.index_img(new_difumo, maps_optimal_features)\n",
    "\n",
    "best_maps_masker = NiftiMapsMasker(maps_img=best_difumo,\n",
    "#                               mask_img=sessions[0].mask_img,\n",
    "                              resampling_target='maps',\n",
    "#                               t_r=2.5,\n",
    "                              standardize_confounds=False,\n",
    "                              smoothing_fwhm=None,\n",
    "                              high_variance_confounds=False,\n",
    "                              detrend=False, dtype='f',\n",
    "                              low_pass=None, high_pass=None,\n",
    "                              allow_overlap=True,\n",
    "                              standardize=False).fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_img = best_maps_masker.inverse_transform(session.computed_.signal_matrix.iloc[:, maps_optimal_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(niplot.view_img(nimage.mean_img(best_img),\n",
    "                        title='Spheres Masker ROIs',\n",
    "                        dim=-1, symmetric_cmap=False, opacity=0.8,\n",
    "                        treshold='auto',\n",
    "                        bg_img=nimage.load_img(sessions[0].anat_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cdict2.signal_matrix.set_index(sessions[0].events.ctl_miss_ws_cs).reset_index()\n",
    "ctl_idx = X[X.ctl_miss_ws_cs=='Ctl'].index\n",
    "enc_idx = X[X.ctl_miss_ws_cs=='Enc'].index\n",
    "\n",
    "end_ctl = nimage.index_img(end_img, ctl_idx)\n",
    "end_enc = nimage.index_img(end_img, enc_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nilearn.plotting.view_img(nimage.mean_img(end_img), dim=-1,\n",
    "                           symmetric_cmap=False,\n",
    "                           opacity=0.8,\n",
    "                           bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "#  for img in (end_ctl, end_enc)]\n",
    "# select_end_img = nimage.index_img(end_img, optimal_features)\n",
    "## resampling_target = 'data'\n",
    "# end_img.shape = (104, 123, 104, 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d1024 = Path('/data/simexp/fnadeau/nilearn_atlases/difumo_atlases/1024/labels_1024_dictionary.csv')\n",
    "# from io import StringIO\n",
    "\n",
    "# difumo1064_labels = pd.read_csv(StringIO(d1024.read_bytes().lower().decode()),\n",
    "#                                 index_col='component')\n",
    "# difumo1064_labels.to_csv(d1024)\n",
    "#['difumo_names'].value_counts()>1).replace({False:np.nan}).dropna()\n",
    "# help(NiftiMapsMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_cols = ['trial_type', 'recognition_performance', 'position_performance']\n",
    "trial_types = get_glm_events(sessions[0].events, trial_type_cols=trial_type_cols)\n",
    "display(*designs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(Path('/data/simexp/fnadeau/wmstim/').iterdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type_cols=['trial_type', 'recognition_performance',\n",
    "                 'ctl_miss_ws_cs']\n",
    "\n",
    "cdict2['contrasts'] = [cdict2[key].contrast_img for key in\n",
    "                       tuple(cdict2.keys())[:1+len(trial_type_cols)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_imgs = Bunch(**dict(tuple((key, Bunch(**dict(tuple((idx, maps_masker.inverse_transform(\n",
    "                   cdict2[key].signals.loc[idx])) for idx in tqdm_(cdict2[key].signals.index)))))\n",
    "                                  for key in tuple(cdict2.keys())[:1+len(trial_type_cols)])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_imgs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilearn.plotting.view_img(inverse_imgs.trial_type.Enc,\n",
    "                          dim=-1,\n",
    "                          symmetric_cmap=False,\n",
    "                          opacity=0.8,\n",
    "                          bg_img=nimage.load_img(sessions[0].anat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilearn.plotting.view_img(inverse_imgs.trial_type.Ctl,\n",
    "                          dim=-1, symmetric_cmap=False,\n",
    "                          opacity=0.8, cut_coords=(-10, -30, 5),\n",
    "                          bg_img=nimage.load_img(sessions[0].anat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[nilearn.plotting.view_img(inverse_imgs.ctl_miss_ws_cs[trial_type],\n",
    "                                    title=trial_type, dim=-1,\n",
    "                                    symmetric_cmap=False, opacity=0.8,\n",
    "                                    cut_coords=(42, 19, -8),\n",
    "                                    bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "          for trial_type in inverse_imgs.ctl_miss_ws_cs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cdict2 for key in tuple(cdict2.keys())[1:]]\n",
    "cdict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ctl_signals = [cdict2[key].signals.loc['Ctl'] for key in tuple(cdict2.keys())[1: 1+len(trial_type_cols)]]\n",
    "all_ctl_imgs = [maps_masker.inverse_transform(sigs) for sigs in all_ctl_signals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(*[nilearn.plotting.view_img(ctl_img,\n",
    "                                    title='Control Trials', dim=-1,\n",
    "                                    symmetric_cmap=False, opacity=0.8,\n",
    "                                    cut_coords=(42, 19, -8),\n",
    "                                    bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "          for ctl_img in all_ctl_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_signals = cdict2.signal_matrix.set_index(sessions[0].events.ctl_miss_ws_cs)\n",
    "results_conditions = results_signals.index.unique()\n",
    "results_imgs = [maps_masker.inverse_transform(results_signals.loc[cond])\n",
    "                for cond in results_conditions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(*[nilearn.plotting.view_img(nimage.mean_img(results_imgs[cond[0]]),\n",
    "                                    title=cond[1], dim=-1,\n",
    "                                    symmetric_cmap=False, opacity=0.8,\n",
    "                                    cut_coords=(-38, 16, -25),\n",
    "                                    bg_img=nimage.load_img(sessions[0].anat_path))\n",
    "          for cond in enumerate(results_conditions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_signals = cdict2.signal_matrix.iloc[:, selected_features]\n",
    "select_names = di2[select_signals.columns].columns\n",
    "select_idx = difumo_cut_coords.reset_index(drop=False).set_index(\n",
    "                 'difumo_names').T[select_names.tolist()].loc['component'].values.astype(int)\n",
    "select_idx\n",
    "# maps_path = '/data/simexp/fnadeau/nilearn_atlases/difumo_atlases/1024/3mm/maps.nii.gz'\n",
    "# select_difumo = nimage.index_img(nimage.load_img(maps_path), select_idx)\n",
    "\n",
    "# select_masker = NiftiMapsMasker(maps_img=select_difumo,\n",
    "#                               mask_img=sessions[0].mask_img,\n",
    "#                               resampling_target='mask',\n",
    "#                               t_r=2.5,\n",
    "#                               standardize_confounds=False,\n",
    "#                               smoothing_fwhm=None,\n",
    "#                               high_variance_confounds=False,\n",
    "#                               detrend=False, dtype='f',\n",
    "#                               low_pass=None, high_pass=None,\n",
    "#                               allow_overlap=True,\n",
    "#                               standardize=False).fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setattr(maps_masker, 'sample_mask', select_idx)\n",
    "select_img = nimage.index_img(maps_masker.inverse_transform(cdict2.signal_matrix.values.T),\n",
    "                              select_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilearn.plotting.view_img(nimage.mean_img(select_img),\n",
    "                          title='Mean Most Significant Trials', dim=-1,\n",
    "                          symmetric_cmap=False, opacity=0.8,\n",
    "#                             cut_coords=(42, 19, -8),\n",
    "                            bg_img=nimage.load_img(sessions[0].anat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_img = select_masker.inverse_transform(pwdist_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nilearn.plotting.view_img(nimage.mean_img(results_imgs[cond[0]]),\n",
    "                                    title=cond[1], dim=-1,\n",
    "                                    symmetric_cmap=False, opacity=0.8,\n",
    "                                    cut_coords=(-38, 16, -25),\n",
    "                                    bg_img=nimage.load_img(sessions[0].anat_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe_components = pd.DataFrame(((True, 'standardize', StandardScaler()),\n",
    "                                 (True, 'maximize', MaxAbsScaler()),\n",
    "                                (True, 'scale', MinMaxScaler()))).set_index(0)\n",
    "pipe_components = [tuple(item) for item in\n",
    "                   pipe_components.loc[pipe_components.index==True].values.tolist()]\n",
    "pipeline = Pipeline(pipe_components)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/data/simexp/fnadeau/best_features/'\n",
    "\n",
    "roi_tables = sorted(Path(src).rglob('*task-memory_bestfeatures.tsv'))\n",
    "test = pd.concat([pd.Series(pd.read_csv(apath, sep='\\t',\n",
    "                                        index_col='trial_type').columns)\n",
    "                  for apath in roi_tables])\n",
    "test.value_counts(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neurora.stuff import limtozero\n",
    "import math\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from neurora.stuff import show_progressbar\n",
    "from neurora.decoding import tbyt_decoding_kfold\n",
    "from nilearn.masking import apply_mask\n",
    "\n",
    "\n",
    "def fmriRDM_roi(fmri_data, mask_data,\n",
    "                sub_opt: bool = True,\n",
    "                method: str = \"correlation\",\n",
    "                abs: bool = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculate the ROI-based fMRI Representational Dissimilarity Matrix - RDM(s).\n",
    "\n",
    "    Args:\n",
    "        fmri_data : array\n",
    "            The fMRI data. The shape of ``fmri_data`` must be\n",
    "            [n_cons, n_subs, nx, ny, nz], respectively representing the\n",
    "            number of conditions, subjects & the size of fMRI-img.\n",
    "\n",
    "        mask_data : array [nx, ny, nz].\n",
    "            The mask data for region of interest (ROI).\n",
    "            The size of the fMRI image (nx, ny, nz) represent the number\n",
    "            of voxels along the x, y, z axis.\n",
    "\n",
    "        sub_opt: bool (Default = True).\n",
    "            If sub_opt is True, return the results of each subject.\n",
    "            Otherwise, return the average result.\n",
    "\n",
    "        method : str {'correlation' | 'euclidean'} (Default = 'correlation').\n",
    "            The method to calculate the dissimilarities, where 'correlation'\n",
    "            means Pearson Correlation and 'euclidean' means Euclidean Distance.\n",
    "            The results will be normalized.\n",
    "\n",
    "        abs : bool (Default = True).\n",
    "            Calculate the absolute value of Pearson r or not.\n",
    "\n",
    "    Returns:\n",
    "        RDM : array\n",
    "            The fMRI-ROI RDM.\n",
    "            If sub_opt is True, the shape of RDM is [n_subs, n_cons, n_cons].\n",
    "            Otherwise, the shape of RDM is [n_cons, n_cons].\n",
    "\n",
    "    Notes:\n",
    "        The sizes (nx, ny, nz) of fmri_data and mask_data should be same.\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape_err = \" \".join([\"\\nThe shape of inputs (fmri_data & mask_data)\",\n",
    "                                  \"for fmriRDM_roi() function should be:\\n\",\n",
    "                                  \"[n_cons, n_subs, nx, ny, nz] &\\n\",\n",
    "                                  \"[nx, ny, nz], respectively.\\n\"])\n",
    "\n",
    "    if len(np.shape(fmri_data)) != 5 or len(np.shape(mask_data)) != 3:\n",
    "        sys.exit(input_shape_err)\n",
    "\n",
    "    # get the number of conditions, subjects, the size of the fMRI-img\n",
    "    ncons, nsubs, nx, ny, nz = fmri_data.shape\n",
    "    \n",
    "    #Masking\n",
    "    \n",
    "\n",
    "    # initialize the RDMs\n",
    "    subrdms = np.zeros([nsubs, ncons, ncons], dtype=np.float)\n",
    "\n",
    "    # shape of data: [ncons, nsubs, n] -> [nsubs, ncons, n]\n",
    "    data = np.transpose(data, (1, 0, 2))\n",
    "\n",
    "    # calculate the values in RDM\n",
    "    for sub in range(nsubs):\n",
    "        f\n",
    "        for i in range(ncons):\n",
    "            for j in range(ncons):\n",
    "\n",
    "                if np.isnan(data).any():\n",
    "                    if method == 'correlation':\n",
    "                        # calculate the Pearson Coefficient\n",
    "                        r = pearsonr(data[sub, i], data[sub, j])[0]\n",
    "                        # calculate the dissimilarity\n",
    "                        if abs == True:\n",
    "                            subrdms[sub, i, j] = limtozero(1 - np.abs(r))\n",
    "                        else:\n",
    "                            subrdms[sub, i, j] = limtozero(1 - r)\n",
    "                    elif method == 'euclidean':\n",
    "                        subrdms[sub, i, j] = np.linalg.norm(data[sub, i] - data[sub, j])\n",
    "                    \"\"\"elif method == 'mahalanobis':\n",
    "                        X = np.transpose(np.vstack((data[sub, i], data[sub, j])), (1, 0))\n",
    "                        X = np.dot(X, np.linalg.inv(np.cov(X, rowvar=False)))\n",
    "                        subrdms[sub, i, j] = np.linalg.norm(X[:, 0] - X[:, 1])\"\"\"\n",
    "        if method == 'euclidean':\n",
    "            data_max, data_min = np.max(subrdms[sub]), np.min(subrdms[sub])\n",
    "            subrdms[sub] = (subrdms[sub] - data_min) / (data_max - data_min)\n",
    "\n",
    "    # average the RDMs\n",
    "    rdm = np.average(subrdms, axis=0)\n",
    "\n",
    "    if sub_opt is True:\n",
    "        print(\"RDMs computing finished!\")\n",
    "        return subrdms\n",
    "\n",
    "    print(\"RDM computing finished!\")\n",
    "    return rdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dst = '/data/simexp/fnadeau/best_features/'\n",
    "for session in tqdm_(sessions):\n",
    "    cdict2 = get_all_contrasts(fmri_img=session.fmri_img,\n",
    "                               events=session.events,\n",
    "                               masker=spheres_masker,\n",
    "                               output_type='effect_size',\n",
    "                               trial_type_cols=['trial_type',\n",
    "                                                'recognition_performance',\n",
    "                                                'ctl_miss_ws_cs'],\n",
    "                               glm_kws=session.glm_defs,\n",
    "                               design_kws=session.design_defs,\n",
    "                               feature_labels=di2.columns.tolist())\n",
    "\n",
    "    X, y = cdict2.signal_matrix, session.events.iloc[:, -1].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5,\n",
    "                                                        random_state=42)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=X_train.shape[0], random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    result = permutation_importance(clf, X_train, y_train, n_repeats=2, random_state=42)\n",
    "    perm_sorted_idx = np.argsort(result.importances_mean)\n",
    "\n",
    "    tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "    tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "    # print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "    feature_importance_sorted = clf.feature_importances_[tree_importance_sorted_idx]\n",
    "\n",
    "    corr = spearmanr(X).correlation\n",
    "\n",
    "    # Ensure the correlation matrix is symmetric\n",
    "    corr = (corr + corr.T) / 2\n",
    "    np.fill_diagonal(corr, 1)\n",
    "\n",
    "    # We convert the correlation matrix to a distance matrix before performing\n",
    "    # hierarchical clustering using Ward's linkage.\n",
    "    distance_matrix = 1 - np.abs(corr)\n",
    "    dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "\n",
    "    cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "    cluster_id_to_feature_ids = defaultdict(list)\n",
    "\n",
    "    for idx, cluster_id in enumerate(cluster_ids):\n",
    "        cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "    selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "    X_train_sel = X_train.iloc[:, selected_features]\n",
    "    X_test_sel = X_test.iloc[:, selected_features]\n",
    "\n",
    "    clf_sel = RandomForestClassifier(n_estimators=X_train_sel.shape[0],\n",
    "                                     random_state=42)\n",
    "    clf_sel.fit(X_train_sel, y_train)\n",
    "    end = 'task-memory_bestfeatures'\n",
    "    top_features = cdict2.signal_matrix.iloc[:, selected_features]\n",
    "\n",
    "    top_features.to_csv(os.path.join(dst,f'{session.sub_id}_{session.ses_id}_{end}.tsv'),\n",
    "                        sep='\\t', index='trial_type', encoding='UTF-8-SIG')\n",
    "\n",
    "    goodfeatures = top_features#.iloc[:, selected_features]\n",
    "\n",
    "    selected_distances = pd.DataFrame(scipy.stats.spearmanr(pairwise_distances(goodfeatures.T.values))[0],\n",
    "                                      index=goodfeatures.columns,\n",
    "                                      columns=goodfeatures.columns)\n",
    "\n",
    "    masked_distances = selected_distances.where(selected_distances.values !=\n",
    "                                                np.tril(selected_distances.values))\n",
    "    masked_distances = masked_distances.where(masked_distances.values!=0)\n",
    "\n",
    "    masked_distances = masked_distances.where(~np.array([row[1].fillna(0).between(-0.50, 0.50).values\n",
    "                                                         for row in masked_distances.iterrows()]))\n",
    "    masked_distances = masked_distances.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "\n",
    "    the_heatmap = sns.heatmap(masked_distances).get_figure().savefig(os.path.join(dst,\n",
    "        f'{session.sub_id}_{session.ses_id}_{end}_heatmap.jpg'))\n",
    "    \n",
    "    trial_type_labels = [session.events.trial_type, session.events.recognition_performance,\n",
    "                         session.events.ctl_miss_ws_cs]\n",
    "    \n",
    "    svc = SVC(tol=0.0001, kernel='linear', cache_size=5,\n",
    "          decision_function_shape='ovr', class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "    ovrc = OneVsRestClassifier(svc)\n",
    "    suffix = 'classification-report.tsv'\n",
    "    try:\n",
    "        cr_results = [validate_model(ovrc, top_features, y=ttlabels)\n",
    "                      for ttlabels in trial_type_labels]\n",
    "\n",
    "        [cr_results[col[0]].to_csv(os.path.join(dst, f'{session.sub_id}_{session.ses_id}_{col[1].name}-'+suffix),\n",
    "                        sep='\\t', index=True, encoding='UTF-8-SIG')\n",
    "         for col in enumerate(trial_type_labels)]\n",
    "    except ValueError:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weightings(signals: pd.DataFrame,\n",
    "#                weights: pd.DataFrame,\n",
    "#                ) -> pd.DataFrame:\n",
    "# #                weight_labels: Union[list, pd.Index] = None\n",
    "#     \"\"\"\n",
    "#     Return condition-wise weighted signals DataFrame.\n",
    "#     \"\"\"\n",
    "\n",
    "#     newsignals = signals.copy(deep=True)#.set_axis(condition_labels, axis=0)\n",
    "# #     if weight_labels is not None:\n",
    "# #         weights = weights.set_axis(weight_labels, axis=0)\n",
    "#     for cond in weights.index.unique():\n",
    "#         newsignals.loc[cond] = newsignals.loc[cond]*weights.loc[cond]\n",
    "#     return newsignals\n",
    "\n",
    "\n",
    "# def get_weighted_signals(signals: pd.DataFrame,\n",
    "#                          weights: list,\n",
    "#                          standardize: bool = True,\n",
    "#                          **kwargs\n",
    "#                          ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return condition-wise weighted signals DataFrame for all conditions.\n",
    "    \n",
    "#     Args:\n",
    "#         signals\n",
    "#     \"\"\"\n",
    "\n",
    "#     orig_index, orig_cols = signals.index, signals.columns\n",
    "# #     if weight_labels is None:\n",
    "# #         weight_labels = [[]] * len(weights)\n",
    "# # weight_labels=weight_labels[weight[0]]\n",
    "#     w0 = [weightings(signals, weights=weights[weight[0]])\n",
    "#           for weight in enumerate(weights)]\n",
    "#     slist = [signals]+w0\n",
    "#     data = np.prod(np.array(slist), axis=0)\n",
    "#     if standardize is True:\n",
    "#         data = StandardScaler().fit_transform(data)\n",
    "#     return pd.DataFrame(data, index=orig_index, columns=orig_cols)\n",
    "\n",
    "# weighted_signals = [weightings(item[0], item[1]) for item in\n",
    "#                     tuple(zip(cdict2.matrices[1:], cdict2.weights))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_signals = [weightings(item[0], item[1]) for item in\n",
    "#                     tuple(zip(cdict2.matrices[1:], cdict2.weights))]\n",
    "\n",
    "# cond_weights = pd.DataFrame(StandardScaler().fit_transform(np.prod(np.array(weighted_signals), axis=0)),\n",
    "#                             index=cdict2.matrices[0].index, columns=cdict2.matrices[0].columns)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all(result.importances_mean.argsort()==np.argsort(result.importances_mean))\n",
    "# all(clf.feature_importances_.argsort()==np.argsort(clf.feature_importances_))\n",
    "# sorted(result.keys())\n",
    "# np.argsort(result.importances)==np.argsort(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(tree_importance_sorted_idx, tree_importance_sorted_idx.shape,\n",
    "#         tree_indices, tree_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display(sorted(clf.__dict__.keys()),\n",
    "#         result, perm_sorted_idx, feature_importance_sorted,\n",
    "#         feature_importance_sorted.shape)\n",
    "# display(np.argsort(clf.feature_importances_))\n",
    "# feature_importance_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.importances[perm_sorted_idx].T.shape\n",
    "# help(np.argsort)\n",
    "# perm_sorted_idx\n",
    "# X.iloc[:, perm_sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(512, 256))\n",
    "# ax1.barh(tree_indices, feature_importance_sorted, height=0.7)\n",
    "# ax1.set_yticks(tree_indices)\n",
    "# ax1.set_yticklabels(X.iloc[:, tree_importance_sorted_idx].columns)\n",
    "# ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "# ax2.boxplot(result.importances[perm_sorted_idx].T,\n",
    "#             vert=False,\n",
    "#             labels=X.iloc[:, perm_sorted_idx].columns)\n",
    "# fig.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(512, 256))\n",
    "# corr = spearmanr(X).correlation\n",
    "\n",
    "# # Ensure the correlation matrix is symmetric\n",
    "# corr = (corr + corr.T) / 2\n",
    "# np.fill_diagonal(corr, 1)\n",
    "\n",
    "# # We convert the correlation matrix to a distance matrix before performing\n",
    "# # hierarchical clustering using Ward's linkage.\n",
    "# distance_matrix = 1 - np.abs(corr)\n",
    "# dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "\n",
    "# dendro = hierarchy.dendrogram(dist_linkage, labels=X.columns.tolist(),\n",
    "#                               ax=ax1, leaf_rotation=90)\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "# plt.savefig('/data/simexp/fnadeau/hierarchical_clustering_dendrogram.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = spearmanr(X).correlation\n",
    "\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "\n",
    "# dendro = hierarchy.dendrogram(dist_linkage, labels=X.columns.tolist(),\n",
    "#                               ax=ax1, leaf_rotation=90)\n",
    "# dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "cluster_ids = hierarchy.fcluster(dist_linkage, 1, criterion=\"distance\")\n",
    "cluster_id_to_feature_ids = defaultdict(list)\n",
    "\n",
    "for idx, cluster_id in enumerate(cluster_ids):\n",
    "    cluster_id_to_feature_ids[cluster_id].append(idx)\n",
    "selected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n",
    "\n",
    "X_train_sel = X_train.iloc[:, selected_features]\n",
    "X_test_sel = X_test.iloc[:, selected_features]\n",
    "\n",
    "clf_sel = RandomForestClassifier(n_estimators=X_train_sel.shape[0],\n",
    "                                 random_state=42)\n",
    "clf_sel.fit(X_train_sel, y_train)\n",
    "print(\n",
    "    \"Accuracy on test data with features removed: {:.2f}\".format(\n",
    "        clf_sel.score(X_test_sel, y_test)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('/data/simexp/fnadeau/best_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = cdict2.signal_matrix.iloc[:, selected_features]\n",
    "\n",
    "top_features.to_csv(f'/data/simexp/fnadeau/best_features/{sess00.sub_id}_{sess00.ses_id}_task-memory_bestfeatures.tsv',\n",
    "                    sep='\\t', index='trial_type', encoding='UTF-8-SIG')\n",
    "# top_features.columns.tolist()\n",
    "# top_features.iloc[:, 0].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodfeatures = top_features#.iloc[:, selected_features]\n",
    "\n",
    "selected_distances = pd.DataFrame(scipy.stats.spearmanr(pairwise_distances(goodfeatures.T.values))[0],\n",
    "                                  index=goodfeatures.columns,\n",
    "                                  columns=goodfeatures.columns)\n",
    "\n",
    "masked_distances = selected_distances.where(selected_distances.values !=\n",
    "                                            np.tril(selected_distances.values))\n",
    "masked_distances = masked_distances.where(masked_distances.values!=0)\n",
    "\n",
    "masked_distances = masked_distances.where(~np.array([row[1].fillna(0).between(-0.50, 0.50).values for row\n",
    "                                           in masked_distances.iterrows()]))\n",
    "masked_distances = masked_distances.dropna(axis=1, how='all').dropna(axis=0, how='all')\n",
    "\n",
    "the_heatmap = sns.heatmap(masked_distances)\n",
    "the_heatmap.get_figure().savefig(f'/data/simexp/fnadeau/best_features/{sess00.sub_id}_{sess00.ses_id}_task-memory_bestfeatures_heatmap.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_distances.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def weightings(signals: pd.DataFrame,\n",
    "#                weights: pd.DataFrame,\n",
    "#                condition_labels: Union[list, pd.Index]\n",
    "#                ) -> pd.DataFrame:\n",
    "# #                weight_labels: Union[list, pd.Index] = None\n",
    "#     \"\"\"\n",
    "#     Return condition-wise weighted signals DataFrame.\n",
    "#     \"\"\"\n",
    "\n",
    "#     newsignals = signals.set_axis(condition_labels, axis=0)\n",
    "# #     if weight_labels is not None:\n",
    "# #         weights = weights.set_axis(weight_labels, axis=0)\n",
    "#     for cond in weights.index.unique():\n",
    "#         newsignals.loc[cond] = newsignals.loc[cond]*weights.loc[cond]\n",
    "#     return newsignals\n",
    "\n",
    "\n",
    "# def get_weighted_signals(signals: pd.DataFrame,\n",
    "#                          weights: list,\n",
    "#                          condition_labels: Union[list, pd.Index],\n",
    "# #                          weight_labels: Union[list, pd.Index] = None,\n",
    "#                          standardize: bool = True,\n",
    "#                          **kwargs\n",
    "#                          ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Return condition-wise weighted signals DataFrame for all conditions.\n",
    "    \n",
    "#     Args:\n",
    "#         signals\n",
    "#     \"\"\"\n",
    "\n",
    "#     orig_index, orig_cols = signals.index, signals.columns\n",
    "# #     if weight_labels is None:\n",
    "# #         weight_labels = [[]] * len(weights)\n",
    "# # weight_labels=weight_labels[weight[0]]\n",
    "#     w0 = [weightings(signals, weights=weights[weight[0]],\n",
    "#                      condition_labels=condition_labels[weight[0]])\n",
    "#           for weight in enumerate(weights)]\n",
    "#     slist = [signals]+w0\n",
    "#     data = np.prod(np.array(slist), axis=0)\n",
    "#     if standardize is True:\n",
    "#         data = StandardScaler().fit_transform(data)\n",
    "#     return pd.DataFrame(data, index=orig_index, columns=orig_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_weighted_signals(cdict2.whole.signals, cdict2[key].signals,\n",
    "#                      cdict2[key].condition_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess00.events.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# trial_type_labels = [sess00.events.trial_type, sess00.events.recognition_performance,\n",
    "#                      sess00.events.ctl_miss_ws_cs]\n",
    "# cr_results = [validate_model(ovrc, top_features,#cdict2.signal_matrix,\n",
    "#                                   y=ttlabels)\n",
    "#                     for ttlabels in trial_type_labels]\n",
    "\n",
    "# [cr_results[col[0]].to_csv(f'/data/simexp/fnadeau/best_features/{sess00.sub_id}_{sess00.ses_id}_{col[1]}-classification-report.tsv',\n",
    "#                 sep='\\t', index=True, encoding='UTF-8-SIG')\n",
    "#  for col in enumerate(['trial_type', 'recognition_performance', 'ctl_miss_ws_cs'])]\n",
    "# display(*crossval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(tol=0.0001,\n",
    "          kernel='linear',\n",
    "          cache_size=5,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "\n",
    "ovrc = OneVsRestClassifier(svc)\n",
    "\n",
    "X, y = cdict2.signal_matrix.astype(np.float16), sess00.events.iloc[:, -1]\n",
    "\n",
    "validation_params = dict(test_size=0.4, shuffle=True,\n",
    "                         stratify=y, random_state=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    **validation_params)\n",
    "ovrc.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "result = permutation_importance(ovrc, X_train, y_train, n_repeats=10, random_state=42)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "X, y = cdict2.signal_matrix, sess00.events.iloc[:, -1]\n",
    "\n",
    "validation_params = dict(test_size=0.4, shuffle=True,\n",
    "                         stratify=y, random_state=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    **validation_params)\n",
    "clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42)\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.barh(tree_indices, clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n",
    "ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "ax2.boxplot(\n",
    "    result.importances[perm_sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=data.feature_names[perm_sorted_idx],\n",
    ")\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "corr = spearmanr(X).correlation\n",
    "\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "dendro = hierarchy.dendrogram(\n",
    "    dist_linkage, labels=data.feature_names.tolist(), ax=ax1, leaf_rotation=90\n",
    ")\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlpX = fullweighted_scaled\n",
    "# mlpY = y = ctl_miss_ws_cs_mat[1]\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', activation='logistic',\n",
    "                    hidden_layer_sizes=mlpX.shape,\n",
    "                    max_iter=3000, max_fun=225000)\n",
    "\n",
    "def validate_mlp(estimator,\n",
    "                 X, y,\n",
    "                 test_size=0.6,\n",
    "                 shuffle=True,\n",
    "                 random_state=0,\n",
    "                 **kwargs):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=0.4,\n",
    "                                                        shuffle=True,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state=None)\n",
    "\n",
    "#     mlp = MLPClassifier(solver=solver, activation=activation,\n",
    "#                         hidden_layer_sizes=mlpX.shape,\n",
    "#                         max_iter=3000, max_fun=225000)\n",
    "\n",
    "    estimator.fit(X_train, y_train)\n",
    "\n",
    "#     mlp_y_pred = estimator.predict(X_test)\n",
    "    acc_score = accuracy_score(y_pred=estimator.predict(X_test), y_true=y_test)\n",
    "\n",
    "#     cr = pd.DataFrame(classification_report(y_pred=mlp_y_pred,\n",
    "#                                             y_true=y_test,\n",
    "#                                             output_dict=True,\n",
    "#                                             zero_division=1))\n",
    "    return acc_score\n",
    "\n",
    "loo_results = pd.Series(((validate_mlp(mlp, X=fws_squared.drop(col,axis=1),\n",
    "                                       y=sess00.events.iloc[:, -1])\n",
    "                          for col in tqdm(fws_squared.columns))),\n",
    "                        index=fws_squared.columns)\n",
    "# display(*[validate_mlp(mlp, X=mlpX, y=sess00.events.trial_type),\n",
    "#           validate_mlp(estimator=mlp, X=mlpX, y=sess00.events.recognition_performance),\n",
    "#           validate_mlp(estimator=mlp, X=mlpX, y=ctl_miss_ws_cs_mat[1])])\n",
    "# mlp.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "from sklearn.decomposition import FastICA, PCA # IncrementalPCA\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "impaths = sorted(filter(os.path.isfile, sorted(Path('/data/simexp/fnadeau/wmstim/').iterdir())))\n",
    "imdict = pd.DataFrame(((StandardScaler().fit_transform(np.uint8(resize(skimage.io.imread(\n",
    "             apath, as_gray=True), (250, 250), anti_aliasing=True).flatten()\n",
    "                                                                for apath in impaths)))),\n",
    "                      index=list(map(str.lower, list(map(os.path.basename, impaths)))))\n",
    "\n",
    "stimlist = sess00.events.stim_file.fillna('empty_box_gris.bmp').str.lower().tolist()\n",
    "stimdata = imdict.loc[stimlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca00 = PCA().fit(mainmat,sess00.events.iloc[:,-1])\n",
    "# ica00 = FastICA(max_iter=600, tol=0.05).fit(mainmat)\n",
    "\n",
    "# ica_results00 = ica00.fit_transform(mainmat)\n",
    "# validate_model(X=ica_results00,y=sess00.events.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_results = pd.read_csv('/data/simexp/fnadeau/loo_1024_results.tsv',\n",
    "                          sep='\\t', index_col='difumo_names').rename({'0':'accuracy'}, axis=1)\n",
    "list(loo_results.sort_values('accuracy', ascending=True).iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loo_results.to_csv('/data/simexp/fnadeau/loo_1024_results.tsv',\n",
    "                   sep='\\t', encoding='UTF-8-SIG', index='difumo_names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(solver='adam', activation='logistic',\n",
    "                    hidden_layer_sizes=mlpX.shape,\n",
    "                    max_iter=3000, max_fun=225000)\n",
    "\n",
    "def leave_one_out(X,y,n_tests=8,**kwargs):\n",
    "    return pd.DataFrame(((pd.Series((validate_mlp(estimator=mlp,X=X.drop(col,axis=1), y=y)\n",
    "                                   for col in X.columns),\n",
    "                                  index=X.columns)\n",
    "                          for n in tqdm(range(n_tests)))))\n",
    "# loss = leave_one_out(X=fullweighted_scaled, y=ctl_miss_ws_cs_mat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(FeatureAgglomeration)n_clusters=#fullweighted_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*[validate_model(X = fullweighted_scaled,\n",
    "                        y = enc_ctl_mat[1]),\n",
    "          validate_model(X = fullweighted_scaled,\n",
    "                        y = recog_perfo_mat[1]),\n",
    "          validate_model(X = fullweighted_scaled,\n",
    "                        y = ctl_miss_ws_cs_mat[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = [sess00.events.trial_type, sess00.events.recognition_performance,\n",
    "         sess00.events.ctl_miss_ws_cs]\n",
    "X_train, X_test, y_train, y_test = train_test_split(fullweighted_scaled,\n",
    "                                                    sess00.events.trial_type,\n",
    "                                                    test_size=0.7,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=sess00.events.trial_type,\n",
    "                                                    random_state=None)\n",
    "mlp = MLPClassifier(solver='adam', activation='logistic',\n",
    "                    hidden_layer_sizes=mlpX.shape,\n",
    "                    max_iter=3000, max_fun=225000)\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "round(len(list(filter(None, mlp.predict(X_test)==y_test)))/len(y_test)*100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "coefs_, intercepts_, loss_curve = itemgetter(*['coefs_','intercepts_','loss_curve_'])(mlp.__dict__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullweighted_scaled.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best2 = FeatureAgglomeration().fit_transform(fullweighted_scaled)\n",
    "# best2\n",
    "# help(FeatureAgglomeration)\n",
    "from sklearn.decomposition import IncrementalPCA, PCA, FastICA\n",
    "\n",
    "pca = PCA().fit(fullweighted_scaled,fullweighted_scaled.index)\n",
    "ica = FastICA(max_iter=600, tol=0.05).fit(fullweighted_scaled)\n",
    "\n",
    "# pca_results = pca.fit_transform(fullweighted_scaled.T,fullweighted_scaled.columns) #sess00.events.trial_type)\n",
    "# pca.shape\n",
    "# help(FastICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_,index=fullweighted_scaled.index,\n",
    "             columns=fullweighted_scaled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ica.components_,index=fullweighted_scaled.index,\n",
    "             columns=fullweighted_scaled.columns)\n",
    "ica.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.__dict__.keys()\n",
    "pd.Series(pca.explained_variance_ratio_,index=pca.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "aggclus = AgglomerativeClustering(n_clusters=16, affinity='euclidean', memory=None,\n",
    "                                  connectivity=conn, compute_full_tree=True, linkage='ward',\n",
    "                                  distance_threshold=None, compute_distances=True)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "#                                                         **validation_params)\n",
    "# aggclus.\n",
    "mds = MDS(n_components=32, metric=True, n_init=4,\n",
    "          max_iter=300, eps=0.001, random_state=None,\n",
    "          dissimilarity='precomputed')\n",
    "newdims = mds.fit_transform(pairwise_distances(fullweighted_scaled))\n",
    "\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "empcov = EmpiricalCovariance(store_precision=True, assume_centered=True)\n",
    "\n",
    "covmat = pd.DataFrame(empcov.fit(cdict2.signal_matrix).covariance_,\n",
    "                      index=cdict2.signal_matrix.columns,\n",
    "                      columns=cdict2.signal_matrix.columns)\n",
    "\n",
    "# help(MDS)\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "\n",
    "cm = ConnectivityMeasure(cov_estimator=MinCovDet(),\n",
    "                         kind='covariance')\n",
    "\n",
    "# conn = cm.fit_transform([MinMaxScaler().fit_transform(fullweighted_scaled.values)])\n",
    "\n",
    "conn = cm.fit_transform([cdict2.signal_matrix.values])\n",
    "conn\n",
    "# sns.heatmap(conn[0])\n",
    "# pd.DataFrame(MinMaxScaler().fit_transform(cdict2.signal_matrix.cov()),\n",
    "#              index=cdict2.signal_matrix.columns,\n",
    "#              columns=cdict2.signal_matrix.columns).min().min()\n",
    "# partdep = partial_dependence(mcd, cdict2.signal_matrix, cdict2.signal_matrix.columns,\n",
    "#                              response_method='auto', percentiles=(0.05, 0.95),\n",
    "#                              grid_resolution=cdict2.signal_matrix.shape[0],\n",
    "#                              method='auto', kind='legacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims=[64,128,256,512,1024]\n",
    "\n",
    "# [get_difumo_cut_coords(dim, resolution_mm=3,\n",
    "#                        data_dir=atlases_dir,\n",
    "#                        as_dataframe=False,\n",
    "#                       output_dir='/data/simexp/fnadeau/')\n",
    "#  for dim in dims]\n",
    "\n",
    "# fullweighted_scaled.shape\n",
    "# sess00.events\n",
    "# label_list = [itm[1] for itm in list(sess00.events[['trial_type','recognition_performance',\n",
    "#                                                 'ctl_miss_ws_cs']].iteritems())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difumo = get_difumo(data_dir=atlases_dir, dimension=256, resolution_mm=3)\n",
    "\n",
    "contrasts = [[NiftiMapsMasker(maps_img=difumo.maps,\n",
    "                              mask_img=sess00.mask_img,\n",
    "                              resampling_target='maps').fit(sess00.fmri_img).inverse_transform(\n",
    "                 fullweighted_scaled.set_index(labels).loc[trial]) for trial in tuple(set(labels))]\n",
    "             for labels in label_list]\n",
    "# help(NiftiMapsMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = list(more_itertools.flatten(contrasts))\n",
    "\n",
    "len(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ipca00 = IncrementalPCA().fit(X, X.columns)\n",
    "pca00 = PCA().fit(X, X.columns)\n",
    "# ipca_table = pd.DataFrame(ipca00.explained_variance_)\n",
    "pca_table = pd.DataFrame(pca00.explained_variance_ratio_)\n",
    "# # ipca00.__dict__\n",
    "# ipca_table = pd.DataFrame(ipca00.explained_variance_ratio_)\n",
    "# pca_table = pd.DataFrame(pca00.explained_variance_ratio_)\n",
    "display(pca_table)\n",
    "sorted(ipca00.__dict__.keys())\n",
    "# Bunch(**pca00.__dict__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca00.singular_values_, pca00.feature_names_in_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "trial_labels = [ev.trial_type for ev in test_glm_events[1:]]\n",
    "bestof = np.array([SelectKBest().fit(fullweighted, ev).get_feature_names_out()\n",
    "          for ev in trial_labels])\n",
    "# [best.shape for best in bestof]\n",
    "allbest = sorted(set(bestof.flatten()))\n",
    "allbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullweighted[allbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess00.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_miss_ws_cs_mat[0][allbest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Control Trials Within-Category Correlations Across Labels\n",
    "# enc_ctl_ctl = enc_ctl_mat[0][allbest].set_index(\n",
    "#                          sess00.events.trial_type).loc['Ctl'].T.corr()\n",
    "# recognition_performance_ctl = recog_perfo_mat[0][allbest].set_index(\n",
    "#                          sess00.events.recognition_performance).loc['Ctl'].T.corr()\n",
    "# ctl_miss_ws_cs_ctl = ctl_miss_ws_cs_mat[0][allbest].set_index(\n",
    "#                          sess00.events.ctl_miss_ws_cs).loc['Ctl'].T.corr()\n",
    "\n",
    "# ctl_mats = [enc_ctl_ctl, recognition_performance_ctl, ctl_miss_ws_cs_ctl]\n",
    "# [mat.where(mat.values!=np.tril(mat.values), inplace=True) for mat in ctl_mats]\n",
    "\n",
    "# [mat.where(mat.values!=np.diag(mat.values), inplace=True) for mat in ctl_mats]\n",
    "\n",
    "# enc_ctl_ctl, recognition_performance_ctl, ctl_miss_ws_cs_ctl = ctl_mats\n",
    "# # fullweighted_euc = pd.DataFrame(pairwise_distances(fullweighted[allbest]),\n",
    "# #                                 columns=sess00.events.trial_type)\n",
    "# sns.set(rc={'figure.figsize': (15,15)})\n",
    "# display(sns.heatmap(enc_ctl_ctl), sns.heatmap(recognition_performance_ctl),\n",
    "#         sns.heatmap(ctl_miss_ws_cs_ctl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import skimage\n",
    "import imageio\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "\n",
    "impaths = sorted(filter(os.path.isfile, sorted(Path('/data/simexp/fnadeau/wmstim/').iterdir())))\n",
    "ctl_stim = '/data/simexp/fnadeau/wmstim/empty_box_gris.bmp'\n",
    "stimfiles = [apath for apath in impaths if os.path.basename(apath) in\n",
    "             list(map(str.lower, sess00.events.stim_file.fillna(ctl_stim)))]\n",
    "# stimfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import antialias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reshaped = np.array([np.reshape(p, (-1, 2)) for p in patches])\n",
    "patches = np.array(patches)\n",
    "euc_imgs = pairwise_distances(patches)\n",
    "# [p.shape for p in reshaped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "euc_trials = pd.DataFrame(pairwise_distances(X),\n",
    "                           index=sess00.events.trial_type,\n",
    "                           columns=fullweighted.columns)\n",
    "\n",
    "# euc_rois = pd.DataFrame(pairwise_distances(X.T),\n",
    "#                            columns=X.columns,\n",
    "#                         index=X.columns)\n",
    "# euc_vectors.shape, euc_rois.shape\n",
    "sns.heatmap(euc_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "roi_cov = EmpiricalCovariance(assume_centered=False).fit(euc_rois).covariance_\n",
    "sns.heatmap(roi_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.second_level import make_second_level_design_matrix\n",
    "from nilearn.glm.second_level import SecondLevelModel\n",
    "first_level_list = list(val.model for val in contrasts_effsiz.values())\n",
    "first_designs = [glm.design_matrices_[0] for glm in first_level_list]\n",
    "\n",
    "secondglm = SecondLevelModel().fit(first_level_list,\n",
    "                                   design_matrix=first_designs)\n",
    "# design_matrix = make_second_level_design_matrix(subjects_label,\n",
    "#                                                 extra_info_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondglm.generate_report(list(val.contrast_img.get_fdata() for val in contrasts_effsiz.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize': (15,15)})\n",
    "covmap = X.cov().where(X.cov()!=np.tril(X.cov().values))\n",
    "covmap = covmap.where(covmap!=np.diag(covmap.values))\n",
    "covmap = covmap.where(covmap.values!=0)\n",
    "covmap = covmap.dropna(axis=1,how='all').dropna(axis=0,how='all')\n",
    "centers = pd.Series([(row[1].dropna().quantile(0.1),\n",
    "                      row[1].dropna().quantile(0.9))\n",
    "                     for row in covmap.iterrows()],\n",
    "                    index=covmap.index)\n",
    "# sns.heatmap(covmap.where(~covmap.isin(centers.values)))\n",
    "# highest = covmap.where([~row[1].between(*centers.loc[row[0]])\n",
    "#                        for row in covmap.iterrows()])\n",
    "# sns.heatmap(covmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = selector.fit(X, X.index).get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(StandardScaler().fit_transform(fullweighted),\n",
    "                 columns=fullweighted.columns,\n",
    "                 index=fullweighted.index)\n",
    "\n",
    "y = enc_ctl_mat[1]\n",
    "\n",
    "selector = RFE(estimator=svc,\n",
    "               n_features_to_select=0.2)\n",
    "# selector.fit(X).get_feature_names_out()\n",
    "validation_params00 = dict(test_size=0.4, shuffle=True,\n",
    "                           stratify=wholemat.index, random_state=None)\n",
    "\n",
    "X_train00, X_test00, y_train00, y_test00 = train_test_split(X, y,\n",
    "                                                            **validation_params00)\n",
    "\n",
    "ovrc.fit(X_train00, y_train00)\n",
    "\n",
    "y_pred_test = ovrc.predict(X_test00)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "conf_mat = confusion_matrix(y_true=y_test00, y_pred=y_pred_test,\n",
    "                            labels=y.unique())\n",
    "# y_pred = cross_val_predict(svc, X_test00, y_test00,\n",
    "#                            groups=y_test00, cv=5)\n",
    "acc = accuracy_score(y_pred=y_pred_test, y_true=y_test00)\n",
    "cr_test = pd.DataFrame(classification_report(y_pred=y_pred_test,\n",
    "                                             y_true=y_test00,\n",
    "                                             output_dict=True))\n",
    "\n",
    "display(cr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ConfusionMatrixDisplay(conf_mat).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{trial: np.array(test_glm_event.trial_type==trial).astype(int)\n",
    " for trial in test_glm_event.trial_type}\n",
    " for test_glm_event in test_glm_events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vistemp_rois = difumo_cut_coords.set_index('difumo_names').T.filter(regex='visual|occip|temp').columns\n",
    "vistemp_rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_trials = test_signals_matrix.dot(\n",
    "                 test_signals_matrix.iloc[:-1,:].T).iloc[:-1,:].set_index(\n",
    "                     sess00.events.trial_type).set_axis(sess00.events.trial_type, axis=1)\n",
    "axis_range = range(int(round(dot_trials.mean().min(), 0)),\n",
    "                   int(round(dot_trials.mean().max(), 0)))\n",
    "\n",
    "def autocorr_plot(inpt):\n",
    "    import statsmodels.tsa.api as smt\n",
    "    acf = smt.graphics.plot_acf(inpt, lags=40 , alpha=0.05)\n",
    "    acf.show()\n",
    "\n",
    "autocorr_plot(dot_trials.mean())\n",
    "# sns.regplot(dot_trials.mean(), dot_trials.mean())\n",
    "# dot_trials.plot.scatter(x=dot_trials.index, y=axis_range)\n",
    "# sns.heatmap(dot_trials.where(dot_trials!=np.tril(dot_trials.values)))\n",
    "# sns.scatterplot(dot_trials.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ctl = test_signals_matrix[vistemp_rois].iloc[:-1].set_index(sess00.events.trial_type)\n",
    "recog_perfo = test_signals_matrix[vistemp_rois].iloc[:-1].set_index(sess00.events.recognition_performance)\n",
    "ctl_miss_ws_cs = test_signals_matrix[vistemp_rois].iloc[:-1].set_index(sess00.events.ctl_miss_ws_cs)\n",
    "\n",
    "scaled = pd.DataFrame(StandardScaler().fit_transform(ctl_miss_ws_cs.values),\n",
    "                      columns=enc_ctl.columns, index=ctl_miss_ws_cs.index)\n",
    "\n",
    "svc = SVC(tol=0.05,\n",
    "          kernel='linear',\n",
    "          cache_size=500,\n",
    "          decision_function_shape='ovr',\n",
    "          class_weight='balanced',\n",
    "          probability=True)\n",
    "ovrc = OneVsRestClassifier(svc)\n",
    "selector = RFE(estimator=svc,\n",
    "               n_features_to_select=0.5)\n",
    "\n",
    "simple_signals_matrix\n",
    "enc_ctl_names = selector.fit(enc_ctl, enc_ctl.index).get_feature_names_out()\n",
    "recog_perfo_names = selector.fit(recog_perfo, recog_perfo.index).get_feature_names_out()\n",
    "ctl_miss_ws_cs_names = selector.fit(ctl_miss_ws_cs, ctl_miss_ws_cs.index).get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC(zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = enc_ctl_names.tolist()+recog_perfo_names.tolist()+ctl_miss_ws_cs_names.tolist()\n",
    "features = set(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [enc_ctl[features], recog_perfo[features], ctl_miss_ws_cs[features]]\n",
    "[itm.shape for itm in features_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_perfo_scores = []\n",
    "rpmatrix = recog_perfo[recog_perfo_names]\n",
    "for col in rpmatrix.columns:\n",
    "    X, y = rpmatrix[[col]], rpmatrix.index.tolist()\n",
    "    X_train00, X_test00, y_train00, y_test00 = train_test_split(X, y,\n",
    "                                                                test_size=0.4,\n",
    "                                                                shuffle=True,\n",
    "                                                                random_state=None,\n",
    "                                                                stratify=rpmatrix.index)\n",
    "\n",
    "    ovrc.fit(X_train00, y_train00)\n",
    "\n",
    "    y_pred_test = ovrc.predict(X_test00)\n",
    "    # y_pred = cross_val_predict(svc, X_test00, y_test00,\n",
    "    #                            groups=y_test00, cv=5)\n",
    "    acc = accuracy_score(y_pred=y_pred_test, y_true=y_test00)\n",
    "    cr_test = pd.DataFrame(classification_report(y_pred=y_pred_test,\n",
    "                                                 y_true=y_test00,\n",
    "                                                 output_dict=True,\n",
    "                                                 zero_division=0))\n",
    "    recog_perfo_scores.append(acc)\n",
    "rp_acc = pd.Series(data=recog_perfo_scores,\n",
    "                   name='rp_acc',\n",
    "                   index=rpmatrix.columns).sort_values(ascending=False)\n",
    "rp_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_scores = []\n",
    "for label_type in features_list:\n",
    "    matrix = label_type\n",
    "    perfo_list = []\n",
    "    for col in matrix.columns:\n",
    "        X, y = matrix[[col]], matrix.index.tolist()\n",
    "        X_train00, X_test00, y_train00, y_test00 = train_test_split(X, y,\n",
    "                                                                    test_size=0.4,\n",
    "                                                                    shuffle=True,\n",
    "                                                                    random_state=None,\n",
    "                                                                    stratify=matrix.index)\n",
    "\n",
    "        ovrc.fit(X_train00, y_train00)\n",
    "\n",
    "        y_pred_test = ovrc.predict(X_test00)\n",
    "        # y_pred = cross_val_predict(svc, X_test00, y_test00,\n",
    "        #                            groups=y_test00, cv=5)\n",
    "        acc = accuracy_score(y_pred=y_pred_test, y_true=y_test00)\n",
    "        cr_test = pd.DataFrame(classification_report(y_pred=y_pred_test,\n",
    "                                                     y_true=y_test00,\n",
    "                                                     output_dict=True,\n",
    "                                                     zero_division=0))\n",
    "        perfo_list.append(acc)\n",
    "    total_scores.append(perfo_list)\n",
    "#     perfo_list.append(f'ROI: {col}\\n\\nTest Accuracy\\n{acc}\\n\\nTest Classification Report{cr_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accdf = pd.DataFrame(total_scores, columns=features_list[0].columns,\n",
    "                     index=['enc_ctl', 'recog_perfo',\n",
    "                            'ctl_miss_ws_cs']).T.sort_values('recog_perfo',\n",
    "                                                             ascending=False)\n",
    "accdf\n",
    "# np.array(total_scores).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def roi_based_score(X, y, test_size=0.4, shuffle=True,\n",
    "                    stratify=None, random_state=None):\n",
    "    scores = []\n",
    "    validation_params = dict(test_size=test_size, shuffle=shuffle,\n",
    "                             stratify=stratify, random_state=random_state)\n",
    "    for col in X.columns:\n",
    "\n",
    "        X_transformed = StandardScaler().fit_transform(X)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_transformed,\n",
    "                                                            y.values.tolist(),\n",
    "                                                            **validation_params)\n",
    "        svc = SVC(tol=0.01, kernel='linear',\n",
    "                  cache_size=5,\n",
    "                  decision_function_shape='ovo',\n",
    "                  class_weight='balanced',\n",
    "                  probability=True).fit(X_train, y_train)\n",
    "        ovoc = OneVsOneClassifier(svc)\n",
    "        # Cross-Validation\n",
    "#         cv = pd.Series(y_train).value_counts().min()\n",
    "#         y_pred_train = cross_val_predict(ovoc, X_train, y_train,\n",
    "#                                            groups=y_train, cv=cv)\n",
    "#         # Scores\n",
    "#         cv_acc_train = cross_val_score(ovoc, X_train, y_train,\n",
    "#                                          groups=y_train, cv=cv)\n",
    "#         overall_acc = accuracy_score(y_pred=y_pred_train, y_true=y_train)\n",
    "        # Testing Performance\n",
    "        ovoc.fit(X_train, y_train)\n",
    "        y_pred_test = ovoc.predict(X_test)\n",
    "        acc_test = ovoc.score(X_test, y_test) # get accuracy\n",
    "        scores.append(acc_test)\n",
    "    return pd.DataFrame(scores, index=X.columns,\n",
    "                        columns=['Accuracy'])\n",
    "\n",
    "contrasts_effsiz = get_contrasts(session=sess00,\n",
    "                                 masker=spheres_masker,\n",
    "                                 output_type='effect_size',\n",
    "                                 glm_kws={'signal_scaling': False,\n",
    "#                                           'high_pass': 0.17,\n",
    "                                          'standardize': True},\n",
    "                                 trial_type_col='ctl_miss_ws_cs',\n",
    "                                 labels=difumo_cut_coords.difumo_names)\n",
    "\n",
    "# result_roi = roi_based_score(X=contrasts_effsiz.signals.iloc[:-1, :],\n",
    "#                              y=sess00.events.trial_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ctl = contrasts_effsiz.signals.iloc[:-1,:].set_index(sess00.events.trial_type)\n",
    "recog_perfo = contrasts_effsiz.signals.iloc[:-1,:].set_index(sess00.events.recognition_performance)\n",
    "ctl_miss_ws_cs = contrasts_effsiz.signals.iloc[:-1,:].set_index(sess00.events.ctl_miss_ws_cs)\n",
    "\n",
    "scaled = pd.DataFrame(StandardScaler().fit_transform(ctl_miss_ws_cs.values),\n",
    "                      columns=enc_ctl.columns, index=ctl_miss_ws_cs.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# import seaborn as sns\n",
    "# mask = enc_ctl.cov().where(enc_ctl.cov() == np.triu(enc_ctl.cov().values))\n",
    "# mask = mask.where(mask.values!=np.diag(mask.values))\n",
    "# mask = mask.where(mask.values!=0)\n",
    "\n",
    "# centers = pd.Series([(row[1].dropna().quantile(0.1),\n",
    "#                       row[1].dropna().quantile(0.9))\n",
    "#                      for row in mask.iterrows()],\n",
    "#                     index=mask.index)\n",
    "# highest = mask.where([~row[1].between(*centers[row[0]])\n",
    "#                       for row in mask.iterrows()])\n",
    "\n",
    "# sns.set(rc={'figure.figsize': (15,15)})\n",
    "# sns.heatmap(highest, square=True)\n",
    "# skip_diag_strided(mask)\n",
    "# sns.heatmap(data=enc_ctl.cov(), mask=enc_ctl.cov().where(enc_ctl.cov()==np.triu(enc_ctl.cov().values)))\n",
    "# mask=enc_ctl.cov().where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_ctl.plot.scatter(y=enc_ctl.index, x=enc_ctl.columns)\n",
    "# sns.scatterplot(data=)\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "minmax_scaled = pd.DataFrame(MinMaxScaler().fit_transform(enc_ctl.values),\n",
    "                      columns=enc_ctl.columns, index=enc_ctl.index)\n",
    "maxabs_scaled = pd.DataFrame(MaxAbsScaler().fit_transform(enc_ctl.values),\n",
    "                      columns=enc_ctl.columns, index=enc_ctl.index)\n",
    "\n",
    "sns.set(rc={'figure.figsize': (10,10)})\n",
    "# sns.scatterplot(data=scaled.groupby(scaled.index).mean())\n",
    "# sns.scatterplot(data=maxabs_scaled.values)\n",
    "# fig, ax = plt.subplots(4, 8, sharex='col', sharey='row')\n",
    "# [scaled.T[col].plot.scatter() for col in scaled.T.columns]\n",
    "# [sns.scatterplot(data=row[1], figure=fig, ax=ax) for row in scaled.T.iterrows()]\n",
    "# mask.where(~np.array([row[1].between(-1.9, 0.2)\n",
    "#             for row in mask.iterrows()]))\n",
    "# mask.describe()#.max()-mask.min()\n",
    "# [row[1].dropna().max()-row[1].dropna().min()\n",
    "#  for row in mask.iterrows()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X00 = enc_ctl[best_of]\n",
    "y00 = maxabs_scaled.index\n",
    "# X00, y00 = signal_table00, signal_table00.index\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "validation_params00 = dict(test_size=0.3, shuffle=True,\n",
    "                           stratify=y00, random_state=None)\n",
    "\n",
    "X_transformed00 = StandardScaler().fit_transform(X00)\n",
    "\n",
    "X_train00, X_validate00, y_train00, y_validate00 = train_test_split(X_transformed00, y00,\n",
    "                                                                    **validation_params00)\n",
    "cv = y_train00.value_counts().min()\n",
    "\n",
    "svc = SVC(tol=0.05,\n",
    "                              kernel='poly',\n",
    "                              cache_size=500,\n",
    "                              decision_function_shape='ovr',\n",
    "                              class_weight='balanced',\n",
    "                              probability=True)\n",
    "\n",
    "svc.fit(X_train00, y_train00)\n",
    "accuracy_score(y_pred=svc.predict(X_validate00), y_true=y_validate00)\n",
    "cr_test = pd.DataFrame(classification_report(y_pred=y_pred_test00,\n",
    "                                               y_true=y_validate00,\n",
    "                                               output_dict=True))\n",
    "# cross_val_predict(svc, X_train00, y_train00,\n",
    "#                                    groups=y_train00, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cross-Validation\n",
    "y_pred_train00 = cross_val_predict(ovoc, X_train00, y_train00,\n",
    "                                   groups=y_train00, cv=cv)\n",
    "# Scores\n",
    "cv_acc_train00 = cross_val_score(ovoc, X_train00, y_train00,\n",
    "                                 groups=y_train00, cv=cv)\n",
    "\n",
    "# Overall model performance evaluation on training data\n",
    "overall_acc00 = accuracy_score(y_pred=y_pred_train00, y_true=y_train00)\n",
    "overall_cr00 = pd.DataFrame(classification_report(y_pred=y_pred_train00,\n",
    "                                                  y_true=y_train00,\n",
    "                                                  output_dict=True))\n",
    "# Testing Performance\n",
    "ovoc.fit(X_train00, y_train00)\n",
    "y_pred_test00 = ovoc.predict(X_validate00)\n",
    "acc_test00 = ovoc.score(X_validate00, y_validate00) # get accuracy\n",
    "acc_test200 = accuracy_score(y_pred=y_pred_test00, y_true=y_validate00)\n",
    "cr_test00 = pd.DataFrame(classification_report(y_pred=y_pred_test00,\n",
    "                                               y_true=y_validate00,\n",
    "                                               output_dict=True))\n",
    "\n",
    "percent_train_ok = round((len(list(filter(None, y_pred_train00==y_train00))) /\n",
    "                          len(y_train00))*100, 2)\n",
    "\n",
    "percent_ok = round((len(list(filter(None, y_pred_test00==y_validate00))) /\n",
    "                    len(y_validate00))*100, 2)\n",
    "\n",
    "train_report00 = '\\n\\n'.join(['Training Performance Evaluation',\n",
    "                              f'Training Predictions: {y_pred_train00}',\n",
    "                              f'Training True Labels: {y_train00.values}',\n",
    "                              f'Training % Correct: {percent_train_ok}',\n",
    "                              f'Cross-Validation Accuracy:\\n{cv_acc_train00}',\n",
    "                              f'Overall Accuracy:\\n{overall_acc00}',\n",
    "                              f'Overall Classification Report:\\n{overall_cr00}'])\n",
    "\n",
    "validate_report00 = '\\n\\n'.join(['Test Performance Evaluation',\n",
    "                                 f'Predictions: {y_pred_test00}',\n",
    "                                 f'True Labels: {y_validate00.values}',\n",
    "                                 f'% Correct: {percent_ok}',\n",
    "                                 f'Test Score: {acc_test00}',\n",
    "                                 f'Test Accuracy: {acc_test200}',\n",
    "                                 f'Classification Report\\n{cr_test00}'])\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "scoring200 = ['explained_variance', 'r2']\n",
    "scores00 = pd.DataFrame(cross_validate(ovoc, X00, y00,\n",
    "                                       scoring=scoring))\n",
    "\n",
    "print(f'{scores00}\\n\\n{train_report00}\\n{validate_report00}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ctl.cov().where(enc_ctl.cov()==np.triu(enc_ctl.cov().values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat = contrasts03_test.signals.cov().values\n",
    "\n",
    "def skip_diag_strided(A):\n",
    "    m = A.shape[0]\n",
    "    strided = np.lib.stride_tricks.as_strided\n",
    "    s0,s1 = A.strides\n",
    "    return strided(A.ravel()[1:], shape=(m-1,m), strides=(s0+s1,s1)).reshape(m,-1)\n",
    "\n",
    "pd.DataFrame(skip_diag_strided(np.tril(covmat))).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_X_train = np.array(X_train00.tolist()+X_train01.tolist())\n",
    "dual_y_train = np.array(y_train00.tolist()+y_train01.tolist())\n",
    "dual_X_validate = np.array(X_validate00.tolist()+X_train01.tolist())\n",
    "dual_y_validate = np.array(y_validate00.tolist()+y_validate01.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ovoc.fit(X_train01, y_train01)\n",
    "# dual_X_validate.shape,dual_y_validate.shape\n",
    "(dual_X_train.shape, dual_y_train.shape), dual_X_validate.shape, dual_y_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovoc.fit(dual_X_train, dual_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(filter(None, ovoc.predict(X_validate00)==y_validate00)))/len(y_validate00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "outcome_means = signal_table.groupby(signal_table.index).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_means.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=0.02):\n",
    "    \"\"\"\n",
    "    Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"\n",
    "    Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# X = signal_table\n",
    "# y = signal_table.index\n",
    "\n",
    "plot_contours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (\n",
    "    svm.SVC(kernel=\"linear\", C=C),\n",
    "    svm.LinearSVC(C=C, max_iter=10000),\n",
    "    svm.SVC(kernel=\"rbf\", gamma=0.7, C=C),\n",
    "    svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=C),\n",
    ")\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = (\n",
    "    \"SVC with linear kernel\",\n",
    "    \"LinearSVC (linear kernel)\",\n",
    "    \"SVC with RBF kernel\",\n",
    "    \"SVC with polynomial (degree 3) kernel\",\n",
    ")\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel(\"Sepal length\")\n",
    "    ax.set_ylabel(\"Sepal width\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca00=PCA().fit(signal_table.T)\n",
    "pca_table = pd.DataFrame(pca00.explained_variance_ratio_)\n",
    "#                          index=signal_table.columns,\n",
    "#                          columns=['var_ratio']).sort_values(\n",
    "#                              'var_ratio',ascending=False)\n",
    "pca_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(ovoc.__dict__['estimators_'][0].__dir__())\n",
    "# ovoc.__dict__['estimators_'][0].predict_proba(X_validate)\n",
    "# ovoc.__dict__['estimator'].\n",
    "# sklearn.metrics.SCORERS['explained_variance'](ovoc, X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ovoc = OneVsOneClassifier(SVC(tol=0.0001, kernel='poly',\n",
    "                              cache_size=600,\n",
    "                              decision_function_shape='ovo',\n",
    "                              class_weight='balanced',\n",
    "#                               break_ties=True,\n",
    "#                               gamma='auto', degree=3, coef0=0.0,\n",
    "                              probability=True))\n",
    "\n",
    "ovoc_pipe = make_pipeline(scaler, ovoc)\n",
    "\n",
    "ovo_cv_acc = cross_val_score(ovoc, X_test, y_test,\n",
    "                             groups=y_test, cv=cv)\n",
    "\n",
    "\n",
    "# .fit(X_train_transformed, y_train)\n",
    "\n",
    "ovoc_score = ovoc.score(X_test_transformed, y_test)\n",
    "# cv = sess00.events.ctl_miss_ws_cs.value_counts().min() -1\n",
    "\n",
    "\n",
    "# ovrc = OneVsRestClassifier(SVC(tol=0.0001, kernel='poly',\n",
    "#                                cache_size=600,\n",
    "#                                decision_function_shape='ovr',\n",
    "#                                class_weight='balanced',\n",
    "#                                break_ties=True,\n",
    "# #                                gamma='auto', degree=3, coef0=0.0,\n",
    "#                                probability=True))\n",
    "\n",
    "\n",
    "\n",
    "# ovrc.fit(X_train, y_train), ovoc.fit(X_train, y_train)\n",
    "\n",
    "ovo_cr = classification_report(y_pred=ovoc.predict(X_test),\n",
    "                               y_true=y_test, zero_division=0)\n",
    "ovr_cv_acc = cross_val_score(ovrc, X_test, y_test,\n",
    "                              groups=y_test, cv=cv)\n",
    "# ovr_cr = classification_report(y_pred=ovrc.predict(X_test),\n",
    "#                                y_true=y_test, zero_division=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n'.join([f'Training: {len(y_train)}, Testing: {len(y_test)}',\n",
    "                   f'True Labels:\\n{y_test.values}',\n",
    "                   f'OVR Predicted Labels:\\n{ovrc.predict(X_test)}',\n",
    "                   f'OVO Predicted Labels:\\n{ovoc.predict(X_test)}',                   \n",
    "                   ', '.join([f'OVR Accuracy: {ovrc.score(X_test,y_test)}',\n",
    "                              f'OVO Accuracy: {ovoc.score(X_test,y_test)}']),\n",
    "                   f'OVR Cross-Validation:\\n{ovr_cv_acc}',\n",
    "                   f'OVO Cross-Validation:\\n{ovo_cv_acc}',\n",
    "                   f'OVR CR:\\n{ovr_cr}',\n",
    "                   f'OVO CR:\\n{ovo_cr}']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_data = [sess00.sub_id]\n",
    "\n",
    "# define the model\n",
    "\n",
    "\n",
    "# s_data.append(overall_acc)\n",
    "\n",
    "# Test model on unseen data from the test set\n",
    "ovoc.fit(X_train, y_train)\n",
    "y_pred_test = ovoc.predict(X_test) # classify age class using testing data\n",
    "acc_test = sub_svc.score(X_test, y_test) # get accuracy\n",
    "\n",
    "cr_test = classification_report(y_pred=y_pred_test, y_true=y_test) # get prec., recall & f1\n",
    "# print results\n",
    "print('\\n\\n'.join(['Test Performance Evaluation',\n",
    "                   f'accuracy: {acc_test}',\n",
    "                   f'Classification Report\\n{cr_test}']))\n",
    "\n",
    "s_data.append(acc)\n",
    "\n",
    "# get map of coefficients    \n",
    "\n",
    "#Save .nii to file\n",
    "# coef_img.to_filename(os.path.join(output_dir, 'Coef_maps', 'SVC_coeff_enc_ctl_sub-'+str(sub)+'.nii'))\n",
    "\n",
    "# enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns), ignore_index=True)\n",
    "\n",
    "# demo_data = sub_data.copy()\n",
    "# demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "# enc_ctl_data.insert(loc = 1, column = 'cognitive_status', value = demo_data['cognitive_status'],\n",
    "#                 allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 2, column = 'total_scrubbed_frames', value = demo_data['total_scrubbed_frames'],\n",
    "#                 allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 3, column = 'mean_FD', value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 6, column = 'correct_source', value = demo_data['correct_source'],\n",
    "#                 allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 7, column = 'wrong_source', value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 8, column = 'dprime', value = demo_data['dprime'], allow_duplicates=True)\n",
    "# enc_ctl_data.insert(loc = 9, column = 'associative_memScore', value = demo_data['associative_memScore'],\n",
    "#                 allow_duplicates=True)    \n",
    "\n",
    "# enc_ctl_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_enc_ctl_wholeBrain.tsv'),\n",
    "# sep='\\t', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_ = sub_svc.coef_\n",
    "# print(coef_.shape)\n",
    "#Return voxel weights into a nifti image using the NiftiMasker\n",
    "coef_img = spheres_masker.inverse_transform(coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "X, y = sigs00, sess00.events.ctl_miss_ws_cs\n",
    "# print(f'Samples: {X.shape}, Labels: {y.shape}')\n",
    "n_samples = X_train.shape[0]\n",
    "\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "\n",
    "scores = cross_val_score(ovrc, X, y, cv=cv)\n",
    "\n",
    "print(f'Mean CV Accuracy Score: {scores.mean().round(3)}, STD = {round(scores.std(), 3)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_predict(ovrc, X_train, y_train, groups=y_train,\n",
    "                  cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "\n",
    "# s_data = []\n",
    "\n",
    "# for i in range(1, difumo.labels.shape[0]):\n",
    "#     enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "#                         column = roi_names[i]+'_coef',\n",
    "#                         value = NaN, allow_duplicates=True)\n",
    "\n",
    "#     betas, sub_mask = alltrials_betas_A, sess00.mask_img\n",
    "#     # initialize NiftiLabelMasker object    \n",
    "#     sub_label_masker = NiftiLabelsMasker(labels_img=basc,\n",
    "#                                          standardize=True,\n",
    "#                                          mask_img=sub_mask,\n",
    "#                                          verbose=0)\n",
    "\n",
    "#     # transform subject's beta maps into vector of network means per trial\n",
    "#     X_enc_ctl = sub_label_masker.fit_transform(betas)\n",
    "\n",
    "#     # load subject's trial labels\n",
    "#     y_enco_ctl = sess00.events.trial_type\n",
    "#     # Split trials into a training and a test set\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_enc_ctl, # x\n",
    "#         y_enco_ctl, # y\n",
    "#         test_size = 0.4, # 60%/40% split\n",
    "#         shuffle = True, # shuffle dataset before splitting\n",
    "#         stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "#         #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "#         ) \n",
    "#     print('\\n\\n'.join(['Training Set',\n",
    "#                        f'Length: {len(X_train)}',\n",
    "#                        f'Labels: {y_train.value_counts()}',\n",
    "#                        'Testing Set',\n",
    "#                        f'Length: {len(X_test)}',\n",
    "#                        f'Labels: {y_test.value_counts()}']))\n",
    "\n",
    "#     # define the model\n",
    "#     sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "#     # Cross-validation to evaluate model performance\n",
    "#     # within 10 folds of training set\n",
    "#     # predict\n",
    "#     y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "#                                groups=y_train, cv=cv)\n",
    "#     # scores\n",
    "#     cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "#                              groups=y_train, cv=cv)\n",
    "#     print(f'Cross-Validation Accuracy Score: {cv_acc}')\n",
    "\n",
    "#     for i in range(0, len(cv_acc)):\n",
    "#         s_data.append(cv_acc[i])\n",
    "\n",
    "#         # evaluate overall model performance on training data\n",
    "#         overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "#         overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "\n",
    "#     #     s_data.append(overall_acc)\n",
    "#         # Test model on unseen data from the test set\n",
    "#         sub_svc.fit(X_train, y_train)\n",
    "#         y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "#         acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "#         cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "#         s_data.append(acc)\n",
    "\n",
    "#         # get coefficients\n",
    "#         coef_ = sub_svc.coef_[0]\n",
    "#         print('\\n\\n'.join([f'Accuracy: {acc}',\n",
    "#                            f'Classification Report: {cr}',\n",
    "#                            f'Overall Accuracy Score: {overall_acc}',\n",
    "#                            f'Overall Classification Report: {overall_cr}',\n",
    "#                            f'Number of Coefficients: {coef_.shape}',\n",
    "#                            f'Coefficients: {coef_}']))\n",
    "#         sub_basc = basc_labels.copy()\n",
    "#         sub_basc.insert(loc=3, column='coef', value=coef_,\n",
    "#                         allow_duplicates=True)\n",
    "\n",
    "#     coef = sub_basc['coef']\n",
    "#     for j in range(0, len(coef)):\n",
    "#         s_data.append(coef[j])\n",
    "\n",
    "#     sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "#     #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "#     enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "#                                        ignore_index=True)\n",
    "\n",
    "\n",
    "demo_data = sub_data.copy()\n",
    "demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "enc_ctl_data.insert(loc = 1, column = 'cognitive_status',\n",
    "                    value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 2, column = 'total_scrubbed_frames',\n",
    "                    value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 3, column = 'mean_FD',\n",
    "                    value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 6, column = 'correct_source',\n",
    "                    value = demo_data['correct_source'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 7, column = 'wrong_source',\n",
    "                    value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 8, column = 'dprime',\n",
    "                    value = demo_data['dprime'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 9, column = 'associative_memScore',\n",
    "                    value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "# enc_ctl_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_enc_ctl_'+str(numnet)+'networks.tsv'),\n",
    "#     sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess00.events.recognition_performance.unique(),sess00.events.ctl_miss_ws_cs.unique()\n",
    "# false_alarms = sess00.behav.where(sess00.behav.values=='FA').dropna(\n",
    "#                    axis=1,how='all').dropna(axis=0,how='all').index\n",
    "# sess00.behav.loc[false_alarms]\n",
    "# sess00.behav.recognition_performance.unique()\n",
    "# 'CR' in sess00.events.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from nilearn.input_data import NiftiSpheresMasker\n",
    "# from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# # help(OneVsRestClassifier)\n",
    "\n",
    "# cv = sess00.events.ctl_miss_ws_cs.value_counts().min() -1\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     sigs00,\n",
    "#     sess00.events.ctl_miss_ws_cs,\n",
    "#     test_size = 0.9,\n",
    "#     shuffle = True,\n",
    "#     stratify = sess00.events.recognition_performance)\n",
    "\n",
    "# ovrc = OneVsRestClassifier(SVC(tol=0.0001, kernel='poly',\n",
    "#                                cache_size=600,\n",
    "#                                decision_function_shape='ovr',\n",
    "#                                class_weight='balanced',\n",
    "#                                break_ties=True,\n",
    "# #                                gamma='auto', degree=3, coef0=0.0,\n",
    "#                                probability=True))\n",
    "\n",
    "# ovoc = OneVsOneClassifier(SVC(tol=0.0001, kernel='poly',\n",
    "#                               cache_size=600,\n",
    "#                               decision_function_shape='ovo',\n",
    "#                               class_weight='balanced',\n",
    "# #                               break_ties=True,\n",
    "# #                               gamma='auto', degree=3, coef0=0.0,\n",
    "#                               probability=True))\n",
    "\n",
    "# ovrc.fit(X_train, y_train), ovoc.fit(X_train, y_train)\n",
    "\n",
    "# ovo_cr = classification_report(y_pred=ovoc.predict(X_test),\n",
    "#                                y_true=y_test, zero_division=0)\n",
    "# ovr_cr = classification_report(y_pred=ovrc.predict(X_test),\n",
    "#                                y_true=y_test, zero_division=0)\n",
    "\n",
    "# ovr_cv_acc = cross_val_score(ovrc, X_test, y_test,\n",
    "#                               groups=y_test, cv=cv)\n",
    "# ovo_cv_acc = cross_val_score(ovoc, X_test, y_test,\n",
    "#                              groups=y_test, cv=cv)\n",
    "\n",
    "# print('\\n\\n'.join([f'Training: {len(y_train)}, Testing: {len(y_test)}',\n",
    "#                    f'True Labels:\\n{y_test.values}',\n",
    "#                    f'OVR Predicted Labels:\\n{ovrc.predict(X_test)}',\n",
    "#                    f'OVO Predicted Labels:\\n{ovoc.predict(X_test)}',                   \n",
    "#                    ', '.join([f'OVR Accuracy: {ovrc.score(X_test,y_test)}',\n",
    "#                               f'OVO Accuracy: {ovoc.score(X_test,y_test)}']),\n",
    "#                    f'OVR Cross-Validation:\\n{ovr_cv_acc}',\n",
    "#                    f'OVO Cross-Validation:\\n{ovo_cv_acc}',\n",
    "#                    f'OVR CR:\\n{ovr_cr}',\n",
    "#                    f'OVO CR:\\n{ovo_cr}']))\n",
    "#               #\\nPredictions:\\n{ovoc.predict(X_test)}']))\n",
    "# # help(SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import FREMRegressor\n",
    "# help(FREMRegressor)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sigs00,\n",
    "    sess00.events.trial_type,\n",
    "    test_size = 0.4,\n",
    "    shuffle = True,\n",
    "    stratify = enc_ctl00)\n",
    "\n",
    "frem = FREMRegressor(ovrc).fit(X_train,y_train)\n",
    "\n",
    "# frem.fit(X_train, y_train)\n",
    "\n",
    "print(frem.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sigs00, # x\n",
    "    sess00.events.trial_type, # y\n",
    "    test_size = 0.4, # 60%/40% split\n",
    "    shuffle = True, # shuffle dataset before splitting\n",
    "    stratify = enc_ctl00, # keep distribution of conditions consistent betw. train & test sets\n",
    "    #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "    )\n",
    "# define the model\n",
    "# kernel='linear', class_weight='balanced'\n",
    "sub_svc = SVC()\n",
    "cv = 15\n",
    "sub_svc.fit(X_train, y_train)\n",
    "y_pred = sub_svc.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "cv_acc = cross_val_score(sub_svc, X_test, y_test,\n",
    "                         groups=y_test, cv=cv)\n",
    "\n",
    "cr = classification_report(y_pred=y_pred, y_true=y_test)\n",
    "print('\\n\\n'.join([f'Cross-Validation Score: {cv_acc}',\n",
    "                   f'Classification Report: {cr}']))\n",
    "# print(f'Accuracy: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cimaq_decoding_utils import get_iso_labels\n",
    "# enc_ctl_iso00 = get_iso_labels(session=sess00)\n",
    "# sigs_iso00 = NiftiMasker().fit_transform(sess00.cleaned_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Iterable, Sequence\n",
    "\n",
    "\n",
    "def flatten(nested_seq: Union[Iterable, Sequence]) -> list:\n",
    "    \"\"\"\n",
    "    Return vectorized (1D) list from nested Sequence ``nested_seq``.\n",
    "    \"\"\"\n",
    "\n",
    "    return [bottomElem for sublist in nested_seq for bottomElem\n",
    "            in (flatten(sublist)\n",
    "                if (isinstance(sublist, Sequence)\n",
    "                    and not isinstance(sublist, str))\n",
    "                else [sublist])]\n",
    "\n",
    "\n",
    "def get_iso_labels(frame_times:Iterable=None,\n",
    "                   t_r:float=None,\n",
    "                   events:pd.DataFrame=None,\n",
    "                   session:Union[dict, Bunch]=None,\n",
    "                   **kwargs) -> list:\n",
    "    \"\"\"\n",
    "    Return a list of which trial condition a given fMRI frame fits into.\n",
    "    \n",
    "    Args:\n",
    "        frame_times: Iterable (default=None)\n",
    "            Iterable array containing the onset of each fMRI frame\n",
    "            since scan start.\n",
    "\n",
    "        t_r: float (default=None)\n",
    "            fMRI scan repetition time, in seconds.\n",
    "\n",
    "        events: pd.DataFrame (default=None)\n",
    "            DataFrame containing experimental procedure details.\n",
    "            The required columns are the same as used by Nilearn functions\n",
    "            (e.g. ``nilearn.glm.first_level.FirstLevelModel``).\n",
    "            Those are [\"onset\", \"duration\", \"trial_type\"].\n",
    "            Other columns are ignored.\n",
    "\n",
    "        session: dict or Bunch (default=None)\n",
    "            Dict or ``sklearn.utils.Bunch`` minimally containg\n",
    "            all of the above parameters just like keyword arguments.\n",
    "\n",
    "    Returns: list\n",
    "        List of which trial condition a given fMRI frame fits into.\n",
    "        The list is of the same lenght as ``frame_times``,\n",
    "        suitable for classification and statistical operations.\n",
    "    \"\"\"\n",
    "#     from cimaq_decoding_utils import flatten\n",
    "    if session is not None:\n",
    "        params = itemgetter(*('frame_times','t_r','events'))(session)\n",
    "        frame_times,t_r,events = params\n",
    "\n",
    "    frame_intervals = [pd.Interval(*item) for item in\n",
    "                       tuple(zip(frame_times, frame_times+t_r))]\n",
    "    trial_ends = (events.onset+events.duration).values\n",
    "    trial_intervals = [pd.Interval(*item) for item in\n",
    "                       tuple(zip(events.onset.values, trial_ends))]\n",
    "    frame_intervals = [frame for frame in frame_intervals\n",
    "                       if frame.left>trial_intervals[0].right]\n",
    "    frame_intervals = [frame for frame in frame_intervals\n",
    "                       if trial_intervals[-1].left<frame.left]    \n",
    "#     return len(frame_intervals)\n",
    "\n",
    "    bold_by_trial_indx = [[frame[0] for frame in\n",
    "                           enumerate(frame_intervals)\n",
    "                           if frame[1].right in trial]\n",
    "                          for trial in trial_intervals]\n",
    "#     return bold_by_trial_indx\n",
    "#     labels = pd.Series([[label] for label in events.trial_type.tolist()])\n",
    "    label_shapes = list(map(len,bold_by_trial_indx))\n",
    "    return sum(label_shapes)\n",
    "#     return np.array(flatten((labels*label_shapes).values))\n",
    "    \n",
    "#     [labels.extend(lst) for lst in\n",
    "#     return np.array(flatten([[item[0]]*len(item[1]) for item in\n",
    "#                      tuple(zip(events.trial_type.values,\n",
    "#                                bold_by_trial_indx))]))\n",
    "#     return np.array(labels)\n",
    "\n",
    "# enc_ctl_iso00 = \n",
    "get_iso_labels(session=sess00)\n",
    "\n",
    "# enc_ctl_iso00, enc_ctl_iso00.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_ctl00 = sess00.events.trial_type.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cross-validation (10 folds)\n",
    "# predict\n",
    "# y_pred = cross_val_predict(sub_svc, X_test, y_test,\n",
    "#                            groups=y_test, cv=cv)\n",
    "# # scores\n",
    "\n",
    "\n",
    "# for i in range(0, len(cv_acc)):\n",
    "# #         s_data.append(cv_acc[i])\n",
    "#     sub_svc.fit(X_train, y_train)\n",
    "#     y_pred = sub_svc.predict(X_test)\n",
    "#     # evaluate overall model performance on training data\n",
    "#     overall_acc = accuracy_score(y_pred = y_pred,\n",
    "#                                  y_true = y_test)\n",
    "#     overall_cr = classification_report(y_pred = y_pred,\n",
    "#                                        y_true = y_test)\n",
    "#     acc = sub_svc.score(X_test, y_test)\n",
    "#     cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "# #         s_data.append(acc)\n",
    "\n",
    "#     # get coefficients\n",
    "#     coef_ = sub_svc.coef_[0]\n",
    "\n",
    "#     print('\\n\\n'.join([f'Accuracy: {acc}',\n",
    "#                        f'Classification Report: {cr}',\n",
    "#                        f'Overall Accuracy Score: {overall_acc}',\n",
    "#                        f'Overall Classification Report: {overall_cr}',\n",
    "#                        f'Number of Coefficients: {coef_.shape}',\n",
    "#                        f'Coefficients: {coef_}'])) \n",
    "# display(y_pred, cv_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "difumo_cut_coords = pd.read_csv('/data/simexp/fnadeau/difumo_64_dims_3mm_cut_coords.tsv',\n",
    "                                sep='\\t', index_col='component')\n",
    "\n",
    "spheres_masker = NiftiSpheresMasker(seeds=difumo_cut_coords[['x', 'y', 'z']].values,\n",
    "                                    standardize='zscore').fit()\n",
    "# mask_img=spheres_masker\n",
    "# help(spheres_masker.fit)\n",
    "signals = spheres_masker.transform_single_imgs(beta_list_test)\n",
    "\n",
    "\n",
    "spacetime_sigs = pd.DataFrame(signals,\n",
    "                              columns=difumo_cut_coords.difumo_names,\n",
    "                              index=trials)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    signals,\n",
    "    sess00.events.trial_type.values,\n",
    "    test_size = 0.3,\n",
    "    shuffle = True,\n",
    "    stratify = trials)\n",
    "sub_svc = SVC(kernel='linear',\n",
    "              class_weight='balanced',\n",
    "#               C=0.1,\n",
    "#               degree=1,\n",
    "#               coef0=1,\n",
    "              tol=0.05,\n",
    "              probability=True,\n",
    "              decision_function_shape='ovr',\n",
    "#               break_ties=True,\n",
    "              gamma='auto',\n",
    "#               random_state=1,\n",
    "              cache_size=600)\n",
    "sub_svc.fit(X_train, y_train)\n",
    "print(sub_svc.score(X_test, y_test))\n",
    "# logreg = LogisticRegression(\n",
    "#                             class_weight='balanced',\n",
    "#                             max_iter=1000000,\n",
    "# #                             tol=0.05,\n",
    "#                             multi_class='ovr',\n",
    "# #                             C=0.1,\n",
    "# #                             solver='liblinear',\n",
    "# #                             penalty='l2',\n",
    "# #                             solver='liblinear'\n",
    "#                            ).fit(X_train, y_train)\n",
    "# logreg.score(X_test, y_test)\n",
    "# logreg = LogisticRegression(class_weight='balanced').fit(list(itm[1] for itm in spacetime_sigs.iterrows()),\n",
    "#                                                          spacetime_sigs.index)\n",
    "\n",
    "\n",
    "# display(logreg.predict_log_proba(X_test),\n",
    "#         logreg.predict_proba(X_test), logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from nibabel.nifti1 import Nifti1Image\n",
    "# from nilearn.glm.first_level import FirstLevelModel\n",
    "# from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "# from nilearn.input_data import NiftiSpheresMasker\n",
    "# from operator import itemgetter\n",
    "# from os import PathLike\n",
    "# from pathlib import PosixPath\n",
    "# from sklearn.utils import Bunch\n",
    "# from typing import Iterable, Union\n",
    "\n",
    "# def make_betamap(fmri_img: Union[str, PathLike, PosixPath,\n",
    "#                                  Nifti1Image] = None,\n",
    "#                  events: Union[str, PathLike, PosixPath,\n",
    "#                                pd.DataFrame] = None,\n",
    "#                  mask_img: Union[bool, str, PathLike, PosixPath,\n",
    "#                                  Nifti1Image] = False,\n",
    "#                  fwhm: Union[float, int] = None,\n",
    "#                  design_kws: Union[dict, Bunch] = None,\n",
    "#                  glm_kws: Union[dict, Bunch] = None,\n",
    "#                  masker_kws: Union[dict, Bunch] = None,\n",
    "#                  session: Union[dict, Bunch] = None,\n",
    "#                  **kwargs):\n",
    "\n",
    "#     def get_frame_times(fmri_img:Nifti1Image):\n",
    "#         return (np.arange(fmri_img.shape[-1]) *\n",
    "#                 fmri_img.header.get_zooms()[-1])\n",
    "\n",
    "#     if session is not None:\n",
    "#         fmri_img, events, design_defs, glm_defs = \\\n",
    "#             itemgetter(*['cleaned_fmri', 'events',\n",
    "#                          'design_defs', 'glm_defs'])(session)\n",
    "    \n",
    "#     else:\n",
    "#         fmri_img = nilearn.image.load_img(fmri_img)\n",
    "#         if not isinstance(events, pd.DataFrame):\n",
    "#             events = pd.read_csv(events, sep=get_separator(events))\n",
    "#         else:\n",
    "#             events = events\n",
    "#     events = preprocess_events(events, fmri_img)[['onset', 'duration',\n",
    "#                                                   'condition']]\n",
    "#     events = events.rename({'condition':'trial_type'}, axis=1)\n",
    "#     if fwhm is not None:\n",
    "#         fmri_img = nilearn.image.smooth_img(fmri_img, fwhm)\n",
    "    \n",
    "#     if design_kws is not None:\n",
    "#         _params.design_defs.update(design_kws)\n",
    "#     if masker_kws is not None:\n",
    "#         _params.masker_defs.update(masker_kws)\n",
    "#     if glm_kws is not None:\n",
    "#         _params.glm_defs.update(glm_kws)\n",
    "\n",
    "#     all_betas_filelist_A = []\n",
    "\n",
    "#     for row in tqdm(list(events.iterrows()),\n",
    "#                     desc='Computing Beta Map'):\n",
    "#         trial_events = events.copy(deep = True)\n",
    "#         conditions = ['X_'+trial_row[1].trial_type if\n",
    "#                       trial_row[0]==row[0]\n",
    "#                       else trial_row[1].trial_type\n",
    "#                       for trial_row in trial_events.iterrows()]\n",
    "#         trial_events['trial_type'] = conditions\n",
    "\n",
    "#         design = [make_first_level_design_matrix(\n",
    "#                       frame_times=get_frame_times(fmri_img),\n",
    "#                       events=trial_events,\n",
    "#                       **design_kws)\n",
    "#                   if design_kws is not None\n",
    "#                   else make_first_level_design_matrix(\n",
    "#                       frame_times=get_frame_times(fmri_img),\n",
    "#                       events=trial_events[['onset','duration','trial_type']])][0]\n",
    "\n",
    "#         trial_model = [FirstLevelModel(**glm_kws).fit(\n",
    "#                            fmri_img, design_matrices=design)\n",
    "#                        if glm_kws is not None else\n",
    "#                        FirstLevelModel().fit(\n",
    "#                            fmri_img, design_matrices=design)][0]\n",
    "#         contrast_vec = [int(trial.startswith('X_'))\n",
    "#                         for trial in trial_events.trial_type.tolist()]\n",
    "\n",
    "#         b_map = trial_model.compute_contrast(design.columns[row[0]],\n",
    "#                                              output_type='effect_size')\n",
    "#         all_betas_filelist_A.append(b_map)\n",
    "\n",
    "#     return nilearn.image.concat_imgs(all_betas_filelist_A)\n",
    "\n",
    "# betamap = make_betamap(session=sess00)\n",
    "# # help(FirstLevelModel)\n",
    "# #  | mask_img : Niimg-like, NiftiMasker object or False, optional\n",
    "# #  |      Mask to be used on data. If an instance of masker is passed,\n",
    "# #  |      then its mask will be used. If no mask is given,\n",
    "# #  |      it will be computed automatically by a NiftiMasker with default\n",
    "# #  |      parameters. If False is given then the data will not be masked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_tcontrasts1(session:Union[dict,Bunch]=None,\n",
    "                    sub_id:str=None,\n",
    "                    tr:float=None,\n",
    "                    frame_times:list=None,\n",
    "                    hrf_model:str=None,\n",
    "                    events:pd.DataFrame=None,\n",
    "                    fmri_img:Nifti1Image=None,\n",
    "                    sub_outdir:Union[str,os.PathLike]=None,\n",
    "                    glm_kws: Union[dict, Bunch] = None):\n",
    "    \"\"\"\n",
    "    Create beta values maps using nilearn first-level model.\n",
    "\n",
    "    The beta values correspond to the following contrasts between conditions:\n",
    "    control, encoding, and encoding_minus_control\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    sub_id: string (subject's dccsub_id)\n",
    "    tr: float (length of time to repetition, in seconds)\n",
    "    frames_times: list of float (onsets of fMRI frames, in seconds)\n",
    "    hrf_model: string (type of HRF model)\n",
    "    confounds: pandas dataframe (motion and other noise regressors)\n",
    "    all_events: string (task information: trials' onset time, duration and label)\n",
    "    fmrsub_idir: string (path to directory with fMRI data)\n",
    "    outdir: string (path to subject's image output directory)\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "    None (beta maps are exported in sub_outdir)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(session, dict):\n",
    "        session = Bunch(**session)\n",
    "    # Model 1: encoding vs control conditions\n",
    "    events = preprocess_events(session.events, fmri_img)[['onset', 'duration',\n",
    "                                                  'condition']]\n",
    "    events = events.rename({'condition':'trial_type'}, axis=1)\n",
    "#     events1 = session.events[['onset', 'duration',\n",
    "#                               'trial_type']].copy(deep = True)\n",
    "    if glm_kws is not None:\n",
    "        session.glm_defs.update(glm_kws)\n",
    "    all_betas_filelist_A = []\n",
    "\n",
    "    for row in tqdm(list(events.iterrows()),\n",
    "                    desc='Computing Beta Map'):\n",
    "        trial_events = events.copy(deep = True)\n",
    "        conditions = ['X_'+trial_row[1].trial_type if\n",
    "                      trial_row[0]==row[0]\n",
    "                      else trial_row[1].trial_type\n",
    "                      for trial_row in trial_events.iterrows()]\n",
    "        trial_events['trial_type'] = conditions     \n",
    "    # create the model - Should data be standardized?\n",
    "    model1 = FirstLevelModel(**session.glm_defs)\n",
    "\n",
    "    # create the design matrices\n",
    "    design1 = make_first_level_design_matrix(events=events1,\n",
    "                                             frame_times=session.frame_times,\n",
    "                                             **session.design_defs)\n",
    "\n",
    "    # fit model with design matrix\n",
    "    model1 = model1.fit(session.cleaned_fmri, design_matrices = design1)\n",
    "\n",
    "    # Condition order: control, encoding (alphabetical)\n",
    "    # contrast 1.1: control condition\n",
    "    ctl_vec = np.repeat(0, design1.shape[1])\n",
    "    ctl_vec[0] = 1\n",
    "    b11_map = model1.compute_contrast(ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "    b11_name = f'betas_{session.sub_id}_ctl.nii'\n",
    "\n",
    "    #contrast 1.2: encoding condition\n",
    "    enc_vec = np.repeat(0, design1.shape[1])\n",
    "    enc_vec[1] = 1\n",
    "    b12_map = model1.compute_contrast(enc_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "    b12_name = f'betas_{session.sub_id}_enc.nii'\n",
    "\n",
    "    #contrast 1.3: encoding minus control\n",
    "    encMinCtl_vec = np.repeat(0, design1.shape[1])\n",
    "    encMinCtl_vec[1] = 1\n",
    "    encMinCtl_vec[0] = -1\n",
    "    b13_map = model1.compute_contrast(encMinCtl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "    b13_name = f'betas_{session.sub_id}_enc_minus_ctl.nii'\n",
    "    contrasts = ((b11_map, b11_name), (b12_map, b12_name), (b13_map, b13_name))\n",
    "    if sub_outdir is not None:\n",
    "        savedir = os.path.join(sub_outdir, session.sub_id, session.ses_id)\n",
    "        os.makedirs(savedir, exist_ok=True)\n",
    "        [nibabel.save(*contrast) for contrast in contrasts]\n",
    "    return contrasts\n",
    "\n",
    "contrasts01 = sub_tcontrasts1(session=sess00, glm_kws={'standardize':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[itm[0].shape for itm in contrasts01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nilearn.input_data import NiftiMapsMasker\n",
    "\n",
    "# atlas_kws = dict(data_dir='/data/simexp/fnadeau/difumo_atlases/',\n",
    "#                  resolution_mm=3, dimension=64)\n",
    "\n",
    "# difumo=get_difumo(**atlas_kws)\n",
    "\n",
    "# difumo_cut_coords = pd.read_csv('/data/simexp/fnadeau/difumo_64_dims_3mm_cut_coords.tsv',\n",
    "#                                 sep='\\t', index_col='component')\n",
    "\n",
    "\n",
    "spheres_masker = NiftiSpheresMasker(seeds=difumo_cut_coords[['x', 'y', 'z']].values,\n",
    "#                                     mask_img=maps_masker.mask_img,\n",
    "                                    standardize=False,\n",
    "                                    standardize_confounds=False).fit()\n",
    "\n",
    "maps_masker = NiftiMapsMasker(maps_img=difumo.maps,\n",
    "                              mask_img=spheres_masker.mask_img,\n",
    "                              standardize_confounds=False,\n",
    "                              resampling_target='data').fit()\n",
    "\n",
    "maps_masker.__dict__.keys()\n",
    "# help(NiftiMapsMasker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version A: A separate model created for each trial.\n",
    "Trial of interest modelled as its own condition, and other trials are modelled as two conditions: control, and encoding (excluding trial of interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dir(inspect))\n",
    "# help(inspect.getattr_static)\n",
    "# attrgetter\n",
    "\n",
    "# inspect.getattr_static(obj=nilearn.datasets,\n",
    "#                        attr=f'fetch_atlas_{atlas_name}')\n",
    "# atlas_name='difumo'\n",
    "# atlas_caller = getattr(nilearn.datasets, f'fetch_atlas_{atlas_name}')\n",
    "# atlas_caller(**atlas_kws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "get_difumo_cut_coords(64, 3, '/data/simexp/fnadeau/difumo_atlases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess00.events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sub_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDClassifier, LogisticRegressionCV\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     signals, trials, test_size = 0.4,\n",
    "#     shuffle = True, stratify = trials)\n",
    "\n",
    "# display(logreg.predict_log_proba(X_test), logreg.predict_proba(X_test),\n",
    "#         logreg.predict_proba(X_test), logreg.score(X_test, y_test))\n",
    "# help(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    session.computed_.signal_matrix, session.events.iloc[:, -1], test_size = 0.4,\n",
    "    shuffle = True, stratify = session.events.iloc[:, -1])\n",
    "linreg = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "sorted(linreg.__dict__.keys())\n",
    "# sorted(dir(linreg))\n",
    "# help(linreg.score)\n",
    "\n",
    "# trial_pred = linreg.predict(X_test)\n",
    "# linreg.score(linreg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacetime_sigs = pd.DataFrame(X_enc_ctl,\n",
    "                              columns=difumo_cut_coords.difumo_names,\n",
    "                              index=sess00.events.trial_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_enco_ctl = sess00.events.trial_type\n",
    "spheres_masker.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split ``stratify``: keep distribution of conditions consistent betw. train & test sets\n",
    "# ``random_state`` = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "def classify_trial(betamap, masker, y, **kwargs):\n",
    "\n",
    "    # transform subject's beta maps into vector of network means per trial\n",
    "    X = masker.transform_single_imgs(betamap)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size = 0.4,\n",
    "        shuffle = True, stratify = y)\n",
    "    \n",
    "    # Model\n",
    "    sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "#     sub_svc.fit(X_train, y_train)\n",
    "\n",
    "    # Cross-validation (10 folds)\n",
    "    y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                               groups=y_train, cv=cv)\n",
    "    # scores\n",
    "    cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=cv)\n",
    "    summary_str = '\\n\\n'.join(['Training Set', f'Length: {len(X_train)}',\n",
    "                               f'Labels:\\n{y_train.value_counts()}',\n",
    "                               'Testing Set', f'Length: {len(X_test)}',\n",
    "                               f'Labels:\\n{y_test.value_counts()}',\n",
    "                               f'Cross-Validation Accuracy Score:\\n{cv_acc}'])\n",
    "    for i in range(0, len(cv_acc)):\n",
    "#         s_data.append(cv_acc[i])\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test)\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = y_pred,\n",
    "                                     y_true = y_test)\n",
    "        overall_cr = classification_report(y_pred = y_pred,\n",
    "                                           y_true = y_test)\n",
    "        acc = sub_svc.score(X_test, y_test)\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "#         s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_, sub_basc = sub_svc.coef_[0], basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_,\n",
    "                        allow_duplicates=True)\n",
    "\n",
    "        print('\\n\\n'.join([f'Accuracy: {acc}',\n",
    "                           f'Classification Report: {cr}',\n",
    "                           f'Overall Accuracy Score: {overall_acc}',\n",
    "                           f'Overall Classification Report: {overall_cr}',\n",
    "                           f'Number of Coefficients: {coef_.shape}',\n",
    "                           f'Coefficients: {coef_}']))       \n",
    "    [s_data.append(sub_basc['coef'][j]) for j in range(1, len(sub_basc['coef']))]\n",
    "\n",
    "    sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "    #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "    enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "                                       ignore_index=True)\n",
    "    print(enc_ctl_data)\n",
    "\n",
    "classify_trial(betamap=betamap,\n",
    "               masker=spheres_masker,\n",
    "               y=sess00.events.trial_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import os\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from sklearn.utils import Bunch\n",
    "from typing import Union\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "# from get_difumo import get_difumo\n",
    "\n",
    "# difumo=get_difumo(data_dir='/data/simexp/fnadeau/difumo_atlases/',\n",
    "#                   resolution_mm=3, dimension=64)\n",
    "\n",
    "cv, basc_labels, s_data = 10, difumo.labels, []\n",
    "# build data structure to store accuracy data and coefficients\n",
    "enc_ctl_data = pd.DataFrame()\n",
    "enc_ctl_data.insert(loc = 0, column = 'pscid', value = 'None',\n",
    "                    allow_duplicates=True)\n",
    "for i in range(0, cv):\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "                        column = 'CV'+str(i+1)+'_acc',\n",
    "                        value = NaN, allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "                        column = 'TrainSet_MeanCV_acc',\n",
    "                        value = 'None', allow_duplicates=True)\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "                        column = 'TestSet_acc',\n",
    "                        value = 'None', allow_duplicates=True)\n",
    "    roi_names = difumo_cut_coords.difumo_names.values\n",
    "\n",
    "\n",
    "for i in range(1, difumo.labels.shape[0]):\n",
    "    enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "                        column = roi_names[i]+'_coef',\n",
    "                        value = NaN, allow_duplicates=True)\n",
    "\n",
    "    betas, sub_mask = betamap, sess00.mask_img\n",
    "\n",
    "    # transform subject's beta maps into vector of network means per trial\n",
    "    X_enc_ctl = spheres_masker.transform_single_imgs(betas)\n",
    "\n",
    "    # load subject's trial labels\n",
    "    y_enco_ctl = sess00.events.trial_type\n",
    "    # Split trials into a training and a test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_enc_ctl, # x\n",
    "        y_enco_ctl, # y\n",
    "        test_size = 0.4, # 60%/40% split\n",
    "        shuffle = True, # shuffle dataset before splitting\n",
    "        stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "        #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "        )\n",
    "    # define the model\n",
    "    sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "    # Cross-validation (10 folds)\n",
    "    # predict\n",
    "    y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                               groups=y_train, cv=cv)\n",
    "    # scores\n",
    "    cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                             groups=y_train, cv=cv)\n",
    "    print('\\n\\n'.join(['Training Set',\n",
    "                       f'Length: {len(X_train)}',\n",
    "                       f'Labels:\\n{y_train.value_counts()}',\n",
    "                       'Testing Set',\n",
    "                       f'Length: {len(X_test)}',\n",
    "                       f'Labels: {y_test.value_counts()}',\n",
    "                       f'Cross-Validation Accuracy Score: {cv_acc}']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "\n",
    "# from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# for i in range(1, difumo.labels.shape[0]):\n",
    "#     enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "#                         column = roi_names[i]+'_coef',\n",
    "#                         value = NaN, allow_duplicates=True)\n",
    "\n",
    "#     betas, sub_mask = betamap, sess00.mask_img\n",
    "\n",
    "#     # transform subject's beta maps into vector of network means per trial\n",
    "#     X_enc_ctl = spheres_masker.transform_single_imgs(betas)\n",
    "\n",
    "#     # load subject's trial labels\n",
    "#     y_enco_ctl = sess00.events.trial_type\n",
    "#     # Split trials into a training and a test set\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_enc_ctl, # x\n",
    "#         y_enco_ctl, # y\n",
    "#         test_size = 0.4, # 60%/40% split\n",
    "#         shuffle = True, # shuffle dataset before splitting\n",
    "#         stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "#         #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "#         )\n",
    "#     # define the model\n",
    "#     sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "#     # Cross-validation (10 folds)\n",
    "#     # predict\n",
    "#     y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "#                                groups=y_train, cv=cv)\n",
    "#     # scores\n",
    "#     cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "#                              groups=y_train, cv=cv)\n",
    "#     print('\\n\\n'.join(['Training Set',\n",
    "#                        f'Length: {len(X_train)}',\n",
    "#                        f'Labels:\\n{y_train.value_counts()}',\n",
    "#                        'Testing Set',\n",
    "#                        f'Length: {len(X_test)}',\n",
    "#                        f'Labels: {y_test.value_counts()}',\n",
    "#                        f'Cross-Validation Accuracy Score: {cv_acc}']))    \n",
    "\n",
    "    for i in range(0, len(cv_acc)):\n",
    "        s_data.append(cv_acc[i])\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        # evaluate overall model performance on training data\n",
    "        overall_acc = accuracy_score(y_pred = sub_svc.predict(X_test),\n",
    "                                     y_true = y_test)\n",
    "        overall_cr = classification_report(y_pred = sub_svc.predict(X_test),\n",
    "                                           y_true = y_test)\n",
    "\n",
    "        s_data.append(overall_acc)\n",
    "        # classify age class using testing data        \n",
    "        y_pred = sub_svc.predict(X_test)\n",
    "        acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "        cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "        s_data.append(acc)\n",
    "\n",
    "        # get coefficients\n",
    "        coef_, sub_basc = sub_svc.coef_[0], basc_labels.copy()\n",
    "        sub_basc.insert(loc=3, column='coef', value=coef_,\n",
    "                        allow_duplicates=True)\n",
    "\n",
    "        print('\\n\\n'.join([f'Accuracy: {acc}',\n",
    "                           f'Classification Report: {cr}',\n",
    "                           f'Overall Accuracy Score: {overall_acc}',\n",
    "                           f'Overall Classification Report: {overall_cr}',\n",
    "                           f'Number of Coefficients: {coef_.shape}',\n",
    "                           f'Coefficients: {coef_}']))        \n",
    "    [s_data.append(sub_basc['coef'][j]) for j in range(1, len(sub_basc['coef']))]\n",
    "\n",
    "    sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "    #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "    enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "                                       ignore_index=True)\n",
    "    print(enc_ctl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODING VERSUS CONTROL TASK CLASSIFICATION\n",
    "\n",
    "# s_data = []\n",
    "\n",
    "# for i in range(1, difumo.labels.shape[0]):\n",
    "#     enc_ctl_data.insert(loc = enc_ctl_data.shape[1],\n",
    "#                         column = roi_names[i]+'_coef',\n",
    "#                         value = NaN, allow_duplicates=True)\n",
    "\n",
    "#     betas, sub_mask = alltrials_betas_A, sess00.mask_img\n",
    "#     # initialize NiftiLabelMasker object    \n",
    "#     sub_label_masker = NiftiLabelsMasker(labels_img=basc,\n",
    "#                                          standardize=True,\n",
    "#                                          mask_img=sub_mask,\n",
    "#                                          verbose=0)\n",
    "\n",
    "#     # transform subject's beta maps into vector of network means per trial\n",
    "#     X_enc_ctl = sub_label_masker.fit_transform(betas)\n",
    "\n",
    "#     # load subject's trial labels\n",
    "#     y_enco_ctl = sess00.events.trial_type\n",
    "#     # Split trials into a training and a test set\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X_enc_ctl, # x\n",
    "#         y_enco_ctl, # y\n",
    "#         test_size = 0.4, # 60%/40% split\n",
    "#         shuffle = True, # shuffle dataset before splitting\n",
    "#         stratify = y_enco_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "#         #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "#         ) \n",
    "#     print('\\n\\n'.join(['Training Set',\n",
    "#                        f'Length: {len(X_train)}',\n",
    "#                        f'Labels: {y_train.value_counts()}',\n",
    "#                        'Testing Set',\n",
    "#                        f'Length: {len(X_test)}',\n",
    "#                        f'Labels: {y_test.value_counts()}']))\n",
    "\n",
    "#     # define the model\n",
    "#     sub_svc = SVC(kernel='linear', class_weight='balanced')\n",
    "\n",
    "#     # Cross-validation to evaluate model performance\n",
    "#     # within 10 folds of training set\n",
    "#     # predict\n",
    "#     y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "#                                groups=y_train, cv=cv)\n",
    "#     # scores\n",
    "#     cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "#                              groups=y_train, cv=cv)\n",
    "#     print(f'Cross-Validation Accuracy Score: {cv_acc}')\n",
    "\n",
    "#     for i in range(0, len(cv_acc)):\n",
    "#         s_data.append(cv_acc[i])\n",
    "\n",
    "#         # evaluate overall model performance on training data\n",
    "#         overall_acc = accuracy_score(y_pred = y_pred, y_true = y_train)\n",
    "#         overall_cr = classification_report(y_pred = y_pred, y_true = y_train)\n",
    "\n",
    "#     #     s_data.append(overall_acc)\n",
    "#         # Test model on unseen data from the test set\n",
    "#         sub_svc.fit(X_train, y_train)\n",
    "#         y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "#         acc = sub_svc.score(X_test, y_test) # get accuracy\n",
    "#         cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "#         s_data.append(acc)\n",
    "\n",
    "#         # get coefficients\n",
    "#         coef_ = sub_svc.coef_[0]\n",
    "#         print('\\n\\n'.join([f'Accuracy: {acc}',\n",
    "#                            f'Classification Report: {cr}',\n",
    "#                            f'Overall Accuracy Score: {overall_acc}',\n",
    "#                            f'Overall Classification Report: {overall_cr}',\n",
    "#                            f'Number of Coefficients: {coef_.shape}',\n",
    "#                            f'Coefficients: {coef_}']))\n",
    "#         sub_basc = basc_labels.copy()\n",
    "#         sub_basc.insert(loc=3, column='coef', value=coef_,\n",
    "#                         allow_duplicates=True)\n",
    "\n",
    "#     coef = sub_basc['coef']\n",
    "#     for j in range(0, len(coef)):\n",
    "#         s_data.append(coef[j])\n",
    "\n",
    "#     sub_basc.sort_values(by='coef', axis = 0, ascending = False, inplace=True)\n",
    "#     #print(sub_basc.iloc[0:12, 2:4])\n",
    "\n",
    "#     enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns),\n",
    "#                                        ignore_index=True)\n",
    "\n",
    "\n",
    "demo_data = sub_data.copy()\n",
    "demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "enc_ctl_data.insert(loc = 1, column = 'cognitive_status',\n",
    "                    value = demo_data['cognitive_status'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 2, column = 'total_scrubbed_frames',\n",
    "                    value = demo_data['total_scrubbed_frames'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 3, column = 'mean_FD',\n",
    "                    value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 6, column = 'correct_source',\n",
    "                    value = demo_data['correct_source'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 7, column = 'wrong_source',\n",
    "                    value = demo_data['wrong_source'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 8, column = 'dprime',\n",
    "                    value = demo_data['dprime'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 9, column = 'associative_memScore',\n",
    "                    value = demo_data['associative_memScore'], allow_duplicates=True)    \n",
    "\n",
    "# enc_ctl_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_enc_ctl_'+str(numnet)+'networks.tsv'),\n",
    "#     sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view_img(mean_img(alltrials_betas_A),bg_img=anat0, threshold='auto')\n",
    "alltrials_betas_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version B: A separate model created for each trial.\n",
    "Trial of interest modelled as its own condition, and other trials (encoding and control, excluding trial of interest) are modelled as a single condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outBeta_dir_B = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Betas/122922/OneModelPerTrial_B'\n",
    "all_betas_filelist_B = []\n",
    "####\n",
    "\n",
    "#Create a design matrix, first level model and beta map for each encoding and control trial \n",
    "for i in range (0, numTrials):\n",
    "\n",
    "    #copy all_events dataframe to keep the original intact\n",
    "    events = all_events.copy(deep = True)\n",
    "\n",
    "    #Determine trial number and condition (encoding or control)\n",
    "    tnum = events.iloc[i, 6]\n",
    "    currentCondi = events.iloc[i, 3]\n",
    "    tname = events.iloc[i, 2]\n",
    "        \n",
    "    #Version A: (2 conditions modelled separately)\n",
    "    #modify trial_type column to model only the trial of interest \n",
    "    for j in events.index:\n",
    "        if events.loc[j, 'trial_number'] != tnum:\n",
    "            events.loc[j, 'trial_type']= 'X_otherCondi'\n",
    "            #X for condition to remain in alphabetical order: trial of interest, X_CTL, X_Enc\n",
    "    #verify: what determines the order of columns in design matrix?    \n",
    "\n",
    "    #remove unecessary columns    \n",
    "    cols = ['onset', 'duration', 'trial_type']\n",
    "    events = events[cols]\n",
    "    \n",
    "    #create the model\n",
    "    s_model = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                               hrf_model = hrf_model)    \n",
    "    #Should data be standardized?\n",
    "\n",
    "    #create the design matrices\n",
    "    design = make_first_level_design_matrix(frame_times,\n",
    "                                            events=events,\n",
    "                                            drift_model=None,\n",
    "#                                             add_regs=confounds, \n",
    "                                            hrf_model=hrf_model)\n",
    "    \n",
    "    #fit model with design matrix\n",
    "    s_model = s_model.fit(fmri_img, design_matrices = design)\n",
    "    \n",
    "    design_matrix = s_model.design_matrices_[0]\n",
    "    \n",
    "    #sanity check: print design matrices and corresponding parameter labels\n",
    "    #plot outputed design matrix for visualization\n",
    "    print(str(tnum), ' ', tname, ' ', design_matrix.columns[0:3])\n",
    "    plot_design_matrix(design_matrix)\n",
    "    plt.show()\n",
    "\n",
    "    #Contrast vector: 1 in design matrix column that corresponds to trial of interest, 0s elsewhere\n",
    "    contrast_vec = np.repeat(0, design_matrix.shape[1])\n",
    "    contrast_vec[0] = 1\n",
    "\n",
    "    #compute the contrast's beta maps with the model.compute_contrast() method,\n",
    "    #based on contrast provided. \n",
    "    #https://nistats.github.io/modules/generated/nistats.first_level_model.FirstLevelModel.html\n",
    "    b_map = s_model.compute_contrast(contrast_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "    b_name = os.path.join(outBeta_dir_B, 'betas_sub'+str(id)+'_Trial'+str(tnum)+'_'+tname+'.nii')\n",
    "    #export b_map .nii image in output directory\n",
    "    nibabel.save(b_map, b_name)\n",
    "    print(os.path.basename(b_name))\n",
    "    all_betas_filelist_B.append(b_name)\n",
    "    \n",
    "alltrials_betas_B = nibabel.funcs.concat_images(images=all_betas_filelist_B, check_affines=True, axis=None)\n",
    "print(alltrials_betas_B.shape)\n",
    "nibabel.save(alltrials_betas_B, os.path.join(outBeta_dir_B, 'concat_all_betas_sub'+str(id)+'.nii'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outBeta_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Betas/122922/Conditions_Contrasts'\n",
    "\n",
    "#Model 1: encoding vs control conditions\n",
    "events1 = all_events.copy(deep = True)\n",
    "cols = ['onset', 'duration', 'condition']\n",
    "events1 = events1[cols]\n",
    "events1.rename(columns={'condition':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events1.head())\n",
    "\n",
    "#create the model\n",
    "model1 = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                         hrf_model = hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design1 = make_first_level_design_matrix(frame_times, events=events1,\n",
    "                                        drift_model=None, add_regs=confounds, \n",
    "                                        hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model1 = model1.fit(fmri_img, design_matrices = design1)    \n",
    "\n",
    "design_matrix1 = model1.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix1)\n",
    "plt.show()\n",
    "print(design_matrix1.columns[0:5])\n",
    "\n",
    "#Condition order: control, encoding (alphabetical)\n",
    "\n",
    "#contrast 1.1: control condition\n",
    "ctl_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "ctl_vec[0] = 1\n",
    "b11_map = model1.compute_contrast(ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b11_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ctl.nii')\n",
    "nibabel.save(b11_map, b11_name)\n",
    "print(os.path.basename(b11_name))\n",
    "print(ctl_vec)\n",
    "\n",
    "#contrast 1.2: encoding condition\n",
    "enc_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "enc_vec[1] = 1\n",
    "b12_map = model1.compute_contrast(enc_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b12_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_enc.nii')\n",
    "nibabel.save(b12_map, b12_name)\n",
    "print(os.path.basename(b12_name))\n",
    "print(enc_vec)\n",
    "\n",
    "#contrast 1.3: encoding minus control \n",
    "encMinCtl_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "encMinCtl_vec[1] = 1\n",
    "encMinCtl_vec[0] = -1\n",
    "b13_map = model1.compute_contrast(encMinCtl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b13_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_enc_minus_ctl.nii')\n",
    "nibabel.save(b13_map, b13_name)\n",
    "print(os.path.basename(b13_name))\n",
    "print(encMinCtl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: missed vs hit encoding trials\n",
    "events2 = all_events.copy(deep = True)\n",
    "cols2 = ['onset', 'duration', 'ctl_miss_hit']\n",
    "events2 = events2[cols2]\n",
    "events2.rename(columns={'ctl_miss_hit':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events2.iloc[0:15, :])\n",
    "\n",
    "#create the model\n",
    "model2 = FirstLevelModel(t_r=tr,\n",
    "                         drift_model = None,\n",
    "                         standardize = True,\n",
    "                         noise_model='ar1',\n",
    "                         hrf_model = hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design2 = make_first_level_design_matrix(frame_times, events=events2,\n",
    "                                         drift_model=None,\n",
    "                                         add_regs=confounds, \n",
    "                                         hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model2 = model2.fit(fmri_img, design_matrices = design2)    \n",
    "\n",
    "design_matrix2 = model2.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix2)\n",
    "plt.show()\n",
    "print(design_matrix2.columns[0:5])\n",
    "\n",
    "##Condition order: control, hit, missed (alphabetical)\n",
    "\n",
    "#contrast 2.1: miss \n",
    "miss_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "miss_vec[2] = 1\n",
    "b21_map = model2.compute_contrast(miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b21_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_miss.nii')\n",
    "nibabel.save(b21_map, b21_name)\n",
    "print(os.path.basename(b21_name))\n",
    "print(miss_vec)\n",
    "\n",
    "#contrast 2.2: hit \n",
    "hit_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_vec[1] = 1\n",
    "b22_map = model2.compute_contrast(hit_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b22_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit.nii')\n",
    "nibabel.save(b22_map, b22_name)\n",
    "print(os.path.basename(b22_name))\n",
    "print(hit_vec)\n",
    "\n",
    "#contrast 2.3: hit minus miss\n",
    "hit_min_miss_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_min_miss_vec[1] = 1\n",
    "hit_min_miss_vec[2] = -1\n",
    "b23_map = model2.compute_contrast(hit_min_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b23_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit_minus_miss.nii')\n",
    "nibabel.save(b23_map, b23_name)\n",
    "print(os.path.basename(b23_name))\n",
    "print(hit_min_miss_vec)\n",
    "\n",
    "#contrast 2.4: hit minus control\n",
    "hit_min_ctl_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_min_ctl_vec[1] = 1\n",
    "hit_min_ctl_vec[0] = -1\n",
    "b24_map = model2.compute_contrast(hit_min_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b24_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit_minus_ctl.nii')\n",
    "nibabel.save(b24_map, b24_name)\n",
    "print(os.path.basename(b24_name))\n",
    "print(hit_min_ctl_vec)\n",
    "\n",
    "#contrast 2.5: miss minus control \n",
    "miss_min_ctl_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "miss_min_ctl_vec[2] = 1\n",
    "miss_min_ctl_vec[0] = -1\n",
    "b25_map = model2.compute_contrast(miss_min_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b25_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_miss_minus_ctl.nii')\n",
    "nibabel.save(b25_map, b25_name)\n",
    "print(os.path.basename(b25_name))\n",
    "print(miss_min_ctl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: correct source vs wrong source encoding trials\n",
    "events3 = all_events.copy(deep = True)\n",
    "cols3 = ['onset', 'duration', 'ctl_miss_ws_cs']\n",
    "events3 = events3[cols3]\n",
    "events3.rename(columns={'ctl_miss_ws_cs':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events3.iloc[0:15, :])\n",
    "\n",
    "#create the model\n",
    "model3 = FirstLevelModel(t_r=tr,\n",
    "                         drift_model=None,\n",
    "                         standardize=True,\n",
    "                         noise_model='ar1',\n",
    "                         hrf_model=hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design3 = make_first_level_design_matrix(frame_times, events=events3,\n",
    "                                        drift_model=None,\n",
    "                                         add_regs=confounds, \n",
    "                                        hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model3 = model3.fit(fmri_img, design_matrices = design3)    \n",
    "\n",
    "design_matrix3 = model3.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix3)\n",
    "plt.show()\n",
    "print(design_matrix3.columns[0:5])\n",
    "\n",
    "##Condition order: control, correct source, missed, wrong source (alphabetical)\n",
    "\n",
    "#contrast 3.1: wrong source \n",
    "ws_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_vec[3] = 1\n",
    "b31_map = model3.compute_contrast(ws_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b31_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws.nii')\n",
    "nibabel.save(b31_map, b31_name)\n",
    "print(os.path.basename(b31_name))\n",
    "print(ws_vec)\n",
    "\n",
    "#contrast 3.2: correct source\n",
    "cs_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_vec[1] = 1\n",
    "b32_map = model3.compute_contrast(cs_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b32_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs.nii')\n",
    "nibabel.save(b32_map, b32_name)\n",
    "print(os.path.basename(b32_name))\n",
    "print(cs_vec)\n",
    "\n",
    "#contrast 3.3: correct source minus wrong source\n",
    "cs_minus_ws_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_ws_vec[1] = 1\n",
    "cs_minus_ws_vec[3] = -1\n",
    "b33_map = model3.compute_contrast(cs_minus_ws_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b33_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_ws.nii')\n",
    "nibabel.save(b33_map, b33_name)\n",
    "print(os.path.basename(b33_name))\n",
    "print(cs_minus_ws_vec)\n",
    "\n",
    "#contrast 3.4: correct source minus miss\n",
    "cs_minus_miss_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_miss_vec[1] = 1\n",
    "cs_minus_miss_vec[2] = -1\n",
    "b34_map = model3.compute_contrast(cs_minus_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b34_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_miss.nii')\n",
    "nibabel.save(b34_map, b34_name)\n",
    "print(os.path.basename(b34_name))\n",
    "print(cs_minus_miss_vec)\n",
    "\n",
    "#contrast 3.5: wrong source minus miss\n",
    "ws_minus_miss_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_minus_miss_vec[3] = 1\n",
    "ws_minus_miss_vec[2] = -1\n",
    "b35_map = model3.compute_contrast(ws_minus_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b35_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws_minus_miss.nii')\n",
    "nibabel.save(b35_map, b35_name)\n",
    "print(os.path.basename(b35_name))\n",
    "print(ws_minus_miss_vec)\n",
    "\n",
    "#contrast 3.6: correct source minus control\n",
    "cs_minus_ctl_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_ctl_vec[1] = 1\n",
    "cs_minus_ctl_vec[0] = -1\n",
    "b36_map = model3.compute_contrast(cs_minus_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b36_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_ctl.nii')\n",
    "nibabel.save(b36_map, b36_name)\n",
    "print(os.path.basename(b36_name))\n",
    "print(cs_minus_ctl_vec)\n",
    "\n",
    "#contrast 3.7: wrong source minus control\n",
    "ws_minus_ctl_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_minus_ctl_vec[3] = 1\n",
    "ws_minus_ctl_vec[0] = -1\n",
    "b37_map = model3.compute_contrast(ws_minus_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b37_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws_minus_ctl.nii')\n",
    "nibabel.save(b37_map, b37_name)\n",
    "print(os.path.basename(b37_name))\n",
    "print(ws_minus_ctl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize beta maps\n",
    "\n",
    "#plotting brain images in nilearn:\n",
    "#http://nilearn.github.io/plotting/index.html\n",
    "\n",
    "#define directory where subject's functional mask and anatomical scan reside\n",
    "anat_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/anat/122922'\n",
    "#subject's anatomical scan\n",
    "anat = nibabel.load(os.path.join(anat_dir, 'anat_sub122922_nuc_stereonl.nii'))\n",
    "plot_anat(anat)\n",
    "\n",
    "beta_list = glob.glob(os.path.join(outBeta_dir, '*.nii'))\n",
    "\n",
    "for beta in beta_list:\n",
    "    print(beta)\n",
    "    plot_img(beta)\n",
    "    plot_stat_map(stat_map_img=beta, bg_img=anat, cut_coords=(0, 0, 0), threshold=0.001, colorbar=True)\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
