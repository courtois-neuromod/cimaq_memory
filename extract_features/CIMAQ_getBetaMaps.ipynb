{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing steps to create trial-unique beta maps from the CIMAQ fMRI data \n",
    "(memory task: image encoding), within a single subject:\n",
    "\n",
    "Outputs (beta maps) are meant to be fed as features to a within-subject nilearn classifier.\n",
    "\n",
    "Input:\n",
    "    - task (event) files \n",
    "    - confound (motion, etc) files generated load_confound\n",
    "    - preprocessed FMRIPrep data (4D .nii file)\n",
    "        - note: data are not smoothed nor denoised)\n",
    "Output: 1 map (3D .nii file) of beta (regression) weights for each trial, and 1 \n",
    "concatenated 4D file of these 3D maps (trials ordered chronologically).\n",
    "\n",
    "Note: comparing 2 different ways to model trials of no interest\n",
    "\n",
    "version 1: Separate model for each trial\n",
    "    - Trial of interest modelled as a separate condition (1 regressor)\n",
    "    - All other trials modelled in either the Encoding or Control condition (2 regressors)\n",
    "**Update: betas from version 1 lead to much better enc/ctl trial classification**\n",
    "\n",
    "version 2: a separate model is built for each trial, with the trial of interest modelled as a separate condition (1 regressor), and all the other trials modelled as a single \"other\" condition (1 regressor)\n",
    "**Update: betas from version2 lead to poorer enc/ctl trial classification**\n",
    "\n",
    "Reference: how to derive beta maps for MVPA classification (Mumford et al., 2012):\n",
    "https://www.sciencedirect.com/science/article/pii/S1053811911010081\n",
    "\n",
    "Also creating contrasts per condition (to derive features for between-subject classification): \n",
    " - Modeling enconding and control conditions across trials\n",
    "     - 3 beta maps:\n",
    "         - encoding (enc) , control (ctl), and encoding minus control (enc_minus_ctl)\n",
    " - Modeling control condition, as well as the encoding condition according to task performance:\n",
    "    - miss and hit (post-scan image recognition performance)\n",
    "    - 5 beta maps:\n",
    "        - miss, hit hit_minus_miss, hit_minus_ctl, miss_minus_ctl\n",
    "    - Modeling control condition & encoding condition according to task performance:\n",
    "        - miss, wrong source, and correct source\n",
    "    - 7 beta maps:\n",
    "         - wrong_source, corr_source, cs_minus_ws, cs_minus_miss, ws_minus_miss, cs_minus_ctl, ws_minus_ctl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import more_itertools\n",
    "import nibabel\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import sys\n",
    "\n",
    "\n",
    "from load_confounds import Minimal\n",
    "from joblib import parallel_backend\n",
    "from numpy import nan as NaN\n",
    "from matplotlib import pyplot as plt\n",
    "from nibabel.nifti1 import Nifti1Image\n",
    "from pathlib import Path\n",
    "# from nilearn.glm.first_level import FirstLevelModel, check_events\n",
    "# from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "# from nilearn import plotting as niplot\n",
    "# from nilearn import image as nimage\n",
    "# from nilearn.masking import apply_mask\n",
    "# from nilearn.signal import clean\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "#libraries need to be installed in conda environment with pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nibabel.nifti1 import Nifti1Image\n",
    "from typing import Union\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\" Yield successive n-sized chunks from lst. \"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def img_to_mask3d(source_img:Nifti1Image\n",
    "                 ) -> Nifti1Image:\n",
    "    import numpy as np\n",
    "    from nilearn.image import new_img_like\n",
    "    return new_img_like(ref_niimg=source_img,\n",
    "                        data=np.where(source_img.get_fdata() \\\n",
    "                                      > 0, 1, 0))\n",
    "\n",
    "def format_difumo(indexes:list,\n",
    "                  target_img:Nifti1Image,\n",
    "                  dimension:int=256,\n",
    "                  resolution_mm:int=3,\n",
    "                  data_dir:Union[str,os.PathLike]=None,\n",
    "                  **kwargs\n",
    "                  ) -> Nifti1Image:\n",
    "    import numpy as np\n",
    "    from nilearn.datasets import fetch_atlas_difumo\n",
    "    from nilearn.masking import intersect_masks\n",
    "    from nilearn import image as nimage\n",
    "    difumo = fetch_atlas_difumo(dimension=dimension,\n",
    "                                resolution_mm=resolution_mm,\n",
    "                                data_dir=data_dir)\n",
    "    rois = nimage.index_img(nimage.load_img(difumo.maps), indexes)\n",
    "    rois_resampled = nimage.resample_to_img(source_img=nimage.iter_img(rois),\n",
    "                                            target_img=target_img,\n",
    "                                            interpolation='nearest',\n",
    "                                            copy=True, order='F',\n",
    "                                            clip=True, fill_value=0,\n",
    "                                            force_resample=True)\n",
    "    return intersect_masks(mask_imgs=list(map(img_to_mask3d,\n",
    "                                              list(nimage.iter_img(rois_resampled)))),\n",
    "                           threshold=0.0, connected=True)\n",
    "\n",
    "\n",
    "#     return next((False if not os.path.exist(evpath)\n",
    "#                  else evpath))\n",
    "\n",
    "\n",
    "# def make_fit_1stlevel_glm(fmri_img:Union[str,os.PathLike,Nifti1Image],\n",
    "#                           events:pd.DataFrame,\n",
    "#                           confounds:pd.DataFrame=None,\n",
    "#                           **kwargs):\n",
    "#     short_events = events[['onset','trial_type','duration']].copy(deep=True)\n",
    "# #     Add modulation column to events since ctl classification is 2x poorer than enc?\n",
    "# #     short_events['modulation'] = [0.2 if 'Enc' in ttype else 0.9\n",
    "# #                                   for ttype in short_events.trial_type]\n",
    "\n",
    "\n",
    "#     glm_params = dict(t_r=fmri_img.header.get_zooms()[-1],\n",
    "#                       mask_img=mask_img,\n",
    "#                       drift_model=design_params['drift_model'],\n",
    "#                       standardize=False,\n",
    "#                       smoothing_fwhm=None,\n",
    "#                       signal_scaling=False,\n",
    "#                       noise_model='ar1',\n",
    "#                       hrf_model=design_params['hrf_model'],\n",
    "#                       minimize_memory=False)\n",
    "#     design = make_first_level_design_matrix(**design_params)\n",
    "#     model = FirstLevelModel(**glm_params).fit(fmri_img, design_matrices=design)\n",
    "#     return model\n",
    "\n",
    "#############################\n",
    "# def trial_fmri(fmri_path:Union[str,os.PathLike, Nifti1Image],\n",
    "#                events_path:Union[str,os.PathLike, pd.DataFrame],\n",
    "#                sep:str='\\t',\n",
    "#                **kwargs):\n",
    "#     from itertools import starmap\n",
    "#     from nilearn import image as nimage\n",
    "#     import pandas as pd\n",
    "#     # Make pandas Intervals (b:list of beginnigs, e:list of ends)\n",
    "#     mkintrvls = lambda b, e: list(starmap(pd.Interval,tuple(zip(b, e))))\n",
    "#     fmri_img = nimage.load_img(fmri_path)\n",
    "#     events = [events if isinstance(events, pd.DataFrame)\n",
    "#                else pd.read_csv(events_path, sep=sep)][0]\n",
    "#     t_r = fmri_img.header.get_zooms()[-1]\n",
    "#     frame_times = np.arange(fmri_img.shape[-1]) * t_r\n",
    "#     frame_intervals = mkintrvls(pd.Series(frame_times).values,\n",
    "#                                 pd.Series(frame_times).add(t_r).values)\n",
    "#     trial_ends=(events.onset+abs(events.onset -\n",
    "#                                  events.offset)+events.isi).values\n",
    "#     trial_intervals = mkintrvls(events.onset.values, trial_ends)\n",
    "# #     trial_intervals = list(starmap(pd.Interval,tuple(zip(events.onset.values, trial_ends))))\n",
    "#     bold_by_trial_indx = [[frame[0] for frame in enumerate(frame_intervals)\n",
    "#                            if frame[1].left in trial] for trial in trial_intervals]\n",
    "#     bold_by_trial = list(nimage.index_img(fmri_img, idx)\n",
    "#                          for idx in bold_by_trial_indx)\n",
    "#     event_list = events.loc[[item[0] for item in\n",
    "#                               enumerate(bold_by_trial) if item != []]]\n",
    "#     mem_labels, recall_labels = events_list.contidion, events_list.recognition_performance\n",
    "#     return bold_by_trial, mem_labels, recall_labels\n",
    "###############################\n",
    "\n",
    "def trial_fmri(fmri_path:Union[str,os.PathLike, Nifti1Image],\n",
    "               events_path:Union[str,os.PathLike, pd.DataFrame],\n",
    "               sep:str='\\t', t_r:float=None,\n",
    "               **kwargs):\n",
    "    from itertools import starmap\n",
    "    from more_itertools import flatten\n",
    "    from nilearn import image as nimage\n",
    "    import pandas as pd\n",
    "    # Make pandas Intervals (b:list of beginnigs, e:list of ends)\n",
    "    mkintrvls = lambda b, e: list(starmap(pd.Interval,tuple(zip(b, e))))\n",
    "    fmri_img = nimage.load_img(fmri_path)\n",
    "    if not isinstance(events_path, pd.DataFrame):\n",
    "        events = pd.read_csv(events_path, sep=sep)\n",
    "    else:\n",
    "        events = events_path\n",
    "    t_r = [t_r if t_r is not None else\n",
    "           fmri_img.header.get_zooms()[-1]][0]\n",
    "    frame_times = np.arange(fmri_img.shape[-1]) * t_r\n",
    "    frame_ends = pd.Series(frame_times).add(t_r).values\n",
    "    frame_intervals = mkintrvls(pd.Series(frame_times).values,\n",
    "                                frame_ends)\n",
    "    trial_ends=(events.onset+abs(events.onset -\n",
    "                                 events.offset)+events.isi).values\n",
    "    trial_intervals = mkintrvls(events.onset.values, trial_ends)\n",
    "    valid_trial_idx = [trial[0] for trial in enumerate(trial_intervals)\n",
    "                       if trial[1].left<frame_intervals[-1].left]\n",
    "    valid_trials = pd.Series(trial_intervals).loc[valid_trial_idx].values\n",
    "#     trial_intervals = list(starmap(pd.Interval,tuple(zip(events.onset.values, trial_ends))))\n",
    "    bold_by_trial_indx = [[frame[0] for frame in enumerate(frame_intervals)\n",
    "                           if frame[1].left in trial] for trial in valid_trials]\n",
    "    bold_by_trial = list(nimage.index_img(fmri_img, idx)\n",
    "                         for idx in bold_by_trial_indx)\n",
    "    valid_frame_intervals = [pd.Series(frame_intervals).loc[bold_idx].values\n",
    "                             for bold_idx in bold_by_trial_indx]\n",
    "    perfo_labels = events.iloc[valid_trial_idx].recognition_performance.fillna('Ctl')\n",
    "    condition_labels = events.iloc[valid_trial_idx].trial_type\n",
    "    stim_labels = events.iloc[valid_trial_idx].stim_file.fillna('Ctl').values\n",
    "    categ_labels = events.iloc[valid_trial_idx].stim_category.fillna('Ctl').values\n",
    "    return pd.DataFrame(tuple(zip(valid_trial_idx, bold_by_trial,\n",
    "                                  bold_by_trial_indx, valid_trials,\n",
    "                                  valid_frame_intervals, condition_labels,\n",
    "                                  perfo_labels, stim_labels, categ_labels)),\n",
    "                        columns=['trials', 'trial_niftis', 'fmri_frames',\n",
    "                                 'trial_intervals', 'fmri_frame_intervals',\n",
    "                                 'condition_labels', 'performance_labels',\n",
    "                                 'stimuli_files', 'category_labels'])\n",
    "#     return pd.DataFrame(tuple(zip(bold_by_trial, condition_labels, perfo_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fmriprep_session(fmri_path:Union[str,os.PathLike],\n",
    "                           events_dir:Union[str,os.PathLike],\n",
    "                           strategy:str='Minimal',\n",
    "                           task:str='memory',\n",
    "                           space:str='MNI152NLin2009cAsym',\n",
    "                           anat_mod:str='T1w',\n",
    "                           lc_kws:dict=None,\n",
    "                           apply_kws:dict=None,\n",
    "                           clean_kws:dict=None,\n",
    "                           design_kws:dict=None,\n",
    "                           glm_kws:dict=None,\n",
    "                           masker_kws:dict=None,\n",
    "                           **kwargs):\n",
    "    import load_confounds\n",
    "    from inspect import getmembers\n",
    "    from nilearn import image as nimage\n",
    "    from sklearn.utils import Bunch\n",
    "    from pathlib import Path\n",
    "    \n",
    "    from cimaq_decoding_params import _params\n",
    "    from cimaq_decoding_utils import get_fmriprep_anat, get_fmriprep_mask\n",
    "    from cimaq_decoding_utils import clean_fmri, get_events\n",
    "    from cimaq_decoding_utils import get_tr, get_frame_times\n",
    "\n",
    "    events_path = get_events(fmri_path, events_dir)\n",
    "    if events_path is False:\n",
    "        return False\n",
    "    events = pd.read_csv(events_path, sep='\\t').iloc[1:,:]    \n",
    "    sub_id, ses_id = Path(fmri_path).parts[-4:-2]\n",
    "    mask_path = get_fmriprep_mask(fmri_path)\n",
    "    anat_path = get_fmriprep_anat(fmri_path)\n",
    "    loader = dict(getmembers(load_confounds))[f'{strategy}']\n",
    "    loader = [loader(**lc_kws) if lc_kws is not None\n",
    "              else loader()][0]\n",
    "    conf = loader.load(fmri_path)\n",
    "    fmri_img, mask_img, anat_img = tuple(map(nimage.load_img,\n",
    "                                             [fmri_path, mask_path,\n",
    "                                              anat_path]))\n",
    "    t_r, frame_times = get_tr(fmri_img), get_frame_times(fmri_img)\n",
    "    if apply_kws is not None:\n",
    "        _params.apply_defs.update(apply_kws)    \n",
    "    if clean_kws is not None:\n",
    "        _params.clean_defs.update(clean_kws)\n",
    "    cleaned_fmri = clean_fmri(fmri_img, mask_img, confounds=conf,\n",
    "                              apply_kws=_params.apply_defs,\n",
    "                              clean_kws=_params.clean_defs)\n",
    "#                               **_params.apply_defs)\n",
    "\n",
    "    # Argument definitions for each preprocessing step\n",
    "    target_shape, target_affine = mask_img.shape, cleaned_fmri.affine\n",
    "\n",
    "    _params.glm_defs.update(dict(t_r=t_r, mask_img=mask_img,\n",
    "                                 target_shape=target_shape,\n",
    "                                 target_affine=target_affine))\n",
    "\n",
    "    if design_kws is not None:\n",
    "        _params.design_defs.update(design_kws)\n",
    "    if masker_kws is not None:\n",
    "        _params.masker_defs.update(masker_kws)\n",
    "    if glm_kws is not None:\n",
    "        _params.glm_defs.update(glm_kws)\n",
    "\n",
    "    return Bunch(sub_id=sub_id, ses_id=ses_id, task=task, space=space,\n",
    "                 fmri_path=fmri_path, mask_path=mask_path, events_path=events_path,\n",
    "                 confounds_loader=loader, confounds=conf, confounds_strategy=strategy,\n",
    "                 smoothing_fwhm=_params.apply_defs.smoothing_fwhm,\n",
    "                 fmri_img=fmri_img, mask_img=mask_img,\n",
    "                 cleaned_fmri=cleaned_fmri, events=events,\n",
    "                 **Bunch(clean_defs=_params.clean_defs,\n",
    "                         design_defs=_params.design_defs,\n",
    "                         glm_defs=_params.glm_defs,\n",
    "                         masker_defs=_params.masker_defs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/'\n",
    "events_dir = '/data/simexp/CIMAQ_AS_BIDS/'\n",
    "path00='/data/simexp/cimaq_preproc/fmriprep/sub-3002498/ses-V10/func/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "\n",
    "sub00 = fetch_fmriprep_session(path00,events_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.vectors = trial_fmri(sub00.cleaned_fmri, sub00.events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "atlases_dir = '../../nilearn_atlases/difumo_atlases/'#/difumo_atlases/256/3mm/maps.nii.gz'\n",
    "\n",
    "difumo_256 = get_difumo('../../nilearn_atlases/difumo_atlases/', 256, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter('ignore', category='VisibleDeprecationWarning')\n",
    "\n",
    "from nilearn.input_data import NiftiMasker, NiftiMapsMasker, NiftiSpheresMasker\n",
    "\n",
    "\n",
    "sub00.spheresmasker = NiftiSpheresMasker(tuple(map(tuple,difumo256.labels[['gm','wm','csf']])),\n",
    "                                        radius=None, allow_overlap=True,\n",
    "                                        **sub00.masker_defs)\n",
    "\n",
    "sub00.mapsmasker = NiftiMapsMasker(difumo256.maps, mask_img=sub00.mask_img,\n",
    "                                   resampling_target='mask',\n",
    "                                   allow_overlap=True, **sub00.masker_defs)\n",
    "\n",
    "sub00.regmasker = NiftiMasker(target_affine=sub00.cleaned_fmri.affine,\n",
    "                              mask_strategy='whole-brain-template',\n",
    "                              **sub00.masker_defs)\n",
    "# help(NiftiMasker)\n",
    "# (seeds, radius=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method transform_single_imgs in module nilearn.input_data.nifti_spheres_masker:\n",
      "\n",
      "transform_single_imgs(imgs, confounds=None, sample_mask=None) method of nilearn.input_data.nifti_spheres_masker.NiftiSpheresMasker instance\n",
      "    Extract signals from a single 4D niimg.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    imgs : 3D/4D Niimg-like object\n",
      "        See http://nilearn.github.io/manipulating_images/input_output.html\n",
      "        Images to process. It must boil down to a 4D image with scans\n",
      "        number as last dimension.\n",
      "    \n",
      "    confounds : CSV file or array-like or pandas DataFrame, optional\n",
      "        This parameter is passed to signal.clean. Please see the related\n",
      "        documentation for details.\n",
      "        shape: (number of scans, number of confounds)\n",
      "    \n",
      "    sample_mask : Any type compatible with numpy-array indexing, optional\n",
      "        Masks the niimgs along time/fourth dimension to perform scrubbing\n",
      "        (remove volumes with high motion) and/or non-steady-state volumes.\n",
      "        This parameter is passed to signal.clean.\n",
      "        shape: (number of scans - number of volumes removed, )\n",
      "    \n",
      "            .. versionadded:: 0.8.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    region_signals : 2D numpy.ndarray\n",
      "        Signal for each sphere.\n",
      "        shape: (number of scans, number of spheres)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sub00.spheresmasker.transform_single_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiftiSpheresMasker(allow_overlap=True,\n",
       "                   seeds=((0.586695983486764, 0.368756961303868,\n",
       "                           0.043084972984381),\n",
       "                          (0.669211638091079, 0.254778712999749,\n",
       "                           0.06971050882056),\n",
       "                          (0.362251970608158, 0.062204248082339,\n",
       "                           0.148603287738793),\n",
       "                          (0.629230791189012, 0.290432238090537,\n",
       "                           0.070572678551846),\n",
       "                          (0.227904058238554, 0.688151064336859,\n",
       "                           0.083951217965771),\n",
       "                          (0.560710360527329, 0.36266...\n",
       "                          (0.582794072304041, 0.350574534648094,\n",
       "                           0.058785683416785),\n",
       "                          (0.610322177238097, 0.310950019622802,\n",
       "                           0.074369088438938),\n",
       "                          (0.567642388970224, 0.244854281471568,\n",
       "                           0.156329745067564),\n",
       "                          (0.772916764257577, 0.126685757122274,\n",
       "                           0.094589720421785),\n",
       "                          (0.629624308488307, 0.143599874535309,\n",
       "                           0.116946956878528),\n",
       "                          (0.636665208013452, 0.180229414000553,\n",
       "                           0.171857982298678), ...),\n",
       "                   standardize_confounds=False)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn import image as nimage\n",
    "sub00.mapsmasker.fit(sub00.cleaned_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmean = nilearn.image.concat_imgs([nilearn.image.mean_img(trial)\n",
    "                           for trial in sub00.vectors.trial_niftis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmean_signals = sub00.spheresmasker.transform_single_imgs(testmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 116/116 [27:36<00:00, 14.28s/it]\n"
     ]
    }
   ],
   "source": [
    "spheres_trials = list(sub00.spheresmasker.transform_single_imgs(trial)\n",
    "                      for trial in tqdm(sub00.vectors.trial_niftis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 116/116 [28:16<00:00, 14.63s/it]\n"
     ]
    }
   ],
   "source": [
    "sheres_trial_means = list(sub00.spheresmasker.fit_transform(nimage.mean_img(trial))\n",
    "                          for trial in tqdm(sub00.vectors.trial_niftis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_trial_means = list(sub00.mapsmasker.fit_transform(nimage.mean_img(trial))\n",
    "                          for trial in tqdm(sub00.vectors.trial_niftis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_trial_means = list(sub00.regmasker.fit_transform(nimage.mean_img(trial))\n",
    "                       for trial in tqdm(sub00.vectors.trial_niftis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trial_number</th>\n",
       "      <th>trial_type</th>\n",
       "      <th>stim_id</th>\n",
       "      <th>position_correct</th>\n",
       "      <th>response_time</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "      <th>isi</th>\n",
       "      <th>duration</th>\n",
       "      <th>stim_file</th>\n",
       "      <th>stim_category</th>\n",
       "      <th>recognition_response</th>\n",
       "      <th>recognition_responsetime</th>\n",
       "      <th>position_response</th>\n",
       "      <th>position_responsetime</th>\n",
       "      <th>recognition_accuracy</th>\n",
       "      <th>position_accuracy</th>\n",
       "      <th>recognition_performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old56</td>\n",
       "      <td>6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>20.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sporting_boxing_gloves.bmp</td>\n",
       "      <td>sporting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old10</td>\n",
       "      <td>5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>23.6</td>\n",
       "      <td>26.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>animal_penguin.bmp</td>\n",
       "      <td>animal</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old24</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>27.6</td>\n",
       "      <td>30.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>food_softcheese.bmp</td>\n",
       "      <td>food</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old77</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>vegie_radish.bmp</td>\n",
       "      <td>vegie</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old55</td>\n",
       "      <td>8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>36.0</td>\n",
       "      <td>39.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>sporting_bicycle_old.bmp</td>\n",
       "      <td>sporting</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>116</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old48</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>694.4</td>\n",
       "      <td>697.4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>musical_piano_old.bmp</td>\n",
       "      <td>musical</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>117</td>\n",
       "      <td>Ctl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>703.4</td>\n",
       "      <td>706.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>118</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old18</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>717.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>food_cracker02.bmp</td>\n",
       "      <td>food</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>119</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old30</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>725.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>kitchen_cookie cutter.bmp</td>\n",
       "      <td>kitchen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>120</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Old26</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>729.0</td>\n",
       "      <td>732.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>food_toast.bmp</td>\n",
       "      <td>food</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Miss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trial_number trial_type stim_id  position_correct  response_time  onset  \\\n",
       "1               5        Enc   Old56                 6            1.6   20.1   \n",
       "2               6        Enc   Old10                 5            1.3   23.6   \n",
       "3               7        Enc   Old24                 6            1.2   27.6   \n",
       "4               8        Enc   Old77                 9            1.0   31.6   \n",
       "5               9        Enc   Old55                 8            0.8   36.0   \n",
       "..            ...        ...     ...               ...            ...    ...   \n",
       "112           116        Enc   Old48                 6            0.7  694.4   \n",
       "113           117        Ctl     NaN                 9            0.7  703.4   \n",
       "114           118        Enc   Old18                 5            1.0  717.0   \n",
       "115           119        Enc   Old30                 5            2.0  725.0   \n",
       "116           120        Enc   Old26                 8            1.0  729.0   \n",
       "\n",
       "     offset   isi  duration                   stim_file stim_category  \\\n",
       "1      23.1   0.5       3.0  sporting_boxing_gloves.bmp      sporting   \n",
       "2      26.6   1.0       3.0          animal_penguin.bmp        animal   \n",
       "3      30.6   1.0       3.0         food_softcheese.bmp          food   \n",
       "4      34.6   1.5       3.0            vegie_radish.bmp         vegie   \n",
       "5      39.1   2.0       3.0    sporting_bicycle_old.bmp      sporting   \n",
       "..      ...   ...       ...                         ...           ...   \n",
       "112   697.4   6.0       3.0       musical_piano_old.bmp       musical   \n",
       "113   706.5  10.0       3.0                         NaN           NaN   \n",
       "114   720.0   5.0       3.0          food_cracker02.bmp          food   \n",
       "115   728.0   1.0       3.0   kitchen_cookie cutter.bmp       kitchen   \n",
       "116   732.0  18.0       3.0              food_toast.bmp          food   \n",
       "\n",
       "     recognition_response  recognition_responsetime  position_response  \\\n",
       "1                     1.0                       7.0                4.0   \n",
       "2                     1.0                       2.3                3.0   \n",
       "3                     1.0                       4.1                4.0   \n",
       "4                     2.0                       NaN                NaN   \n",
       "5                     1.0                       1.7                1.0   \n",
       "..                    ...                       ...                ...   \n",
       "112                   1.0                       2.2                4.0   \n",
       "113                   NaN                       NaN                NaN   \n",
       "114                   1.0                       2.0                3.0   \n",
       "115                   1.0                       2.0                3.0   \n",
       "116                   1.0                       1.8                1.0   \n",
       "\n",
       "     position_responsetime  recognition_accuracy  position_accuracy  \\\n",
       "1                      2.4                   1.0                0.0   \n",
       "2                      1.7                   1.0                0.0   \n",
       "3                      0.8                   1.0                0.0   \n",
       "4                      NaN                   0.0                NaN   \n",
       "5                      0.9                   1.0                0.0   \n",
       "..                     ...                   ...                ...   \n",
       "112                    0.9                   1.0                0.0   \n",
       "113                    NaN                   NaN                NaN   \n",
       "114                    1.7                   1.0                0.0   \n",
       "115                    1.7                   1.0                0.0   \n",
       "116                    0.7                   1.0                0.0   \n",
       "\n",
       "    recognition_performance  \n",
       "1                      Miss  \n",
       "2                      Miss  \n",
       "3                      Miss  \n",
       "4                        FA  \n",
       "5                      Miss  \n",
       "..                      ...  \n",
       "112                    Miss  \n",
       "113                     NaN  \n",
       "114                    Miss  \n",
       "115                    Miss  \n",
       "116                    Miss  \n",
       "\n",
       "[116 rows x 18 columns]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub00.events#.category_labels.replace({'K':'kitchen'}).str.lower().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVC in module sklearn.svm._classes:\n",
      "\n",
      "class LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |  \n",
      " |  Linear Support Vector Classification.\n",
      " |  \n",
      " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input and the multiclass support\n",
      " |  is handled according to a one-vs-the-rest scheme.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2'}, default='l2'\n",
      " |      Specifies the norm used in the penalization. The 'l2'\n",
      " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      " |      vectors that are sparse.\n",
      " |  \n",
      " |  loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
      " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      " |      square of the hinge loss. The combination of ``penalty='l1'``\n",
      " |      and ``loss='hinge'`` is not supported.\n",
      " |  \n",
      " |  dual : bool, default=True\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Regularization parameter. The strength of the regularization is\n",
      " |      inversely proportional to C. Must be strictly positive.\n",
      " |  \n",
      " |  multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
      " |      Determines the multi-class strategy if `y` contains more than\n",
      " |      two classes.\n",
      " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      " |      While `crammer_singer` is interesting from a theoretical perspective\n",
      " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      " |      to better accuracy and is more expensive to compute.\n",
      " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      " |      will be ignored.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      ``[x, self.intercept_scaling]``,\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the pseudo random number generation for shuffling the data for\n",
      " |      the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
      " |      underlying implementation of :class:`LinearSVC` is not random and\n",
      " |      ``random_state`` has no effect on the results.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem).\n",
      " |  \n",
      " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The unique classes labels.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Maximum number of iterations run across all classes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |      Furthermore SVC multi-class mode is implemented using one\n",
      " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      " |      possible to implement one vs the rest with SVC by using the\n",
      " |      :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      " |  \n",
      " |      Finally SVC can fit dense data without memory copy if the input\n",
      " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
      " |      cost function as LinearSVC\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller ``tol`` parameter.\n",
      " |  \n",
      " |  The underlying implementation, liblinear, uses a sparse internal\n",
      " |  representation for the data that will incur a memory copy.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  `LIBLINEAR: A Library for Large Linear Classification\n",
      " |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      " |  >>> clf = make_pipeline(StandardScaler(),\n",
      " |  ...                     LinearSVC(random_state=0, tol=1e-5))\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
      " |  \n",
      " |  >>> print(clf.named_steps['linearsvc'].coef_)\n",
      " |  [[0.141...   0.526... 0.679... 0.493...]]\n",
      " |  \n",
      " |  >>> print(clf.named_steps['linearsvc'].intercept_)\n",
      " |  [0.1693...]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVC\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples in the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.18\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          An instance of the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# counts=.condition_labels.value_counts()\n",
    "# (counts.min()/counts.max())/\n",
    "# {'Enc': round(counts.max()/counts.sum(), 2),\n",
    "#  'Ctl': round(counts.min()/counts.sum(), 2)}\n",
    "# # from sklearn.svm import SVC, LinearSVC\n",
    "help(LinearSVC)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# help(MinMaxScaler)\n",
    "# sub00.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_list=events00.recognition_performance.fillna('Ctl').values\n",
    "def classify_signals(signals, labels,\n",
    "                     test_size:float=0.4,\n",
    "                     n_tests:int=15,\n",
    "                     **kwargs):\n",
    "    import scipy\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "#     from sklearn.metrics import accuracy_score, classification_report\n",
    "#     from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "#     from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    scores = []\n",
    "#         with parallel_backend('threading',n_jobs=-1):\n",
    "    for test in range(n_tests):\n",
    "        sub_svc = sklearn.svm.LinearSVC(max_iter=10000000,\n",
    "                                        penalty='l2',\n",
    "                                        loss='hinge',\n",
    "#                                         multi_class='crammer_singer',\n",
    "#                                         class_weight={'Enc': 1/round(counts.max()/counts.sum(),2),\n",
    "#                                                       'Ctl': 1/round(counts.min()/counts.sum(),2)},\n",
    "#                                         intercept_scaling=0.1,\n",
    "                                        class_weight='balanced',\n",
    "                                        fit_intercept=True)\n",
    "#         scaler = MinMaxScaler()\n",
    "#         scaler.fit(signals)\n",
    "#         signals = scaler.transform(signals)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            signals, labels, # x & y\n",
    "            test_size = test_size, \n",
    "            shuffle = True,\n",
    "            stratify = labels)\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        print(len(list(filter(None, y_pred==y_test)))/len(y_test))\n",
    "        print(y_pred)\n",
    "        scores.append(sub_svc.score(X_test, y_test))\n",
    "    return np.mean(scores).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5531914893617021\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss' 'Miss' 'Ctl' 'Miss' 'Ctl'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Ctl' 'Miss' 'Miss' 'Ctl' 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.574468085106383\n",
      "['FA' 'FA' 'Miss' 'Ctl' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.5531914893617021\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'FA' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.574468085106383\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.5319148936170213\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss' 'FA' 'Ctl' 'FA' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Ctl'\n",
      " 'Miss' 'FA' 'Ctl' 'Miss' 'Miss' 'Miss' 'Ctl' 'Ctl' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.5957446808510638\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.5957446808510638\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.574468085106383\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'FA' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss']\n",
      "0.5106382978723404\n",
      "['Miss' 'Miss' 'Miss' 'FA' 'Miss' 'FA' 'FA' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.5531914893617021\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'FA' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.46808510638297873\n",
      "['Miss' 'Miss' 'FA' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Ctl' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl' 'Miss'\n",
      " 'Ctl' 'FA' 'Miss' 'Miss']\n",
      "0.5957446808510638\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.574468085106383\n",
      "['Miss' 'Ctl' 'Miss' 'Miss' 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Ctl'\n",
      " 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Ctl' 'Ctl' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss']\n",
      "0.48936170212765956\n",
      "['Miss' 'FA' 'Miss' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'FA'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'FA' 'Miss' 'Miss' 'FA' 'FA' 'FA' 'Miss'\n",
      " 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss'\n",
      " 'Miss' 'FA' 'Miss' 'Miss' 'FA' 'FA' 'Miss' 'Miss' 'FA' 'FA' 'Miss' 'Miss'\n",
      " 'Miss']\n",
      "0.48936170212765956\n",
      "['Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'FA' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss'\n",
      " 'FA' 'FA' 'Miss' 'Miss' 'Miss' 'Miss' 'Miss' 'FA' 'Miss' 'FA' 'Miss'\n",
      " 'Ctl' 'Miss' 'FA' 'Miss' 'Miss' 'Miss' 'FA' 'FA' 'Miss' 'Miss' 'Miss'\n",
      " 'Miss' 'Miss' 'Miss' 'Miss']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stim_categs = sub00.vectors.category_labels.replace({'K':'kitchen'}).str.lower()\n",
    "classify_signals(signals=testmean_signals,\n",
    "                labels=sub00.vectors.performance_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_files = fetch_cimaq(fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/',\n",
    "#                           events_dir = '/data/simexp/CIMAQ_AS_BIDS/',\n",
    "#                           atlases_dir = '../../nilearn_atlases/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as nimage\n",
    "sub00.maps_masker = NiftiMapsMasker(difumo256.maps, **sub00.masker_params)\n",
    "sub00.maps_labels = difumo256.labels.difumo_names\n",
    "sub00.vectors.time_series = [sub00.maps_masker.fit_transform(nimage.mean_img(row[1].trial_niftis))\n",
    "                             for row in sub00.vectors.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure as CM\n",
    "for time_series in sub00.stackdx_ts:\n",
    "    conn_meas = CM(kind='correlation').fit_transform([time_series.reshape(-1,1)])\n",
    "    print(conn_meas)\n",
    "# sub00.vectors.corr_mats = [np.fill_diagonal(CM(kind='correlation').fit_transform([time_series.reshape(-1,1)])[0],0)\n",
    "# #                            for time_series in sub00.stackdx_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# difumo_masker = NiftiMapsMasker(maps_img=difumo256.maps, standardize=False,\n",
    "#                                 memory='nilearn_cache', verbose=0)\n",
    "\n",
    "def get_corr_mat(fmri_img, masker, cond_idx:list=None,\n",
    "                 **kwargs):\n",
    "    import numpy as np\n",
    "    from nilearn import plotting as niplot\n",
    "    from nilearn.connectome import ConnectivityMeasure as CM\n",
    "    from nilearn.image import index_img\n",
    "    from nilearn.input_data import NiftiMapsMasker\n",
    "    if cond_idx is not None:\n",
    "        fmri_img = index_img(fmri_img, cond_idx)\n",
    "    time_series = masker.fit_transform(fmri_img)\n",
    "    correlation_matrix = CM(kind='correlation').fit_transform([time_series])[0]\n",
    "    np.fill_diagonal(correlation_matrix, 0) # Mask out the major diagonal\n",
    "    return time_series, correlation_matrix\n",
    "\n",
    "# [sub00.__setattr__(*attrs) for attrs in tuple(zip(['time_series', 'correlation_matrix'],\n",
    "#                                           get_corr_mat(sub00.cleaned_fmri,\n",
    "#                                                        sub00.maps_masker,\n",
    "#                                                        cond_idx)))]\n",
    "sub00.enc_ctl_timeseries, sub00.enc_ctl_corr_mat = \\\n",
    "    get_corr_mat(sub00.cleaned_fmri, sub00.maps_masker, )\n",
    "#     niplot.plot_matrix(correlation_matrix, labels=difumo256.labels,\n",
    "#                          colorbar=True, vmax=0.8, vmin=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.[vectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.vectors[['trial_arrays', 'performance_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub00.enc_ts = list(item[0] for item in tuple(zip(sub00.vectors.time_series,\n",
    "                                   sub00.vectors.condition_labels))\n",
    "              if item[1]=='Enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.ctl_ts = list(item[0] for item in tuple(zip(sub00.vectors.time_series,\n",
    "                                   sub00.vectors.condition_labels))\n",
    "              if item[1]=='Ctl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.vectors['time_series'] = [sub00.vectors.time_series[:,:,list(map(int,row[1].fmri_frames))]\n",
    "                        for row in sub00.vectors.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(np.nditer(sub00.vectors.time_series, itershape=(0,0,1))).__len__()\n",
    "# help(np.nditer)\n",
    "# sub00.vectors.time_series=np.array(sub00.vectors['time_series'].values.tolist())\n",
    "# sub00.vectors.time_series[:,:,0].shape\n",
    "sub00.vectors['trial_arrays'] = [sub00.vectors.time_series[:,:,ind]\n",
    "                                  for ind in sub00.vectors.index]\n",
    "cov_estimator=sklearn.svm.LinearSVC(max_iter=1000000,\n",
    "                                                                penalty='l2', loss='hinge',\n",
    "                                                                class_weight='balanced'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_mat_from_ts(time_series, labels):\n",
    "    import numpy as np\n",
    "    from nilearn.connectome import ConnectivityMeasure as CM\n",
    "    correlation_matrix = CM(kind='correlation').fit_transform([time_series],\n",
    "                                                              labels)\n",
    "    np.fill_diagonal(correlation_matrix, 0) # Mask out the major diagonal\n",
    "    return correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=np.array([difumo256_labels]*goodshape.shape[1]).T\n",
    "signals=goodshape\n",
    "labels.shape,signals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corr_mat_from_ts(time_series=signals, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([difumo256_labels]*goodshape.shape[1]).T.shape, goodshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure as CM\n",
    "# help(CM)\n",
    "goodshape=sub00.vectors.time_series.transpose(1,-1,0)[0]\n",
    "#.shape#[-1,::].shape for ind in sub00.vectors.index].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_corr_mat_from_ts(time_series=sub00.vectors.time_series)\n",
    "# cond_ts['Enc_ts'].shape\n",
    "difumo256_labels = pd.DataFrame(difumo256.labels).difumo_names.values\n",
    "[get_corr_mat_from_ts(sub00.vectors.time_series[:,ind,:], difumo256_labels) for ind in sub00.vectors.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfo_ts = dict(tuple((f'{outcome}_ts', sub00.vectors.loc[sub00.vectors['performance_labels'] == outcome].trial_arrays.values)\n",
    "                      for outcome in sub00.vectors.performance_labels.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_ts = dict(tuple((f'{cond}_ts', sub00.vectors.loc[sub00.vectors['condition_labels'] == cond].trial_arrays.values)\n",
    "                      for cond in sub00.vectors.condition_labels.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_ts['Enc_ts'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[type(sub00[key]) for key in ['enc_ts', 'ctl_ts', 'Miss_ts', 'Hit_ts', 'Ctl_ts', 'FA_ts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.vectors['time_series'].values.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.enc_ctl_timeseries, sub00.enc_ctl_corr_mat = \\\n",
    "    get_corr_mat(sub00.cleaned_fmri, sub00.maps_masker, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(key for key in sub00.keys() if key.endswith('_ts'))\n",
    "cmdict = dict(tuple((tskey.split('_')[0]+'_corr_mat',get_corr_mat_from_ts(sub00[tskey]))\n",
    "                    for tskey in ['Miss_ts',\n",
    "                                  'Hit_ts', 'Ctl_ts', 'FA_ts']))\n",
    "\n",
    "# sub00.vectors.stimuli_files.value_counts().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sub00.__setattr__(*attrs) for attrs in tuple(zip(['time_series', 'correlation_matrix'],\n",
    "                                          get_corr_mat(sub00)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_bunch = sklearn.utils.Bunch(**dict(('-'.join([i.sub_id,i.ses_id]), i)\n",
    "                                       for i in filter(None,cimaq_bunch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_bold55, mask55, anat_img55, events55 = \n",
    "test_subject = load_fmriprep_session(cimaq_files.bolds[55], anat_mod='T1w',\n",
    "                                     events_dir='/data/simexp/CIMAQ_AS_BIDS/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from nilearn import datasets\n",
    "atlases_dir = '../../nilearn_atlases/'\n",
    "difumo256 = datasets.fetch_atlas_difumo(256, 3, data_dir=atlases_dir)\n",
    "difumo_masker55 = NiftiMapsMasker(maps_img=difumo256.maps, standardize=False,\n",
    "                         memory='nilearn_cache', verbose=0)\n",
    "\n",
    "time_series55 = difumo_masker55.fit_transform(cleaned_bold55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series55])[0]\n",
    "\n",
    "# Display the correlation matrix\n",
    "import numpy as np\n",
    "from nilearn import plotting\n",
    "# Mask out the major diagonal\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "plotting.plot_matrix(correlation_matrix, labels=difumo256.labels.difumo_names,\n",
    "                     colorbar=True, vmax=0.8, vmin=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation_matrix.__dir__()\n",
    "mat_labels = pd.DataFrame(difumo256.labels).difumo_names.values\n",
    "corrdf = pd.DataFrame(correlation_matrix, columns=mat_labels,index=mat_labels)\n",
    "display(corrdf.max(), corrdf.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs, bbt55 = trial_fmri(cleaned_bold55, events55)\n",
    "mbbt55 = [nilearn.image.mean_img(im) for im in bbt55]\n",
    "mean_signals55 = nilearn.masking.apply_mask(mbbt55, mask55)\n",
    "fmri_bt = nilearn.image.concat_imgs(mbbt55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 1\n",
    "\n",
    "# Define the cross-validation scheme used for validation.\n",
    "# Here we use a KFold cross-validation on the session, which corresponds to\n",
    "# splitting the samples in 4 folds and make 4 runs using each fold as a test\n",
    "# set once and the others as learning sets\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=4)\n",
    "\n",
    "import nilearn.decoding\n",
    "# The radius is the one of the Searchlight sphere that will scan the volume\n",
    "searchlight = nilearn.decoding.SearchLight(\n",
    "    mask55,\n",
    "    process_mask_img=mask55,\n",
    "    radius=9.0, n_jobs=n_jobs,\n",
    "    verbose=1, cv=cv)\n",
    "searchlight.fit(fmri_bt, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "\n",
    "# For decoding, standardizing is often very important\n",
    "nifti_masker = NiftiMasker(mask_img=mask55, sessions=session,\n",
    "                           standardize=True, memory='nilearn_cache',\n",
    "                           memory_level=1)\n",
    "fmri_masked = nifti_masker.fit_transform(fmri_img)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "f_values, p_values = f_classif(fmri_masked, y)\n",
    "p_values = -np.log10(p_values)\n",
    "p_values[p_values > 10] = 10\n",
    "p_unmasked = get_data(nifti_masker.inverse_transform(p_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting as niplot\n",
    "# coords = difumo256.region_coords\n",
    "\n",
    "# We threshold to keep only the 20% of edges with the highest value\n",
    "# because the graph is very dense\n",
    "niplot.plot_connectome(correlation_matrix,\n",
    "                       edge_threshold=\"80%\", colorbar=True)\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=idxs.fillna('Ctl').replace(dict(enumerate(idxs.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(enumerate(idxs.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(dir(torch))\n",
    "torch.Tensor(mean_signals55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Equates to one random 28x28 image\n",
    "random_data = torch.rand((1, 1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(torch.nn.functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Equates to one random 28x28 image\n",
    "# random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # First 2D convolutional layer, taking in 1 input channel (image),\n",
    "        # outputting 32 convolutional features, with a square kernel size of 3\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "        # outputting 64 convolutional features, with a square kernel size of 3\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "        # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "        # with an input probability\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        # Second fully connected layer that outputs our 10 labels\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "my_nn = Net()\n",
    "print(my_nn)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = F.max_pool2d(F.relu(self.conv2(F.relu(self.conv1(x)))), 2)\n",
    "        # Use the rectified-linear activation function over x\n",
    "        # Run max pooling over x\n",
    "        # Pass data through dropout1\n",
    "        x = F.relu(self.fc1(torch.flatten(self.dropout1(x), 1)))\n",
    "        # Flatten x with start_dim=1\n",
    "        # Pass data through fc1\n",
    "        x = self.fc2(self.dropout2(x))\n",
    "        # Apply softmax to x\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# Equates to one random 28x28 image\n",
    "# random_data = torch.rand((1, 1, 28, 28))\n",
    "\n",
    "my_nn = Net()\n",
    "result = my_nn(tuple(zip(torch.Tensor(mean_signals55), idxs.values.tolist())))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n",
    "data_loader = torch.utils.data.DataLoader(yesno_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_signals(signals=time_series, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "signals=mean_signals55\n",
    "labels=idxs\n",
    "sub_svc = sklearn.svm.LinearSVC(max_iter=1000000,\n",
    "                                penalty='l2', loss='hinge',\n",
    "                                class_weight='balanced')\n",
    "with parallel_backend('threading',n_jobs=-1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        signals, labels, # x & y\n",
    "        test_size = 0.5, \n",
    "        shuffle = True,\n",
    "        stratify = labels)\n",
    "    sub_svc.fit(X_train, y_train)\n",
    "    print(sub_svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sub_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_signals(signals=mean_signals55, labels=idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_classif(X, Y,\n",
    "                   svm_kws:dict=None,\n",
    "                   train_kws:dict=None,\n",
    "                   test_size:float=0.5,\n",
    "                   **kwargs):\n",
    "    import scipy\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "    from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    svm_defs = dict(max_iter=1000000, penalty='l2',\n",
    "                    loss='hinge', class_weight='balanced')\n",
    "    if svm_kws is not None:\n",
    "        svm_defs.update(svm_kws)\n",
    "    train_defs = dict(test_size=test_size,\n",
    "                        shuffle=True, stratify=Y)\n",
    "    if train_kws is not None:\n",
    "        train_defs.update(train_kws)\n",
    "    sub_svc = sklearn.svm.LinearSVC(**svm_defs)\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X, Y,**train_defs)\n",
    "    return sub_svc.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "# A path to a T1-weighted brain .nii image:\n",
    "# t1_fn = './brain_t1_0001.nii'\n",
    "\n",
    "# Read the .nii image containing the volume with SimpleITK:\n",
    "# sitk_t1 = sitk.ReadImage(mbbt55)\n",
    "tensorlist = [sitk.GetArrayFromImage(sitk.ReadImage(sub_img))\n",
    "              for sub_img in mbbt55]\n",
    "# and access the numpy array:\n",
    "# t1 = sitk.GetArrayFromImage(sitk_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkintrvls = lambda b, e: list(starmap(pd.Interval,tuple(zip(b, e))))\n",
    "from itertools import starmap\n",
    "t_r = cleaned_bold55.header.get_zooms()[-1]\n",
    "frame_times = np.arange(cleaned_bold55.shape[-1]) * t_r\n",
    "frame_ends = pd.Series(frame_times).add(t_r).values\n",
    "frame_intervals = mkintrvls(pd.Series(frame_times).values,\n",
    "                            frame_ends)\n",
    "trial_ends = (events55.onset+abs(events55.onset -\n",
    "                                 events55.offset)+events55.isi).values\n",
    "trial_intervals = mkintrvls(events55.onset.values, trial_ends)\n",
    "valid_frame_intervals = [intv for intv in frame_intervals if\n",
    "                         intv.right<trial_intervals[-1].left]\n",
    "valid_trial_intervals = [intv for intv in trial_intervals if\n",
    "                         intv.right<frame_intervals[-1].left]\n",
    "len(valid_trial_intervals)\n",
    "# frame_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_frame_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals55 = nilearn.masking.apply_mask(mbbt55, mask55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals55.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as nimage\n",
    "nimage.load_img(nimage.load_img(cleaned_bold)).header.get_zooms()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_files.events.apply(os.path.basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bold_by_trial = list(nimage.mean_img(bbt) for bbt in bold_by_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_signal_by_trial = apply_mask(mean_bold_by_trial, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_signals(mean_signal_by_trial, labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_based_fmri = nimage.concat_imgs([nimage.mean_img(nimage.index_img(sub00.cleaned_fmri,frames))\n",
    "                                       for frames in sub00.vectors.fmri_frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(regmasker.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.glm.first_level import FirstLevelModel\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "help(make_first_level_design_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sub00.events.trial_number.astype(str).str.zfill(3)\n",
    "padnums = lambda l: [str(x).zfill(3) for x in l]\n",
    "list(map(padnums,sub00.events.groupby('trial_type')['trial_type'].groups.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sub00.events.copy(deep=True).trial_number.astype(str).str.zfill(3)+sub00.events.trial_type).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_design():\n",
    "sub00.condition_events = sub00.events.copy(deep=True)[['onset','trial_type','duration']]\n",
    "sub00.performance_events = sub00.events.copy(deep=True).drop('trial_type',axis=1).rename(\n",
    "                               {'recognition_performance':'trial_type'},\n",
    "                                axis=1)[['onset','trial_type','duration']].fillna('Ctl')\n",
    "sub00.global_events = sub00.events.copy(deep=True)\n",
    "sub00.global_events['trial_type'] = (sub00.global_events.trial_number.astype(str).str.zfill(3) +\n",
    "                                     sub00.global_events.trial_type).values\n",
    "sub00.global_events = sub00.global_events[['onset','trial_type','duration']]\n",
    "# sub00.global_events = sub00.events.copy(deep=True)[['onset','trial_type','duration']].fillna('Ctl')\n",
    "sub00.global_events['trial_type'] = sub00.events.trial_number.astype(str)+sub00.events.trial_type\n",
    "sub00.design_params = dict(frame_times=(np.arange(sub00.cleaned_fmri.shape[-1]) *\n",
    "                                        sub00.cleaned_fmri.header.get_zooms()[-1]),\n",
    "                           drift_model=None,\n",
    "                           hrf_model='spm')\n",
    "sub00.condition_design = make_first_level_design_matrix(events=sub00.condition_events,\n",
    "                                                        **sub00.design_params)\n",
    "sub00.performance_design = make_first_level_design_matrix(events=sub00.performance_events,\n",
    "                                                          **sub00.design_params)\n",
    "sub00.global_design = make_first_level_design_matrix(events=sub00.global_events,\n",
    "                                                          **sub00.design_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(nilearn.plotting.plot_design_matrix(sub00.condition_design),\n",
    "        nilearn.plotting.plot_design_matrix(sub00.performance_design),\n",
    "        nilearn.plotting.plot_design_matrix(sub00.global_design))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00['glm_params']=dict(t_r=sub00.cleaned_fmri.header.get_zooms()[-1],\n",
    "                          mask_img=sub00.full_mask_img,\n",
    "                          drift_model=sub00.design_params['drift_model'],\n",
    "                          standardize=False,\n",
    "                          smoothing_fwhm=None,\n",
    "                          signal_scaling=(0,1), # Other choices: 0, 1, False\n",
    "                          noise_model='ar1',\n",
    "                          hrf_model=sub00.design_params['hrf_model'],\n",
    "                          subject_label=sub00.sub_id,\n",
    "                          target_affine=sub00.cleaned_fmri.affine,\n",
    "                          target_shape=sub00.full_mask_img.shape,\n",
    "                          n_jobs=-1,\n",
    "                          minimize_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.condition_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fit_1stlevel_glm(fmri_img:Union[str,os.PathLike,Nifti1Image],\n",
    "                          events:pd.DataFrame,\n",
    "                          confounds:pd.DataFrame=None,\n",
    "                          **kwargs):\n",
    "#     short_events = events[['onset','trial_type','duration']].copy(deep=True)\n",
    "#     Add modulation column to events since ctl classification is 2x poorer than enc?\n",
    "#     short_events['modulation'] = [0.2 if 'Enc' in ttype else 0.9\n",
    "#                                   for ttype in short_events.trial_type]\n",
    "#     sub00.condition_events = sub00.events[['onset','trial_type','duration']]\n",
    "#     sub00.performance_events = sub00.events.rename({'recognition_performance':'trial_type'},\n",
    "#                                              axis=1)[['onset','trial_type','duration']]\n",
    "#     design_params = dict(frame_times=(np.arange(sub00.cleaned_fmri.shape[-1]) *\n",
    "#                                       fmri_img.header.get_zooms()[-1]),\n",
    "#                          events=sub00.condition_events[['onset','trial_type','duration']],\n",
    "#                          drift_model=None,\n",
    "#                          hrf_model='spm')\n",
    "\n",
    "glm_params = dict(t_r=sub00.cleaned_fmri.header.get_zooms()[-1],\n",
    "                  mask_img=sub00.full_mask_img,\n",
    "                  drift_model=design_params['drift_model'],\n",
    "                  standardize=False,\n",
    "                  smoothing_fwhm=None,\n",
    "                  signal_scaling=(0,1), # Other choices: 0, 1, False\n",
    "                  noise_model='ar1',\n",
    "                  hrf_model=design_params['hrf_model'],\n",
    "                  subject_label=sub00.sub_id,\n",
    "                  target_affine=sub00.cleaned_fmri.affine,\n",
    "                  target_shape=sub00.full_mask_img.shape,\n",
    "                  n_jobs=-1,\n",
    "                  minimize_memory=False)\n",
    "    # Signal extraction methods: \n",
    "    # compute_contrast(self, contrast_def, stat_type=None, output_type='z_score')\n",
    "    # DONE fit(self, run_imgs, events=None, confounds=None, design_matrices=None, bins=100)\n",
    "    # predicted(), r_square(), residuals\n",
    "    # fit_transform(self, X, y=None, **fit_params)\n",
    "#     design = make_first_level_design_matrix(**design_params)\n",
    "    model = FirstLevelModel(**glm_params).fit(sub00.cleaned_fmri,\n",
    "                                              design_matrices=design)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.condition_glm.design_matrices_[0].columns[:-1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.condition_contrasts = sklearn.utils.Bunch(**dict(tuple((col,sub00.condition_glm.compute_contrast(contrast_def=col,\n",
    "                                                                                 output_type='all'))\n",
    "                      for col in sub00.condition_glm.design_matrices_[0].columns[:-1].tolist())))\n",
    "sub00.performance_contrasts = sklearn.utils.Bunch(**dict(tuple((col,sub00.performance_glm.compute_contrast(contrast_def=col,\n",
    "                                                                                   output_type='all'))\n",
    "                      for col in sub00.performance_glm.design_matrices_[0].columns[:-1].tolist())))\n",
    "sub00.global_contrasts = sklearn.utils.Bunch(**dict(tuple((col,sub00.global_glm.compute_contrast(contrast_def=col,\n",
    "                                                                           output_type='all'))\n",
    "                      for col in sub00.global_glm.design_matrices_[0].columns[:-1].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# condcp = copy.deepcopy(sub00.condition_contrasts)\n",
    "# def contrast2signal(contrast)\n",
    "sub00.condition_glm.masker_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame.from_dict(condcp)\n",
    "globalcp = copy.deepcopy(sub00.global_contrasts)\n",
    "globdf = pd.DataFrame.from_dict(globalcp)\n",
    "# help(sklearn.utils.Bunch)#(**condcp).Enc.effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # [setattr(sub00, f'{attr}_glm', FirstLevelModel(**sub00.glm_params))\n",
    "# #                         for attr in ['condition','performance','global']]\n",
    "# [sub00[f'{attr}_glm'].fit(sub00.cleaned_fmri,\n",
    "#                         design_matrices=sub00[f'{attr}_design'])\n",
    "#  for attr in ['condition','performance','global']]\n",
    "fxsize_globaltest = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (trial_based_fmri)\n",
    "mapsmasker=NiftiMapsMasker(maps_img=difumo256.maps,\n",
    "                           mask_img=sub00.full_mask_img,\n",
    "                           standardize_confounds=False,\n",
    "                           resampling_target='data')\n",
    "mapsmasker.fit(sub00.cleaned_fmri,sub00.vectors.performance_labels)\n",
    "\n",
    "regmasker = NiftiMasker(mask_img=sub00.full_mask_img,\n",
    "                        mask_strategy='whole-brain-template',\n",
    "                        target_affine=sub00.cleaned_fmri.affine,\n",
    "                        t_r=sub00.cleaned_fmri.header.get_zooms()[-1],\n",
    "                        target_shape=sub00.full_mask_img.shape,\n",
    "                        standardize_confounds=False)\n",
    "# trial_based_signals = mapsmasker.transform_single_imgs(trial_based_fmri)\n",
    "regmasker.fit(sub00.cleaned_fmri,\n",
    "              sub00.events[['onset','trial_type','duration']])\n",
    "# single_signals = regmasker.transform_single_imgs(sub00.cleaned_fmri)\n",
    "# fit_trans_signals = regmasker.fit_transform(X=sub00.cleaned_fmri,\n",
    "#                                             y=sub00.events[['onset','trial_type','duration']])\n",
    "\n",
    "# 65*77*60==300300\n",
    "# help(NiftiMapsMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.condition_contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globdf.loc[f'{stat}_sig']=\n",
    "\n",
    "globsignaldf = sklearn.utils.Bunch(pd.DataFrame([[regmasker.fit_transform(signal) for signal\n",
    "                             in globdf.loc[stat].values]\n",
    "                             for stat in globdf.index.tolist()],\n",
    "                            index=[ind+'_sig' for ind in globdf.index.tolist()],\n",
    "                            columns=globdf.columns))\n",
    "# globdf.loc['effect_size'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_attempt = regmasker.fit_transform(globalcp.loc['effect_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from googlenet_pytorch import GoogLeNet \n",
    "model = GoogLeNet.from_pretrained('googlenet')\n",
    "\n",
    "# ... image preprocessing as in the classification example ...\n",
    "inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "print(inputs.shape) # torch.Size([1, 3, 224, 224])\n",
    "\n",
    "# features = model.extract_features(inputs)\n",
    "# torch.Size([1, 1024, 7, 7])\n",
    "# print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i for i in sub00.condition_contrasts]\n",
    "enc_map=sub00.condition_contrasts['Enc']['effect_size']\n",
    "ctl_map=sub00.condition_contrasts['Ctl']['effect_size']\n",
    "ctl_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tst=list(next(iter(item)) for item in )\n",
    "# tst[0].shape\n",
    "# globalcp.iloc[-2][0].shape\n",
    "os.listdir('../../WMStim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../../../fnadeau/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shelve\n",
    "# # help(shelve.Shelf)\n",
    "# with open(,'w') as savedst:\n",
    "#           pickle.dump(savedst,sub00)\n",
    "import loadutils as lu\n",
    "lu.save_pickle('../../../fnadeau/sub00_bunch.pickle',sub00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure as CM\n",
    "# X = list(next(iter(item)) for item in globalcp.iloc[-2])\n",
    "X = last_attempt.T\n",
    "# y = [col.lstrip(digits) for col in globalcp.columns]\n",
    "#\n",
    "X = X = trial_based_signals\n",
    "# y = pd.DataFrame(difumo256.labels).difumo_names.values\n",
    "\n",
    "linsvm = sklearn.svm.LinearSVC(max_iter=1000000,\n",
    "                                    penalty='l2',\n",
    "                                    loss='hinge',\n",
    "                                    class_weight='balanced')\n",
    "\n",
    "linsvm.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linsvm.__dict__\n",
    "from string import digits\n",
    "y = [col.lstrip(digits) for col in globalcp.columns]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = CM(cov_estimator=linsvm,\n",
    "                         kind='correlation')\n",
    "# correlation_measure.fit(X,y)\n",
    "correlation_matrix2 = correlation_measure.fit_transform(X,y)\n",
    "\n",
    "# # Display the correlation matrix\n",
    "# import numpy as np\n",
    "# from nilearn import plotting\n",
    "# # Mask out the major diagonal\n",
    "# np.fill_diagonal(correlation_matrix2, 0)\n",
    "# correlation_matrix2.shape\n",
    "# plotting.plot_matrix(correlation_matrix2, labels=y,\n",
    "# #                      figure=(128,128),\n",
    "#                      colorbar=True, vmax=0.8, vmin=-0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker, NiftiMapsMasker, NiftiSpheresMasker\n",
    "help(NiftiMasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdf = pd.DataFrame(correlation_matrix, index=difumo256.labels.difumo_names,\n",
    "                              columns=difumo256.labels.difumo_names)\n",
    "# corrdf.where(corrdf==corrdf.max()).dropna(how='all',axis=0)\n",
    "# [(row[1].index,row[1].name) for row in corrdf.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure as CM\n",
    "help(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difumo256.labels.difumo_names.shape\n",
    "trial_based_signals.T.shape,difumo256.labels['difumo_names'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(linsvm)\n",
    "linsvm.__dict__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60%/40% split with LinearSVC, 50-50 with NuSVC\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def classify_bmap(beta_map, labels,\n",
    "                  mask=None, n_tests=15):\n",
    "    from nilearn import image as nimage\n",
    "    from nilearn.masking import apply_mask\n",
    "# max_iter=1000000, dual=True, tol=1E-4\n",
    "    sub_svc = sklearn.svm.LinearSVC(max_iter=1000000,\n",
    "                                    penalty='l2',\n",
    "                                    loss='hinge',\n",
    "                                    class_weight='balanced',\n",
    "#                                     fit_intercept=True,\n",
    "#                                     intercept_scaling=1.0\n",
    "#                                     probability=True,\n",
    "                                ) # 0.73 avg acc up to now\n",
    "    scores = []\n",
    "    for test in range(n_tests):\n",
    "        X_enc_ctl = beta_map\n",
    "#         X_enc_ctl = apply_mask(list(nimage.iter_img(beta_map)), mask)\n",
    "#         X_enc_ctl = [[np.float(s) for s in scipy.fft.fft(sig)]\n",
    "#                      for sig in X_enc_ctl]\n",
    "        y_enc_ctl = labels\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_enc_ctl, # x\n",
    "            y_enc_ctl, # y\n",
    "            test_size = 0.4, \n",
    "            shuffle = True, # shuffle dataset before splitting\n",
    "            stratify = y_enc_ctl, # keep distribution of conditions consistent betw. train & test sets\n",
    "            #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "            )\n",
    "        # Test model on unseen data from the test set\n",
    "        sub_svc.fit(X_train, y_train)\n",
    "        y_pred = sub_svc.predict(X_test) # classify age class using testing data\n",
    "        print(len(list(filter(None, y_pred==y_test)))/len(y_test))\n",
    "        print(y_pred)\n",
    "        scores.append(sub_svc.score(X_test, y_test)) # get accuracy\n",
    "    print(np.mean(scores).round(2))\n",
    "#     cr = classification_report(y_pred=y_pred, y_true=y_test).precision # get prec., recall & f1\n",
    "#     crdf = pd.DataFrame(tuple(filter(None,[line.split()[:-2]\n",
    "#                                            for line in cr.splitlines()]))).T.set_index(0).T\n",
    "#     precision_df.append(crdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_bmap(trial_based_signals.T,labels=sub00.vectors.performance_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.keras.applications.vgg16.VGG16(\n",
    "#     include_top=True, weights='imagenet',\n",
    "#     input_tensor=None,\n",
    "#     input_shape=None, pooling=None, classes=1000,\n",
    "#     classifier_activation='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data into memory\n",
    "data = load_data(all_filenames, tf.estimator.ModeKeys.TRAIN, reader_params)\n",
    "\n",
    "# Create placeholder variables and define their shapes (here, \n",
    "# we input a volume image of size [128, 224, 244] and a single\n",
    "# channel (i.e. greyscale):\n",
    "x = tf.placeholder(reader_example_dtypes['features']['x'], \n",
    "                   [None, 128, 224, 224, 1])\n",
    "y = tf.placeholder(reader_example_dtypes['labels']['y'], \n",
    "                   [None, 1])\n",
    "\n",
    "# Create a tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "dataset = dataset.repeat(None)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Create an iterator\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "nx = iterator.get_next()\n",
    "\n",
    "with tf.train.MonitoredTrainingSession() as sess_dict:\n",
    "    \n",
    "    sess_dict.run(iterator.initializer, \n",
    "               feed_dict={x: data['features'], y: data['labels']})\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Get next features/labels pair\n",
    "        dict_batch_feat, dict_batch_lbl = sess_dict.run(nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input, VGG16\n",
    "model = VGG16(include_top=True, weights='imagenet',\n",
    "              input_tensor=None,\n",
    "              input_shape=None, pooling=None, classes=1000,\n",
    "              classifier_activation='softmax')\n",
    "\n",
    "def make_tf_inputs(inpt_list:list)->list:\n",
    "    from keras.preprocessing.image import img_to_array, load_img\n",
    "# load an image from file\n",
    "    images = [img_to_array(load_img(fpath, target_size=(224, 224)))\n",
    "              for fpath in inpt_list]\n",
    "    images = [preprocess_input(image.reshape((1, *image.shape[:2])))\n",
    "              for image in images]\n",
    "\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "# convert the image pixels to a numpy array\n",
    "image = img_to_array(image)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(os.listdir(os.path.dirname(fmriprep_dir)+'/sub-3002498/'))\n",
    "# Path(cimaq_files.bolds[0]).parts[-4:-2]\n",
    "tpath=os.path.join(cimaq_files.bolds[0].split('fmriprep')[0],\n",
    "                                     *Path(cimaq_files.bolds[0]).parts[-4:-2])\n",
    "os.listdir(tpath)\n",
    "events_path = glob.glob(os.path.join(events_dir,\n",
    "                                     *Path(cimaq_files.bolds[0]).parts[-4:-2],\n",
    "                            '*events.tsv'))[0]\n",
    "events_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load confound parameters outputed from the NIAK preprocessing pipeline (slow signal drift, motion parameters, mean white matter and mean ventricle signal intensity)\n",
    "\n",
    "**Note: The full NIAK preprocessing pipeline scrubs (regresses out) motion outlier frames, which may not be compatible with other software like Nilearn or Nistats.\n",
    "Intermediate data from the preprocessing pipeline should be used that have been slice-timed, co-registered and resampled (motion-corrected).  \n",
    "\n",
    "These data are found under the **resample** directory (not under fMRI), in .nii format, with accompanying _extra.mat and confounds.tsv.gz files. \n",
    "These data have undergone **no smoothing**, and confounds have NOT been regressed out. \n",
    "\n",
    "Use gunzip *gz command to unzip .tsv files inside working directory\n",
    "\n",
    "**Update: the nistats first-level model can model some of the slow drift and noise parameters (not used here). Here, I use all the confounds in *counfounds.tsv (including slow drift) as regressors, so I don't model slow drift, etc in the first-level model (it's redundant). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/'\n",
    "# events_dir = '/data/simexp/CIMAQ_AS_BIDS/'\n",
    "# atlases_dir = '../../nilearn_atlases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: create events variable, and events.tsv file from the 'sub-*_ses-4_task-memory_events.tsv' task file outputed by the cimaq_convert_eprime_to_bids_event.py script \n",
    "\n",
    "Number of rows = number of trials (first-level model uses onset times to match trial conditions to fMRI frames)\n",
    "\n",
    "Documentation:\n",
    "https://nistats.github.io/auto_examples/04_low_level_functions/write_events_file.html#sphx-glr-auto-examples-04-low-level-functions-write-events-file-py\n",
    "\n",
    "Each encoding trial is modelled as a different condition (under trial_type column) so that it is modelled separately when creating the design matrix. The trial has its own column in the design matrix; the other columns = other trials (modelled together as single regressor), and confound regressors.\n",
    "\n",
    "**Note: Some scans were cut short, meaning that the last few trials do not have associated brain activation frames, and they need to be left out of the analysis; 310 frames = full scan, 288 frames = incomplete (~15 participants).\n",
    "\"unscanned\" trials need to be excluded from the model (about ~2-4 trials missing).\n",
    "288*2.5 = 720s. \n",
    "Trial #115 (out of 117) offset time ~ 710s\n",
    "Trial #116 (out of 117) onset ~ 723s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unique trial identifiers & clean fMRI BOLD signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reduced_sig = pd.DataFrame([[np.mean(chunk) for chunk in chunks(row[1].values, 27)]\n",
    "                            for row in signals.iterrows()], dtype=np.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctl_idx = events00.iloc[:-1,:].loc[events00.condition=='Ctl'].index.tolist()\n",
    "enc_idx = events00.iloc[:-1,:].loc[events00.condition=='Enc'].index.tolist()\n",
    "X_ctl = apply_mask(list(nimage.iter_img(nimage.index_img(betamap00, ctl_idx))), vVS_mask)\n",
    "X_enc = apply_mask(list(nimage.iter_img(nimage.index_img(betamap00, enc_idx))), vVS_mask)\n",
    "# betamap00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "difumo256 = datasets.fetch_atlas_difumo(256, 3, data_dir=atlases_dir)\n",
    "difumo_sorted = pd.DataFrame(difumo256.labels).iloc[:, 1:]\n",
    "type(difumo256)\n",
    "# yeo7_vis=difumo_sorted.loc[difumo_sorted.yeo_networks7=='VisCent'].difumo_names.index.tolist()\n",
    "\n",
    "# # Lateral-Occipital Complex (LOC) - Not yet\n",
    "# # Occipital Visual only\n",
    "# vis_patt = re.compile('\\.*occipital\\.*|\\.*visual\\.*', re.I)\n",
    "# vis_components = difumo_sorted.difumo_names.str.contains(vis_patt)\n",
    "# vis_areas = vis_components.replace(False, pd.NA).dropna(how='all').index.tolist()\n",
    "# vVS_mask = format_difumo([84],target_img=mask00,data_dir=atlases_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nilearn.input_data import NiftiMapsMasker\n",
    "# # help(NiftiMapsMasker)\n",
    "# maps_masker = NiftiMapsMasker(maps_img=nimage.index_img(nimage.load_img(difumo256.maps), [84]),\n",
    "#                               standardize=False)\n",
    "# time_series = maps_masker.fit_transform(mem_betamap, confounds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [:-1] to remove ``constant`` \n",
    "# glm_betamap = nimage.concat_imgs(tuple(glm_model.compute_contrast(cont, output_type='effect_size')\n",
    "#                                        for cont in tqdm(glm_model.design_matrices_[0].columns[:-1].tolist(),\n",
    "#                                                         desc='computing contrasts')))\n",
    "betamap00 = nimage.concat_imgs(tuple(model00.compute_contrast(cont, output_type='effect_size')\n",
    "                                     for cont in tqdm(design00.columns[:-1].tolist(),\n",
    "                                                    desc='computing contrasts')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_trials = events00.loc[events00.recognition_performance.notna()]\n",
    "mem_indx = memory_trials.index.tolist()\n",
    "mem_acc_labels = memory_trials.recognition_performance.values.tolist()[:-1]\n",
    "mem_betamap = nimage.index_img(betamap, mem_indx[:-1])\n",
    "mem_betamap.shape, len(mem_acc_labels)\n",
    "enc_ctl_labels = [re.match('\\w{3}',trial).group() for trial\n",
    "                  in events00.condition.tolist()]\n",
    "# events00.shape, betamap_str.shape\n",
    "set(mem_acc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolmask=nimage.new_img_like(ref_niimg=mask00,\n",
    "#                                     data=v_roi_data)\n",
    "visual_masker = nilearn.input_data.NiftiMasker(mask_img=visual_mask,\n",
    "                                               mask_strategy='whole-brain-template')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_ctl = dict(tuple((cond,events00.condition.eq(cond).astype(int).values.tolist()+[0])\n",
    "#                      for cond in events00.condition.unique()))\n",
    "# len(enc_ctl['Ctl'])\n",
    "# enc_ctl_betamap = model00.compute_contrast(enc_ctl['Enc'], output_type='effect_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mem_betamap.shape, maps_masker.fit_transform(mem_betamap).shape, len(mem_acc_labels)\n",
    "nilearn.image.concat_imgs(mbbt55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_bmap(nilearn.image.concat_imgs(mbbt55), mask55, labels_list=idxs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_svc = sklearn.svm.NuSVC() # 0.68 avg acc up to now\n",
    "# sub_svc = sklearn.linear_model.SGDClassifier()\n",
    "# \n",
    "# Cross-validation within 10 folds of training set\n",
    "# predict\n",
    "# class_weight='balanced'\n",
    "cv_y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                              groups=y_train, cv=10)\n",
    "# scores\n",
    "cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                         groups=y_train, cv=10)\n",
    "print(f'Cross-validation accuracy: {cv_acc}')\n",
    "\n",
    "# evaluate overall model performance on training data\n",
    "overall_acc = accuracy_score(y_pred = cv_y_pred, y_true = y_train)\n",
    "overall_cr = classification_report(y_pred = cv_y_pred, y_true = y_train)\n",
    "print(f'Accuracy\\n{round(overall_acc, 3)}\\n\\nOverall Score\\n{overall_cr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "pd.read_fwf(StringIO('\\n'.join(list(filter(None,overall_cr.splitlines()[1:])))),\n",
    "            sep='\\\\s+', header=None).set_axis(['Result','precision','recall',\n",
    "                                               'f1-score','support'],\n",
    "                                              axis=1).set_index('Result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_betas = list(nimage.iter_img(betamap))[:-1]\n",
    "# decoder = nilearn.decoding.FREMClassifier(standardize=False,\n",
    "#                                           mask=mask00,\n",
    "#                                           target_affine=cleaned_bold.affine,\n",
    "#                                           target_shape=cleaned_bold.shape[:-1],\n",
    "#                                           clustering_percentile = 20,\n",
    "#                                           screening_percentile = 100,\n",
    "#                                           mask_strategy='whole-brain-template')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     trial_betas,\n",
    "# #     apply_mask(trial_betas, mask00), # x\n",
    "#     events00.condition, # y\n",
    "#     test_size = 0.4, # 60%/40% split\n",
    "#     shuffle = True, # shuffle dataset before splitting\n",
    "#     stratify = events00.condition, # keep distribution of conditions consistent betw. train & test sets\n",
    "#     #random_state = 123  # if set number, same shuffle each time, otherwise randomization algo\n",
    "#     ) \n",
    "# decoder.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = decoder.predict(X_test)\n",
    "# help(decoder.score)\n",
    "# acc = decoder.score(X_test, y_test)\n",
    "decoder_cr = classification_report(y_pred=y_pred, y_true=y_test) # get prec., recall & f1\n",
    "# print results\n",
    "print(decoder_cr)\n",
    "# print(f'Accuracy\\n{acc}\\n{cr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model00.generate_report(contrasts=cdef,\n",
    "#                         title=None, bg_img='MNI152TEMPLATE',\n",
    "#                         threshold=3.09, alpha=0.001,\n",
    "#                         cluster_threshold=0, height_control='fpr',\n",
    "#                         min_distance=8.0, plot_type='slice', display_mode='ortho',\n",
    "#                         report_dims=(1600, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_def = dict(tuple((cond,events00.condition.str.contains(cond).astype(int).values)\n",
    "#                       for cond in events00.condition.unique().tolist()))\n",
    "# cont_def['Enc-Ctl'] = cont_def['Enc']-cont_def['Ctl']\n",
    "# cont_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.decoding import Decoder\n",
    "# Here screening_percentile is set to 5 percent\n",
    "# mask_img = haxby_dataset.mask\n",
    "trial_betamap = nimage.concat_imgs(tuple(nimage.iter_img(betamap))[:-1])\n",
    "decoder = Decoder(estimator='svc_l1', mask=mask00, smoothing_fwhm=None,\n",
    "                  clustering_percentile = 20,\n",
    "                  standardize=False, screening_percentile=5, scoring='accuracy')\n",
    "decoder.fit(trial_betamap,\n",
    "            events00.condition)\n",
    "y_pred = decoder.predict(trial_betamap)\n",
    "\n",
    "# Print the CV scores\n",
    "print(decoder.cv_scores_['Enc'], decoder.cv_scores_['Ctl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_img = decoder.coef_img_['Enc']\n",
    "view_img(weight_img,\n",
    "         title=\"SVM weights\")\n",
    "#          dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # get map of coefficients    \n",
    "    # coef_ = sub_svc.coef_\n",
    "    # print(coef_.shape)\n",
    "    #Return voxel weights into a nifti image using the NiftiMasker\n",
    "    # coef_img = sub_masker.inverse_transform(coef_)\n",
    "    #Save .nii to file\n",
    "    # coef_img.to_filename(os.path.join(output_dir, 'Coef_maps', 'SVC_coeff_enc_ctl_sub-'+str(sub)+'.nii'))\n",
    "\n",
    "enc_ctl_data = enc_ctl_data.append(pd.Series(s_data, index=enc_ctl_data.columns), ignore_index=True)\n",
    "\n",
    "demo_data = sub_data.copy()\n",
    "demo_data.reset_index(level=None, drop=False, inplace=True)\n",
    "\n",
    "enc_ctl_data.insert(loc = 1, column = 'cognitive_status', value = demo_data['cognitive_status'],\n",
    "                    allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 2, column = 'total_scrubbed_frames', value = demo_data['total_scrubbed_frames'],\n",
    "                    allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 3, column = 'mean_FD', value = demo_data['mean_FD'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 4, column = 'hits', value = demo_data['hits'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 5, column = 'miss', value = demo_data['miss'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 6, column = 'correct_source', value = demo_data['correct_source'],\n",
    "                    allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 7, column = 'wrong_source', value = demo_data['wrong_source'],\n",
    "                    allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 8, column = 'dprime', value = demo_data['dprime'], allow_duplicates=True)\n",
    "enc_ctl_data.insert(loc = 9, column = 'associative_memScore', value = demo_data['associative_memScore'],\n",
    "                    allow_duplicates=True)    \n",
    "    \n",
    "enc_ctl_data.to_csv(os.path.join(output_dir, 'SVC_withinSub_enc_ctl_wholeBrain.tsv'),\n",
    "    sep='\\t', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s_task = pd.read_csv(subject_data.events, sep='\\t')\n",
    "#rename \"trial_type\" column as \"condition\"\n",
    "s_task.rename(columns={'trial_type':'condition'}, inplace=True)\n",
    "confounds = Minimal().load(subject_data.bolds)\n",
    "scanDur = confounds.shape[0]*2.5\n",
    "#add columns to DataFrame\n",
    "numCol = s_task.shape[1] \n",
    "insertCol = [4, numCol+1, numCol+2, numCol+3]\n",
    "colNames = ['trial_type', 'unscanned', 'ctl_miss_hit', 'ctl_miss_ws_cs']\n",
    "colVals = ['TBD', 0, 'TBD', 'TBD']\n",
    "for i in range(0, len(colNames)):\n",
    "    s_task.insert(loc=insertCol[i], column=colNames[i], value=colVals[i], allow_duplicates=True)\n",
    "\n",
    "#The 'unscanned' column flag trials for which no brain data was acquired.\n",
    "#The scan's duration is shorter than the trial's offset time\n",
    "#(0 = data, 1 = no data)\n",
    "for j in s_task[s_task['offset']>scanDur].index:\n",
    "    s_task.loc[j, 'unscanned']=1\n",
    "\n",
    "#pas trial numbers with zeros (on the left) to preserve trial \n",
    "#temporal order when trials are alphabetized\n",
    "s_task['trial_number'] = s_task['trial_number'].astype('object',\n",
    "                                                       copy=False)\n",
    "for k in s_task.index:\n",
    "    s_task.loc[k, 'trial_number'] = str(s_task.loc[k, 'trial_number']).zfill(3)\n",
    "\n",
    "#trial_type should have a unique entry per row so that each trial \n",
    "#can be modelled as a separate condition\n",
    "countEnc = 0\n",
    "countCTL = 0\n",
    "for m in s_task[s_task['condition']=='Enc'].index:\n",
    "    countEnc = countEnc + 1\n",
    "    s_task.loc[m, 'trial_type'] = 'Enc'+str(countEnc)\n",
    "    if s_task.loc[m, 'position_accuracy'] == 0:\n",
    "        s_task.loc[m, 'ctl_miss_hit']='missed'\n",
    "        s_task.loc[m, 'ctl_miss_ws_cs']='missed'\n",
    "    elif s_task.loc[m, 'position_accuracy'] == 1:\n",
    "        s_task.loc[m, 'ctl_miss_hit']='hit'\n",
    "        s_task.loc[m, 'ctl_miss_ws_cs']='wrongsource'\n",
    "    elif s_task.loc[m, 'position_accuracy'] == 2:\n",
    "        s_task.loc[m, 'ctl_miss_hit']='hit'\n",
    "        s_task.loc[m, 'ctl_miss_ws_cs']='correctsource' \n",
    "for n in s_task[s_task['condition']=='CTL'].index:\n",
    "    countCTL = countCTL + 1\n",
    "    s_task.loc[n, 'trial_type'] = 'CTL'+str(countCTL)\n",
    "    s_task.loc[n, 'ctl_miss_hit']='control'\n",
    "    s_task.loc[n, 'ctl_miss_ws_cs']='control'\n",
    "\n",
    "#78 encoding and 39 control trials if full scan\n",
    "print('Number of encoding trials:  ', countEnc)\n",
    "print('Number of control trials:  ', countCTL)\n",
    "    \n",
    "#keep only trials for which fMRI data was collected\n",
    "s_task = s_task[s_task['unscanned']==0]\n",
    "\n",
    "#Save vectors of trial labels (e.g., encoding vs control)\n",
    "#to label trials for classification analyses\n",
    "ttypes1 = s_task['condition']\n",
    "# ttypes1.to_csv(outTask_dir+'/sub-'+id+'_enco_ctl.tsv',\n",
    "#               sep='\\t', header=True, index=False)\n",
    "\n",
    "ttypes2 = s_task['ctl_miss_hit']\n",
    "# ttypes2.to_csv(outTask_dir+'/sub-'+id+'_ctl_miss_hit.tsv',\n",
    "#               sep='\\t', header=True, index=False)\n",
    "\n",
    "ttypes3 = s_task['ctl_miss_ws_cs']\n",
    "# ttypes3.to_csv(outTask_dir+'/sub-'+id+'_ctl_miss_ws_cs.tsv',\n",
    "#               sep='\\t', header=True, index=False)\n",
    "\n",
    "\n",
    "#from s_task dataframe, create an events dataframe to create a design matrix \n",
    "#that will be inputed into a first-level model in nistats\n",
    "ev_cols = ['onset', 'duration', 'trial_type', 'condition', 'ctl_miss_hit', \n",
    "           'ctl_miss_ws_cs', 'trial_number']\n",
    "all_events = s_task[ev_cols]  \n",
    "\n",
    "print(all_events.shape)\n",
    "print(all_events.iloc[0:30, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = dict(tuple((col, np.array(list(map(int,col==design_matrix0.columns))))\n",
    "   for col in ['Enc','Ctl', 'constant']))\n",
    "enc_minus_ctl = conditions['Enc'] - conditions['Ctl']\n",
    "enc_minus_const = conditions['Enc'] - conditions['constant']\n",
    "ctl_minus_const =  conditions['Ctl'] - conditions['constant']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step3 : Implement first-level model (implements regression in nistats), generate contrasts and output maps of beta values (parameter estimators; one map of betas per trial).\n",
    "\n",
    "**About first-level model:\n",
    "\n",
    "Note 1: nistats.first_level_model provides an interface to use the glm implemented in nistats.regression\n",
    "\n",
    "Note 2: each encoding trial is modelled as a separate condition to obtain separate maps of beta values (model's output type to get betas = **effect sizes** (Nistats name), aka parameter estimates)\n",
    "\n",
    "**UPDATE: a separate model is created to obtain a beta map for each trial. \n",
    "The trial of interest is modelled as a unique condition (it has its own column in the design matrix), and the other trials are modelled together.\n",
    "Version A: beside the trial of interest (1 regressor), the control trials and encoding trials are modelled separately (2 regressors)\n",
    "Version B: beside the trial of interest (1 regressor), the control trials and encoding trials are modelled together as a single \"other_trials\" condition (1 regressor)\n",
    "\n",
    "Note 3: the first_level_model can either be given a pre-constructed **design matrix** (built from the events and confounds files in a separate preparatory step) as a parameter, or be given the **events and confounds** files directly as parameters (skipping the need to create a design matrix in a separate step). In the second situation, the model will generate and output the design matrix.  \n",
    "\n",
    "As a parameter to the first-level-model, the design matrix takes precedence over the events and confounds parameters.\n",
    "\n",
    "Nistats link on design matrices:\n",
    "https://nistats.github.io/auto_examples/04_low_level_functions/plot_design_matrix.html\n",
    "\n",
    "https://nistats.github.io/modules/generated/nistats.design_matrix.make_first_level_design_matrix.html\n",
    "\n",
    "Example where the design matrix is created directly within the first level model interface by inputing events and confounds files directly as parameters (I'm not doing that here): https://nistats.github.io/auto_examples/01_tutorials/plot_first_level_model_details.html#running-a-basic-model\n",
    "\n",
    "CHOSEN: 2-step method. Create design matrix directly from events file and confounds outputed from NIAK (resample directory), then generate and fit first-level-model\n",
    "\n",
    "**About contrasts and maps of beta values (parameter estimators):\n",
    "\n",
    "When each trial is modeled as a separate condition, it is represented as its own column in the design matrix. \n",
    "\n",
    "To access the estimated coefficients (betas of the GLM model), we need to specify \"canonical contrasts\" (one per trial) that isolate design matrix columns (each contrast has a single 1 in its corresponding colum, and 0s for all the other columns).\n",
    "\n",
    "Sources of info: how to extract beta maps in nistats (instructions are a bit obscure)\n",
    "https://15-35545854-gh.circle-artifacts.com/0/home/ubuntu/nistats/doc/_build/html/auto_examples/plot_spm_auditory.html\n",
    "https://github.com/poldracklab/fitlins/pull/48\n",
    "https://nistats.github.io/modules/generated/nistats.first_level_model.FirstLevelModel.html#nistats.first_level_model.FirstLevelModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version A: A separate model created for each trial.\n",
    "Trial of interest modelled as its own condition, and other trials are modelled as two conditions: control, and encoding (excluding trial of interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Apply 8mm smoothing since using unsmoothed preprocessed data \n",
    "#taken from NIAK's \"resample\" output directory\n",
    "fmri_imgNS = load_img(subject_data.bolds)\n",
    "fmri_img = image.smooth_img(fmri_imgNS, 8)\n",
    "\n",
    "#Define parameters related to image acquisition\n",
    "# time in seconds & number of frames\n",
    "# Frame times for each epi scan\n",
    "# alternatives to 'spm': 'glover', or 'spm + derivative'\n",
    "\n",
    "# hrf_model, numTrials = 'spm', \n",
    "\n",
    "# This concatenation step might become obsolete, test Nilearn concatenation based on padded numbers!!\n",
    "# Compile list of beta maps and their trial number, to be sorted by order of trial\n",
    "all_betas_filelist_A = []\n",
    "\n",
    "#Create a design matrix, first level model and beta map for each encoding and control trial \n",
    "# 117 trials if full scan (no missing frames)\n",
    "for i in tqdm(list(range(all_events.shape[0]))):\n",
    "\n",
    "    #copy all_events dataframe to keep the original intact\n",
    "    events = all_events.copy(deep = True)\n",
    "    #Determine trial number and condition (encoding or control)\n",
    "    tnum, tname = events.iloc[i, 6], events.iloc[i, 2]\n",
    "    #Version A: (2 conditions modelled separately)\n",
    "    #modify trial_type column to model only the trial of interest\n",
    "    events['trial_type'] = ['X_'+row[1].condition if row[1].trial_number == tnum\n",
    "                            else row[1].condition for row in events.iterrows()]\n",
    "    #verify: what determines the order of columns in design matrix?    \n",
    "    #remove unecessary columns\n",
    "    events = events[['onset', 'duration', 'trial_type']]\n",
    "    #create design matrix\n",
    "    design = make_first_level_design_matrix(**design_params)    \n",
    "    #create & fit model - Should data be standardized?\n",
    "    trial_model = FirstLevelModel(**glm_params).fit(fmri_img,\n",
    "                                                    design_matrices=design)\n",
    "    design_matrix = trial_model.design_matrices_[0]\n",
    "\n",
    "    #sanity check: print design matrices and corresponding parameter labels\n",
    "    #Contrast vector: 1 in design matrix column that corresponds to trial of interest, 0s elsewhere\n",
    "    contrast_vec = np.repeat(0, design_matrix.shape[1])\n",
    "    contrast_vec[0] = 1\n",
    "    print(contrast_vec)\n",
    "#     break\n",
    "    #compute contrast's beta maps with ``model.compute_contrast``\n",
    "    #https://nistats.github.io/modules/generated/nistats.first_level_model.FirstLevelModel.html\n",
    "    b_map = trial_model.compute_contrast(contrast_vec,\n",
    "                                         output_type='effect_size') #\"effect_size\" for betas\n",
    "    all_betas_filelist_A.append(b_map)\n",
    "#     b_name = os.path.join(outBeta_dir_A, 'betas_sub'+str(id)+'_Trial'+str(tnum)+'_'+tname+'.nii')\n",
    "#     #export b_map .nii image in output directory\n",
    "#     nibabel.save(b_map, b_name)\n",
    "#     print(os.path.basename(b_name))\n",
    "#     all_betas_filelist_A.append(b_name)\n",
    "\n",
    "    \n",
    "alltrials_betas_A = nibabel.funcs.concat_images(images=all_betas_filelist_A, check_affines=True, axis=None)\n",
    "# print(alltrials_betas_A.shape)\n",
    "# nibabel.save(alltrials_betas_A, os.path.join(outBeta_dir_A, 'concat_all_betas_sub'+str(id)+'.nii'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_img(mean_img(alltrials_betas_A),bg_img=anat0, threshold='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version B: A separate model created for each trial.\n",
    "Trial of interest modelled as its own condition, and other trials (encoding and control, excluding trial of interest) are modelled as a single condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outBeta_dir_B = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Betas/122922/OneModelPerTrial_B'\n",
    "all_betas_filelist_B = []\n",
    "####\n",
    "\n",
    "#Create a design matrix, first level model and beta map for each encoding and control trial \n",
    "for i in range (0, numTrials):\n",
    "\n",
    "    #copy all_events dataframe to keep the original intact\n",
    "    events = all_events.copy(deep = True)\n",
    "\n",
    "    #Determine trial number and condition (encoding or control)\n",
    "    tnum = events.iloc[i, 6]\n",
    "    currentCondi = events.iloc[i, 3]\n",
    "    tname = events.iloc[i, 2]\n",
    "        \n",
    "    #Version A: (2 conditions modelled separately)\n",
    "    #modify trial_type column to model only the trial of interest \n",
    "    for j in events.index:\n",
    "        if events.loc[j, 'trial_number'] != tnum:\n",
    "            events.loc[j, 'trial_type']= 'X_otherCondi'\n",
    "            #X for condition to remain in alphabetical order: trial of interest, X_CTL, X_Enc\n",
    "    #verify: what determines the order of columns in design matrix?    \n",
    "\n",
    "    #remove unecessary columns    \n",
    "    cols = ['onset', 'duration', 'trial_type']\n",
    "    events = events[cols]\n",
    "    \n",
    "    #create the model\n",
    "    s_model = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                               hrf_model = hrf_model)    \n",
    "    #Should data be standardized?\n",
    "\n",
    "    #create the design matrices\n",
    "    design = make_first_level_design_matrix(frame_times,\n",
    "                                            events=events,\n",
    "                                            drift_model=None,\n",
    "#                                             add_regs=confounds, \n",
    "                                            hrf_model=hrf_model)\n",
    "    \n",
    "    #fit model with design matrix\n",
    "    s_model = s_model.fit(fmri_img, design_matrices = design)\n",
    "    \n",
    "    design_matrix = s_model.design_matrices_[0]\n",
    "    \n",
    "    #sanity check: print design matrices and corresponding parameter labels\n",
    "    #plot outputed design matrix for visualization\n",
    "    print(str(tnum), ' ', tname, ' ', design_matrix.columns[0:3])\n",
    "    plot_design_matrix(design_matrix)\n",
    "    plt.show()\n",
    "\n",
    "    #Contrast vector: 1 in design matrix column that corresponds to trial of interest, 0s elsewhere\n",
    "    contrast_vec = np.repeat(0, design_matrix.shape[1])\n",
    "    contrast_vec[0] = 1\n",
    "\n",
    "    #compute the contrast's beta maps with the model.compute_contrast() method,\n",
    "    #based on contrast provided. \n",
    "    #https://nistats.github.io/modules/generated/nistats.first_level_model.FirstLevelModel.html\n",
    "    b_map = s_model.compute_contrast(contrast_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "    b_name = os.path.join(outBeta_dir_B, 'betas_sub'+str(id)+'_Trial'+str(tnum)+'_'+tname+'.nii')\n",
    "    #export b_map .nii image in output directory\n",
    "    nibabel.save(b_map, b_name)\n",
    "    print(os.path.basename(b_name))\n",
    "    all_betas_filelist_B.append(b_name)\n",
    "    \n",
    "alltrials_betas_B = nibabel.funcs.concat_images(images=all_betas_filelist_B, check_affines=True, axis=None)\n",
    "print(alltrials_betas_B.shape)\n",
    "nibabel.save(alltrials_betas_B, os.path.join(outBeta_dir_B, 'concat_all_betas_sub'+str(id)+'.nii'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outBeta_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/Nistats/Betas/122922/Conditions_Contrasts'\n",
    "\n",
    "#Model 1: encoding vs control conditions\n",
    "events1 = all_events.copy(deep = True)\n",
    "cols = ['onset', 'duration', 'condition']\n",
    "events1 = events1[cols]\n",
    "events1.rename(columns={'condition':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events1.head())\n",
    "\n",
    "#create the model\n",
    "model1 = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                         hrf_model = hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design1 = make_first_level_design_matrix(frame_times, events=events1,\n",
    "                                        drift_model=None, add_regs=confounds, \n",
    "                                        hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model1 = model1.fit(fmri_img, design_matrices = design1)    \n",
    "\n",
    "design_matrix1 = model1.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix1)\n",
    "plt.show()\n",
    "print(design_matrix1.columns[0:5])\n",
    "\n",
    "#Condition order: control, encoding (alphabetical)\n",
    "\n",
    "#contrast 1.1: control condition\n",
    "ctl_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "ctl_vec[0] = 1\n",
    "b11_map = model1.compute_contrast(ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b11_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ctl.nii')\n",
    "nibabel.save(b11_map, b11_name)\n",
    "print(os.path.basename(b11_name))\n",
    "print(ctl_vec)\n",
    "\n",
    "#contrast 1.2: encoding condition\n",
    "enc_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "enc_vec[1] = 1\n",
    "b12_map = model1.compute_contrast(enc_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b12_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_enc.nii')\n",
    "nibabel.save(b12_map, b12_name)\n",
    "print(os.path.basename(b12_name))\n",
    "print(enc_vec)\n",
    "\n",
    "#contrast 1.3: encoding minus control \n",
    "encMinCtl_vec = np.repeat(0, design_matrix1.shape[1])\n",
    "encMinCtl_vec[1] = 1\n",
    "encMinCtl_vec[0] = -1\n",
    "b13_map = model1.compute_contrast(encMinCtl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b13_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_enc_minus_ctl.nii')\n",
    "nibabel.save(b13_map, b13_name)\n",
    "print(os.path.basename(b13_name))\n",
    "print(encMinCtl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: missed vs hit encoding trials\n",
    "events2 = all_events.copy(deep = True)\n",
    "cols2 = ['onset', 'duration', 'ctl_miss_hit']\n",
    "events2 = events2[cols2]\n",
    "events2.rename(columns={'ctl_miss_hit':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events2.iloc[0:15, :])\n",
    "\n",
    "#create the model\n",
    "model2 = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                         hrf_model = hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design2 = make_first_level_design_matrix(frame_times, events=events2,\n",
    "                                        drift_model=None, add_regs=confounds, \n",
    "                                        hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model2 = model2.fit(fmri_img, design_matrices = design2)    \n",
    "\n",
    "design_matrix2 = model2.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix2)\n",
    "plt.show()\n",
    "print(design_matrix2.columns[0:5])\n",
    "\n",
    "##Condition order: control, hit, missed (alphabetical)\n",
    "\n",
    "#contrast 2.1: miss \n",
    "miss_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "miss_vec[2] = 1\n",
    "b21_map = model2.compute_contrast(miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b21_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_miss.nii')\n",
    "nibabel.save(b21_map, b21_name)\n",
    "print(os.path.basename(b21_name))\n",
    "print(miss_vec)\n",
    "\n",
    "#contrast 2.2: hit \n",
    "hit_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_vec[1] = 1\n",
    "b22_map = model2.compute_contrast(hit_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b22_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit.nii')\n",
    "nibabel.save(b22_map, b22_name)\n",
    "print(os.path.basename(b22_name))\n",
    "print(hit_vec)\n",
    "\n",
    "#contrast 2.3: hit minus miss\n",
    "hit_min_miss_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_min_miss_vec[1] = 1\n",
    "hit_min_miss_vec[2] = -1\n",
    "b23_map = model2.compute_contrast(hit_min_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b23_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit_minus_miss.nii')\n",
    "nibabel.save(b23_map, b23_name)\n",
    "print(os.path.basename(b23_name))\n",
    "print(hit_min_miss_vec)\n",
    "\n",
    "#contrast 2.4: hit minus control\n",
    "hit_min_ctl_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "hit_min_ctl_vec[1] = 1\n",
    "hit_min_ctl_vec[0] = -1\n",
    "b24_map = model2.compute_contrast(hit_min_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b24_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_hit_minus_ctl.nii')\n",
    "nibabel.save(b24_map, b24_name)\n",
    "print(os.path.basename(b24_name))\n",
    "print(hit_min_ctl_vec)\n",
    "\n",
    "#contrast 2.5: miss minus control \n",
    "miss_min_ctl_vec = np.repeat(0, design_matrix2.shape[1])\n",
    "miss_min_ctl_vec[2] = 1\n",
    "miss_min_ctl_vec[0] = -1\n",
    "b25_map = model2.compute_contrast(miss_min_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b25_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_miss_minus_ctl.nii')\n",
    "nibabel.save(b25_map, b25_name)\n",
    "print(os.path.basename(b25_name))\n",
    "print(miss_min_ctl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: correct source vs wrong source encoding trials\n",
    "events3 = all_events.copy(deep = True)\n",
    "cols3 = ['onset', 'duration', 'ctl_miss_ws_cs']\n",
    "events3 = events3[cols3]\n",
    "events3.rename(columns={'ctl_miss_ws_cs':'trial_type'}, inplace=True)\n",
    "\n",
    "print(events3.iloc[0:15, :])\n",
    "\n",
    "#create the model\n",
    "model3 = FirstLevelModel(t_r=tr, drift_model = None, standardize = True, noise_model='ar1',\n",
    "                         hrf_model = hrf_model)    \n",
    "#Should data be standardized?\n",
    "\n",
    "#create the design matrices\n",
    "design3 = make_first_level_design_matrix(frame_times, events=events3,\n",
    "                                        drift_model=None, add_regs=confounds, \n",
    "                                        hrf_model=hrf_model)\n",
    "\n",
    "#fit model with design matrix\n",
    "model3 = model3.fit(fmri_img, design_matrices = design3)    \n",
    "\n",
    "design_matrix3 = model3.design_matrices_[0]    \n",
    "plot_design_matrix(design_matrix3)\n",
    "plt.show()\n",
    "print(design_matrix3.columns[0:5])\n",
    "\n",
    "##Condition order: control, correct source, missed, wrong source (alphabetical)\n",
    "\n",
    "#contrast 3.1: wrong source \n",
    "ws_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_vec[3] = 1\n",
    "b31_map = model3.compute_contrast(ws_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b31_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws.nii')\n",
    "nibabel.save(b31_map, b31_name)\n",
    "print(os.path.basename(b31_name))\n",
    "print(ws_vec)\n",
    "\n",
    "#contrast 3.2: correct source\n",
    "cs_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_vec[1] = 1\n",
    "b32_map = model3.compute_contrast(cs_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b32_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs.nii')\n",
    "nibabel.save(b32_map, b32_name)\n",
    "print(os.path.basename(b32_name))\n",
    "print(cs_vec)\n",
    "\n",
    "#contrast 3.3: correct source minus wrong source\n",
    "cs_minus_ws_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_ws_vec[1] = 1\n",
    "cs_minus_ws_vec[3] = -1\n",
    "b33_map = model3.compute_contrast(cs_minus_ws_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b33_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_ws.nii')\n",
    "nibabel.save(b33_map, b33_name)\n",
    "print(os.path.basename(b33_name))\n",
    "print(cs_minus_ws_vec)\n",
    "\n",
    "#contrast 3.4: correct source minus miss\n",
    "cs_minus_miss_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_miss_vec[1] = 1\n",
    "cs_minus_miss_vec[2] = -1\n",
    "b34_map = model3.compute_contrast(cs_minus_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b34_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_miss.nii')\n",
    "nibabel.save(b34_map, b34_name)\n",
    "print(os.path.basename(b34_name))\n",
    "print(cs_minus_miss_vec)\n",
    "\n",
    "#contrast 3.5: wrong source minus miss\n",
    "ws_minus_miss_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_minus_miss_vec[3] = 1\n",
    "ws_minus_miss_vec[2] = -1\n",
    "b35_map = model3.compute_contrast(ws_minus_miss_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b35_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws_minus_miss.nii')\n",
    "nibabel.save(b35_map, b35_name)\n",
    "print(os.path.basename(b35_name))\n",
    "print(ws_minus_miss_vec)\n",
    "\n",
    "#contrast 3.6: correct source minus control\n",
    "cs_minus_ctl_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "cs_minus_ctl_vec[1] = 1\n",
    "cs_minus_ctl_vec[0] = -1\n",
    "b36_map = model3.compute_contrast(cs_minus_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b36_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_cs_minus_ctl.nii')\n",
    "nibabel.save(b36_map, b36_name)\n",
    "print(os.path.basename(b36_name))\n",
    "print(cs_minus_ctl_vec)\n",
    "\n",
    "#contrast 3.7: wrong source minus control\n",
    "ws_minus_ctl_vec = np.repeat(0, design_matrix3.shape[1])\n",
    "ws_minus_ctl_vec[3] = 1\n",
    "ws_minus_ctl_vec[0] = -1\n",
    "b37_map = model3.compute_contrast(ws_minus_ctl_vec, output_type='effect_size') #\"effect_size\" for betas\n",
    "b37_name = os.path.join(outBeta_dir, 'betas_sub'+str(id)+'_ws_minus_ctl.nii')\n",
    "nibabel.save(b37_map, b37_name)\n",
    "print(os.path.basename(b37_name))\n",
    "print(ws_minus_ctl_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize beta maps\n",
    "\n",
    "#plotting brain images in nilearn:\n",
    "#http://nilearn.github.io/plotting/index.html\n",
    "\n",
    "#define directory where subject's functional mask and anatomical scan reside\n",
    "anat_dir = '/Users/mombot/Documents/Simexp/CIMAQ/Data/anat/122922'\n",
    "#subject's anatomical scan\n",
    "anat = nibabel.load(os.path.join(anat_dir, 'anat_sub122922_nuc_stereonl.nii'))\n",
    "plot_anat(anat)\n",
    "\n",
    "beta_list = glob.glob(os.path.join(outBeta_dir, '*.nii'))\n",
    "\n",
    "for beta in beta_list:\n",
    "    print(beta)\n",
    "    plot_img(beta)\n",
    "    plot_stat_map(stat_map_img=beta, bg_img=anat, cut_coords=(0, 0, 0), threshold=0.001, colorbar=True)\n",
    "    show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
