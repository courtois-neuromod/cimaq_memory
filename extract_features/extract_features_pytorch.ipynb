{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0272b49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/datasets/__init__.py:93: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  warn(\"Fetchers from the nilearn.datasets module will be \"\n",
      "/home/fnadeau/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/nilearn/glm/__init__.py:55: FutureWarning: The nilearn.glm module is experimental. It may change in any future release of Nilearn.\n",
      "  warn('The nilearn.glm module is experimental. '\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import more_itertools\n",
    "import nibabel\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from load_confounds import Minimal\n",
    "from joblib import parallel_backend\n",
    "from numpy import nan as NaN\n",
    "from matplotlib import pyplot as plt\n",
    "from nibabel.nifti1 import Nifti1Image\n",
    "from os import PathLike\n",
    "from pathlib import Path, PosixPath\n",
    "from random import sample\n",
    "from nilearn.glm.first_level import FirstLevelModel, check_events\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix\n",
    "from nilearn import image as nimage\n",
    "from nilearn.input_data import MultiNiftiMasker, NiftiLabelsMasker\n",
    "from nilearn.input_data import NiftiMapsMasker, NiftiMasker, NiftiSpheresMasker\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable, Sequence, Union\n",
    "# from nilearn import plotting as niplot\n",
    "# from nilearn import image as nimage\n",
    "\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "from cimaq_decoding_utils import get_difumo, get_difumo_cut_coords\n",
    "from cimaq_decoding_utils import get_fmri_sessions, fetch_fmriprep_session\n",
    "# from cimaq_decoding_utils import clean_scale_mask_img\n",
    "\n",
    "#libraries need to be installed in conda environment with pip install\n",
    "\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "# from sklearn.image.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf5ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"${PYTHONPATH}:/home/fnadeau/.linuxbrew/lib/python3.10/site-packages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0af6607",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imageio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41616/510263085.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimageio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# sorted(dict(os.environ).keys())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageio'"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "# sorted(dict(os.environ).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9068b80e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b27093",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'PIL.Image' has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41616/1271074469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mapath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mpilimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpilimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mimlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mpilimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.linuxbrew/opt/python@3.10/lib/python3.10/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 )\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'PIL.Image' has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "impaths = sorted(filter(os.path.isfile, sorted(Path('/data/simexp/fnadeau/WMStim/').iterdir())))\n",
    "# import PIL.Image as pilimage\n",
    "\n",
    "imlist = []\n",
    "for apath in impaths:\n",
    "    with pilimage.open(apath) as imfile:\n",
    "        image = pilimage.load(imfile)\n",
    "        imlist.append((os.path.basename(imfile, image)))\n",
    "        pilimage.close(imfile)\n",
    "\n",
    "imdict = dict(tuple(imlist))\n",
    "# sorted(dir(pilimage))\n",
    "# with pil\n",
    "# help(pilimage.open)\n",
    "# # help(extract_patches_2d)\n",
    "# sorted(dir(image))\n",
    "# patches = extract_patches_2d(one_image, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/'\n",
    "events_dir = '/data/simexp/CIMAQ_AS_BIDS/'\n",
    "atlases_dir = '../../nilearn_atlases/'\n",
    "\n",
    "\n",
    "\n",
    "subjects = fetch_cimaq(topdir=fmriprep_dir, events_dir=events_dir, n_ses=2).data\n",
    "sub00, sub01 = tuple(subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d777e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial_fmri(fmri_path:Union[str,os.PathLike, Nifti1Image],\n",
    "               events_path:Union[str,os.PathLike, pd.DataFrame],\n",
    "               sep:str='\\t', t_r:float=None,\n",
    "               **kwargs):\n",
    "    from itertools import starmap\n",
    "    from more_itertools import flatten\n",
    "    from nilearn import image as nimage\n",
    "    import pandas as pd\n",
    "    # Make pandas Intervals (b:list of beginnigs, e:list of ends)\n",
    "    mkintrvls = lambda b, e: list(starmap(pd.Interval,tuple(zip(b, e))))\n",
    "    fmri_img = nimage.load_img(fmri_path)\n",
    "    if not isinstance(events_path, pd.DataFrame):\n",
    "        events = pd.read_csv(events_path, sep=sep)\n",
    "    else:\n",
    "        events = events_path\n",
    "    t_r = [t_r if t_r is not None else\n",
    "           fmri_img.header.get_zooms()[-1]][0]\n",
    "    frame_times = np.arange(fmri_img.shape[-1]) * t_r\n",
    "    frame_ends = pd.Series(frame_times).add(t_r).values\n",
    "    frame_intervals = mkintrvls(pd.Series(frame_times).values,\n",
    "                                frame_ends)\n",
    "    trial_ends=(events.onset+abs(events.onset -\n",
    "                                 events.offset)+events.isi).values\n",
    "    trial_intervals = mkintrvls(events.onset.values, trial_ends)\n",
    "    valid_trial_idx = [trial[0] for trial in enumerate(trial_intervals)\n",
    "                       if trial[1].left<frame_intervals[-1].left]\n",
    "    valid_trials = pd.Series(trial_intervals).loc[valid_trial_idx].values\n",
    "#     trial_intervals = list(starmap(pd.Interval,tuple(zip(events.onset.values, trial_ends))))\n",
    "    bold_by_trial_indx = [[frame[0] for frame in enumerate(frame_intervals)\n",
    "                           if frame[1].left in trial] for trial in valid_trials]\n",
    "    bold_by_trial = list(nimage.index_img(fmri_img, idx)\n",
    "                         for idx in bold_by_trial_indx)\n",
    "    valid_frame_intervals = [pd.Series(frame_intervals).loc[bold_idx].values\n",
    "                             for bold_idx in bold_by_trial_indx]\n",
    "    perfo_labels = events.iloc[valid_trial_idx].recognition_performance.fillna('Ctl')\n",
    "    condition_labels = events.iloc[valid_trial_idx].trial_type\n",
    "    stim_labels = events.iloc[valid_trial_idx].stim_file.fillna('Ctl').values\n",
    "    categ_labels = events.iloc[valid_trial_idx].stim_category.fillna('Ctl').values\n",
    "    return pd.DataFrame(tuple(zip(valid_trial_idx, bold_by_trial,\n",
    "                                  bold_by_trial_indx, valid_trials,\n",
    "                                  valid_frame_intervals, condition_labels,\n",
    "                                  perfo_labels, stim_labels, categ_labels)),\n",
    "                        columns=['trials', 'trial_niftis', 'fmri_frames',\n",
    "                                 'trial_intervals', 'fmri_frame_intervals',\n",
    "                                 'condition_labels', 'performance_labels',\n",
    "                                 'stimuli_files', 'category_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64693f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.cleaned_fmri.get_fdata().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a084c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg_data=sub00.cleaned_fmri.get_fdata().reshape(-1,77,60,117)\n",
    "# testimg_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from sklearn.utils import Bunch\n",
    "from get_difumo import get_difumo\n",
    "atlases_dir = '../../nilearn_atlases/difumo_atlases/'\n",
    "#/difumo_atlases/256/3mm/maps.nii.gz'\n",
    "\n",
    "difumo_256 = get_difumo('../../nilearn_atlases/difumo_atlases/', 256, 3)\n",
    "\n",
    "sub00.mapsmasker = NiftiMapsMasker(difumo_256.maps, mask_img=sub00.mask_img,\n",
    "                                   resampling_target='mask',\n",
    "                                   allow_overlap=True, **sub00.masker_defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53398a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.mapsmasker.fit(sub00.cleaned_fmri)\n",
    "sub00.maps_signals = sub00.mapsmasker.transform_single_imgs(sub00.cleaned_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6927102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.maps_signals.shape#reshape(117,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71573407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn \n",
    "sub00.vectors = trial_fmri(sub00.cleaned_fmri, sub00.events)\n",
    "newshape=tuple(list(sub00.cleaned_fmri.shape)[:-1]+[117])\n",
    "testimg = nilearn.image.resample_img(sub00.cleaned_fmri,\n",
    "                                     target_affine=sub00.cleaned_fmri.affine,\n",
    "                                     target_shape=newshape,\n",
    "                                     interpolation='nearest',\n",
    "                                     clip=True, force_resample=True)\n",
    "# testmean = nilearn.image.concat_imgs([nilearn.image.mean_img(trial)\n",
    "#                            for trial in sub00.vectors.trial_niftis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611a8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.mapsmasker.fit(testmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4dcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.events.stim_category.replace({'K':'kitchen'}).fillna('Ctl').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4201da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub00.mapsmasker_mean = NiftiMapsMasker(difumo_256.maps, mask_img=sub00.mask_img,\n",
    "#                                         resampling_target='mask',\n",
    "#                                         allow_overlap=True, **sub00.masker_defs)\n",
    "# sub00.mapsmasker_mean.fit(testmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c250af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.maps_mean_signals = sub00.mapsmasker.transform_single_imgs(testmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d36a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sub00.behav.stim_category.replace({'K':'kitchen'}).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# sub_svc = sklearn.svm.NuSVC() # 0.68 avg acc up to now\n",
    "# sub_svc = sklearn.linear_model.SGDClassifier()\n",
    "# \n",
    "# Cross-validation within 10 folds of training set\n",
    "# predict\n",
    "# class_weight='balanced'\n",
    "cv_y_pred = cross_val_predict(sub_svc, X_train, y_train,\n",
    "                              groups=y_train, cv=10)\n",
    "# scores\n",
    "cv_acc = cross_val_score(sub_svc, X_train, y_train,\n",
    "                         groups=y_train, cv=10)\n",
    "print(f'Cross-validation accuracy: {cv_acc}')\n",
    "\n",
    "# evaluate overall model performance on training data\n",
    "overall_acc = accuracy_score(y_pred = cv_y_pred, y_true = y_train)\n",
    "overall_cr = classification_report(y_pred = cv_y_pred, y_true = y_train)\n",
    "print(f'Accuracy\\n{round(overall_acc, 3)}\\n\\nOverall Score\\n{overall_cr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub00.events.recognition_performance.values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size=0.4\n",
    "svc = LinearSVC(max_iter=100000)\n",
    "signals = sub00.maps_mean_signals\n",
    "# conditions = sub00.events.trial_type.values#.replace({'Enc':1,'Ctl':2}).values\n",
    "conditions = sub00.events.recognition_performance.fillna('Ctl').values\n",
    "# conditions = sub00.events.trial_type.replace({'Enc':'enc','Ctl':'ctl'}).astype('str')\n",
    "groups = sub00.behav.recognition_performance.fillna('Ctl').values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import it and define your fancy objects\n",
    "from sklearn.feature_selection import RFE\n",
    "svc = LinearSVC()\n",
    "rfe = RFE(SVC(kernel='linear', C=1.), n_features_to_select=117, step=0.25)\n",
    "\n",
    "# Create a new pipeline, composing the two classifiers `rfe` and `svc`\n",
    "\n",
    "rfe_svc = Pipeline([('rfe', rfe), ('svc', svc)])\n",
    "\n",
    "# Recompute the cross-validation score\n",
    "cv_scores = cross_val_score(rfe_svc, signals, target, cv=cv, n_jobs=-1, verbose=1)\n",
    "# But, be aware that this can take * A WHILE * ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ecab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1e-4<\n",
    "5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070c178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.svm import LinearSVC\n",
    "feature_selection = SelectPercentile(f_classif, percentile=10)\n",
    "anova_svc = Pipeline([('anova', feature_selection), ('svc', LinearSVC(max_iter=10000000))])\n",
    "# We can use our ``anova_svc`` object exactly as we were using our ``svc``\n",
    "# object previously.\n",
    "# As we want to investigate our model, we use sklearn `cross_validate` function\n",
    "# with `return_estimator = True` instead of cross_val_score, to save the estimator\n",
    "\n",
    "cv = LeaveOneGroupOut()\n",
    "fitted_pipeline = cross_validate(anova_svc, signals, conditions,\n",
    "                                 cv=cv, groups=groups, return_estimator=True)\n",
    "print(\n",
    "    \"ANOVA+SVC test score: {:.3f}\".format(fitted_pipeline[\"test_score\"].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66059c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     signals, groups, # x & y\n",
    "#     test_size = test_size, \n",
    "#     shuffle = True,\n",
    "#     stratify = groups)\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# svc.fit(signals, groups)\n",
    "# Here `cv=5` stipulates a 5-fold cross-validation\n",
    "cv_scores = cross_val_score(svc, signals,#sub00.maps_mean_signals,\n",
    "                            groups,#sub00.vectors.condition_labels,\n",
    "                            groups=conditions,\n",
    "                            cv=5)\n",
    "\n",
    "print(\"SVC accuracy: {:.3f}\".format(cv_scores.mean()))\n",
    "\n",
    "cv = LeaveOneGroupOut()\n",
    "cv_scores = cross_val_score(svc, signals,\n",
    "                            groups,\n",
    "                            cv=cv,\n",
    "                            scoring='roc_auc',\n",
    "                            groups=conditions,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "print(\"SVC accuracy (tuned parameters): {:.3f}\".format(cv_scores.mean()))\n",
    "\n",
    "# from sklearn.dummy import DummyClassifier\n",
    "# null_cv_scores = cross_val_score(DummyClassifier(),\n",
    "#                                  sub00.maps_mean_signals,\n",
    "#                                  conditions,\n",
    "#                                  cv=cv,\n",
    "#                                  groups=groups)\n",
    "\n",
    "# print(\"Dummy accuracy: {:.3f}\".format(null_cv_scores.mean()))\n",
    "\n",
    "# from sklearn.model_selection import permutation_test_score\n",
    "# null_cv_scores = permutation_test_score(svc,\n",
    "#                                         sub00.maps_mean_signals,\n",
    "#                                         conditions,\n",
    "#                                         cv=cv,\n",
    "#                                         groups=groups)\n",
    "\n",
    "# print(\"Permutation test score: {:.3f}\".format(null_cv_scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d698e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from dltk.io.augmentation import *\n",
    "from dltk.io.preprocessing import *\n",
    "\n",
    "fmriprep_dir = '/data/simexp/cimaq_preproc/fmriprep/'\n",
    "events_dir = '/data/simexp/fnadeau/CIMAQ_AS_BIDS_4/'\n",
    "%matplotlib inline\n",
    "\n",
    "# Timer helper class for benchmarking reading methods\n",
    "class Timer(object):\n",
    "    \"\"\"\n",
    "    Timer class\n",
    "    Wrap a will with a timing function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.t = time.time()\n",
    "        \n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        print(\"{} took {} seconds\".format(\n",
    "        self.name, time.time() - self.t))\n",
    "# Set up parameters\n",
    "batch_size = 1\n",
    "iterations = sub00.testmean.shape[-1]\n",
    "\n",
    "# Define the desired shapes and types of the training examples to pass to `read_fn`:\n",
    "reader_params = {'n_examples': 1,\n",
    "                 'example_size': list(sub00.testmean.shape),#[128, 224, 224],\n",
    "                 'extract_examples': False}\n",
    "\n",
    "reader_example_shapes = {'features': {'x': reader_params['example_size'] + [1,]},\n",
    "                         'labels': {'y': []}}\n",
    " \n",
    "reader_example_dtypes = {'features': {'x': tf.float32},\n",
    "                         'labels': {'y': tf.int32}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5258fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "def load_data(file_references, y, mode, params=None):\n",
    "    \n",
    "    data = {'features': [], 'labels': []}\n",
    "    sitk_t1 = sitk.ReadImage(file_references)\n",
    "    t1 = whitening(sitk.GetArrayFromImage(sitk_t1))\n",
    "    data['features'].append(images)\n",
    "    data['labels'].append(y.astype(np.int32))\n",
    "\n",
    "    data['features'] = np.array(data['features'])\n",
    "    data['labels'] = np.vstack(data['labels'])\n",
    "    \n",
    "    x = tf.placeholder(reader_example_dtypes['features']['x'], \n",
    "                       [*data['features'], 1])\n",
    "    y = tf.placeholder(reader_example_dtypes['labels']['y'], \n",
    "                       [None, 1])            \n",
    "    return x, y\n",
    "\n",
    "# Load all data into memory\n",
    "data = load_data(sub00.testmean, sub00.vectors.condition_labels,\n",
    "                 tf.estimator.ModeKeys.TRAIN, params=reader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# x = tf.placeholder(reader_example_dtypes['features']['x'], \n",
    "#                    [*data['features'], 1])\n",
    "# y = tf.placeholder(reader_example_dtypes['labels']['y'], \n",
    "#                    [None, 1])\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "dataset = dataset.repeat(None)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(1)\n",
    "\n",
    "# Check that features and labels dimensions match\n",
    "assert features.shape[0] == labels.shape[0]\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "nx = iterator.get_next()\n",
    "\n",
    "with tf.train.MonitoredTrainingSession() as sess_dict:\n",
    "    # Initialize iterator\n",
    "    sess_dict.run(iterator.initializer,\n",
    "                  feed_dict={x: data['features'],\n",
    "                             y: data['labels']})\n",
    "    \n",
    "    with Timer('Feed dictionary'):\n",
    "        # Timed feed dictionary example\n",
    "        for i in range(iterations):\n",
    "            # Get next features-labels pair\n",
    "            dict_batch_feat, dict_batch_lbl = sess_dict.run(nx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Method 1\n",
    "# def load_data(file_references, mode, params=None):\n",
    "    \n",
    "#     data = {'features': [], 'labels': []}\n",
    "#     sitk_t1 = sitk.ReadImage(file_references)\n",
    "#     t1 = whitening(sitk.GetArrayFromImage(sitk_t1))\n",
    "#     data['features'].append(images)\n",
    "#     data['labels'].append(y.astype(np.int32))\n",
    "\n",
    "#     data['features'] = np.array(data['features'])\n",
    "#     data['labels'] = np.vstack(data['labels'])\n",
    "            \n",
    "#     return data\n",
    "    \n",
    "    # We define a `read_fn` and iterate through the `file_references`, which\n",
    "    # can contain information about the data to be read (e.g. a file path):\n",
    "#     for meta_data in file_references:\n",
    "        \n",
    "        # Here, we parse the `sub_id` to construct a file path to read\n",
    "        # an image from.\n",
    "#         sub_id = meta_data[0]\n",
    "#         data_path = '../../data/IXI_HH/1mm'\n",
    "#         t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(sub_id))\n",
    "        \n",
    "        # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "        # the numpy array:\n",
    "      \n",
    "\n",
    "        # Normalise the image to zero mean/unit std dev:\n",
    "#         t1 = whitening(t1)\n",
    "        \n",
    "        # Create a 4D Tensor with a dummy dimension for channels\n",
    "#         t1 = t1[..., np.newaxis]\n",
    "\n",
    "        # Labels: Here, we parse the class *sex* from the file_references \n",
    "        # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "#         sex = np.int32(meta_data[1]) - 1\n",
    "#         y = sex\n",
    "#         y = np.int32(sub00.events.trial_type.values)\n",
    "        \n",
    "#         # If training should be done on image patches for improved mixing, \n",
    "#         # memory limitations or class balancing, call a patch extractor\n",
    "#         if params['extract_examples']:\n",
    "#             images = extract_random_example_array(\n",
    "#                 t1,\n",
    "#                 example_size=params['example_size'],\n",
    "#                 n_examples=params['n_examples'])\n",
    "            \n",
    "#             # Loop the extracted image patches\n",
    "#             for e in range(params['n_examples']):\n",
    "#                 data['features'].append(images[e].astype(np.float32))\n",
    "#                 data['labels'].append(y.astype(np.int32))\n",
    "                     \n",
    "#         # If desired (i.e. for evaluation, etc.), return the full images\n",
    "#         else:\n",
    "#             data['features'].append(images)\n",
    "#             data['labels'].append(y.astype(np.int32))\n",
    "\n",
    "#     data['features'] = np.array(data['features'])\n",
    "#     data['labels'] = np.vstack(data['labels'])\n",
    "            \n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensortest=load_data(sub00.cleaned_fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "######## Method 1 continued\n",
    "# Visualise the `dict_batch_feat` using matplotlib.\n",
    "input_tensor_shape = dict_batch_feat.shape\n",
    "center_slices = [s//2 for s in input_tensor_shape]\n",
    "\n",
    "# Visualise the `gen_batch_feat` using matplotlib.\n",
    "f, axarr = plt.subplots(1, input_tensor_shape[0], figsize=(15,5));\n",
    "f.suptitle(' '.join(['Visualisation of the `dict_batch_feat`',\n",
    "                     f'input tensor with shape={input_tensor_shape}']))\n",
    "\n",
    "for batch_id in range(input_tensor_shape[0]):\n",
    "    # Extract a center slice image\n",
    "    img_slice_ = np.squeeze(dict_batch_feat[batch_id, center_slices[1], :, :, :])\n",
    "    img_slice_ = np.flip(img_slice_, axis=0)\n",
    "    \n",
    "    # Plot\n",
    "    axarr[batch_id].imshow(img_slice_, cmap='gray');\n",
    "    axarr[batch_id].axis('off')\n",
    "    axarr[batch_id].set_title('batch_id={}'.format(batch_id))\n",
    "    \n",
    "f.subplots_adjust(wspace=0.05, hspace=0, top=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fa00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fn(file_references, mode, params=None):\n",
    "    \n",
    "    # We define a `read_fn` and iterate through the `file_references`, which\n",
    "    # can contain information about the data to be read (e.g. a file path):\n",
    "    for meta_data in file_references:\n",
    "        \n",
    "        # Here, we parse the `subject_id` to construct a file path to read\n",
    "        # an image from.\n",
    "        subject_id = meta_data[0]\n",
    "        data_path = '../../data/IXI_HH/1mm'\n",
    "        t1_fn = os.path.join(data_path, '{}/T1_1mm.nii.gz'.format(subject_id))\n",
    "        \n",
    "        # Read the .nii image containing a brain volume with SimpleITK and get \n",
    "        # the numpy array:\n",
    "        sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "        t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "        # Normalise the image to zero mean/unit std dev:\n",
    "        t1 = whitening(t1)\n",
    "        \n",
    "        # Create a 4D Tensor with a dummy dimension for channels\n",
    "        t1 = t1[..., np.newaxis]\n",
    "        \n",
    "        # If in PREDICT mode, yield the image (because there will be no label\n",
    "        # present). Additionally, yield the sitk.Image pointer (including all\n",
    "        # the header information) and some metadata (e.g. the subject id),\n",
    "        # to facilitate post-processing (e.g. reslicing) and saving.\n",
    "        # This can be useful when you want to use the same read function as \n",
    "        # python generator for deployment.\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            yield {'features': {'x': t1}}\n",
    "\n",
    "        # Labels: Here, we parse the class *sex* from the file_references \n",
    "        # \\in [1,2] and shift them to \\in [0,1] for training:\n",
    "        sex = np.int32(meta_data[1]) - 1\n",
    "        y = sex\n",
    "        \n",
    "        # If training should be done on image patches for improved mixing, \n",
    "        # memory limitations or class balancing, call a patch extractor\n",
    "        if params['extract_examples']:\n",
    "            images = extract_random_example_array(\n",
    "                t1,\n",
    "                example_size=params['example_size'],\n",
    "                n_examples=params['n_examples'])\n",
    "            \n",
    "            # Loop the extracted image patches and yield\n",
    "            for e in range(params['n_examples']):\n",
    "                yield {'features': {'x': images[e].astype(np.float32)},\n",
    "                       'labels': {'y': y.astype(np.int32)}}\n",
    "                     \n",
    "        # If desired (i.e. for evaluation, etc.), return the full images\n",
    "        else:\n",
    "            yield {'features': {'x': images},\n",
    "                   'labels': {'y': y.astype(np.int32)}}\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1005709",
   "metadata": {},
   "outputs": [],
   "source": [
    "path00='/data/simexp/fnadeau/cimaq_2d/sub-3002498/ses-V10/func/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold.nii.gz'\n",
    "# path00 = Path(path00)\n",
    "# # dir(path00)\n",
    "# # path00.split(path00.suffixes[0])[0]\n",
    "# justname = str(path00).rstrip(''.join(path00.suffixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa52d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fmriprep_mask(fmri_path:Union[str,os.PathLike],\n",
    "                      mask_ext:str='nii.gz', **kwargs):\n",
    "    bids_patt = lambda p: f'(?<={p})[a-zA-Z0-9]*'\n",
    "    prefixes = ['sub-','ses-','task-','space-']\n",
    "    mask_sfx = '_'.join([pf+re.search(bids_patt(pf),\n",
    "                                      os.path.basename(fmri_path)).group()\n",
    "                         for pf in prefixes]+[f'desc-brain_mask.{mask_ext}'])\n",
    "    return glob.glob(os.path.join(os.path.dirname(fmri_path), mask_sfx))[0]\n",
    "\n",
    "def get_fmriprep_anat(fmri_path:Union[str,os.PathLike],\n",
    "                      mdlt:str='T1w', ext:str='nii.gz',\n",
    "                      **kwargs):\n",
    "    space = '_space-'+re.search(f'(?<=_space-)[a-zA-Z0-9]*',\n",
    "                      os.path.basename(fmri_path)).group()\n",
    "    anat_suffix = f'*{space}_desc-preproc_{mdlt}.{ext}'\n",
    "    return str(next(list(Path(fmri_path).parents)[2].rglob(anat_suffix)))\n",
    "\n",
    "def get_events(fmri_path:Union[str,os.PathLike],\n",
    "               events_dir:Union[str,os.PathLike])->str:\n",
    "    sub_id, ses_id = Path(fmri_path).parts[-4:-2]\n",
    "    globbed = glob.glob(os.path.join(events_dir,\n",
    "                                  *Path(fmri_path).parts[-4:-2],\n",
    "                                  '*events.tsv'))\n",
    "    return [False if globbed == [] else globbed[0]][0]\n",
    "\n",
    "        \n",
    "def fetch_fmriprep_session(fmri_path:Union[str,os.PathLike],\n",
    "                           events_dir:Union[str,os.PathLike],\n",
    "                           strategy:str='Minimal',\n",
    "                           anat_mod:str='T1w',\n",
    "                           lc_kws:dict=None,\n",
    "                           clean_kws:dict=None,\n",
    "                           **kwargs):\n",
    "    import load_confounds\n",
    "    from inspect import getmembers\n",
    "    from sklearn.utils import Bunch\n",
    "    from pathlib import Path\n",
    "    sub_id, ses_id = Path(fmri_path).parts[-4:-2]\n",
    "    mask_path = get_fmriprep_mask(fmri_path)\n",
    "    anat_path = get_fmriprep_anat(fmri_path)\n",
    "    loader = dict(getmembers(load_confounds))[f'{strategy}']\n",
    "    loader = [loader(**lc_kws) if lc_kws is not None\n",
    "              else loader()][0]    \n",
    "    event_path = get_events(fmri_path,events_dir)\n",
    "    if event_path is False:\n",
    "        return False\n",
    "    else:\n",
    "        return Bunch(**dict(sub_id=sub_id, ses_id=ses_id,\n",
    "                            full_mask_path=mask_path,\n",
    "                            fmri_path=fmri_path,\n",
    "                            anat_path=anat_path,\n",
    "                            event_path=event_path,\n",
    "                            loader=loader,\n",
    "                            confounds_strategy=strategy))\n",
    "\n",
    "def load_fmriprep_session(fmri_path:Union[str,os.PathLike],\n",
    "                          events_dir:Union[str,os.PathLike],\n",
    "                          clean_kws:dict=None,\n",
    "                          masker_kws:dict=None,\n",
    "                          **kwargs):\n",
    "    from nilearn import image as nimage\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "    from nilearn.signal import clean\n",
    "    from sklearn.utils import Bunch\n",
    "    from glob import glob\n",
    "    from nilearn import image as nimage\n",
    "    session = fetch_fmriprep_session(fmri_path,events_dir)\n",
    "    fmri_img, mask_img, anat_img = \\\n",
    "        tuple(map(nimage.load_img,[session.fmri_path, session.full_mask_path,\n",
    "                                   session.anat_path]))\n",
    "    t_r = fmri_img.header.get_zooms()[-1]\n",
    "    conf = session.loader.load(session.fmri_path)\n",
    "    clean_defs = dict(standardize=False,\n",
    "                      standardize_confounds=False,\n",
    "                      high_pass=None, low_pass=None,\n",
    "                      t_r=fmri_img.header.get_zooms()[-1],\n",
    "                      ensure_finite=True)\n",
    "    masker_defs = dict(mask_img=mask_img,\n",
    "                       allow_overlap=True, t_r=t_r,\n",
    "                       standardize_confounds=False,\n",
    "                       resampling_target='mask')\n",
    "    if masker_kws is not None:\n",
    "        masker_defs.update(masker_kws)\n",
    "    if clean_kws is not None:\n",
    "        clean_defs.update(clean_kws)\n",
    "    cleaned_fmri = unmask(clean(apply_mask(fmri_img, mask_img,\n",
    "                                           smoothing_fwhm=8,\n",
    "                                           dtype='f'),\n",
    "                                confounds=conf,\n",
    "                                **clean_defs),\n",
    "                          mask_img)\n",
    "    cleaned_fmri = nimage.new_img_like(fmri_img, cleaned_fmri.get_fdata(),\n",
    "                               copy_header=True)\n",
    "    events = pd.read_csv(session.event_path, sep='\\t').iloc[1:,:]\n",
    "    loaded = Bunch(**dict(fmri_img=fmri_img, cleaned_fmri=cleaned_fmri,\n",
    "                          full_mask_img=mask_img, anat_img=anat_img,\n",
    "                          clean_params=clean_defs, masker_params=masker_defs,\n",
    "                          confounds=conf, events=events, t_r=t_r))\n",
    "    loaded.update(session)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# def rot90flip(data, rot_angle:int=None, scl, vol):\n",
    "    \n",
    "rot90flip=lambda a, s, v: np.rot90(a[:,:,s,v])\n",
    "rot180flip=lambda a, s, v: np.rot90(np.rot90(a[:,:,s,v]))\n",
    "rot270flip=lambda a, s, v: np.rot90(np.rot90(np.rot90(a[:,:,s,v])))\n",
    "rot_dict = {90: rot90flip, 180: rot180flip, 270: rot270flip}\n",
    "rot_dict[90]([0,1,2], 2, 3)\n",
    "# np.rot90([[0,1,2]][:, :, 2, 33])\n",
    "# import scipy\n",
    "# dir(scipy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a88560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = '../../flat_bold_png/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold_t306_z46.png'\n",
    "\n",
    "\n",
    "# imageio.imread(src).shape\n",
    "help(np.repeat)\n",
    "\n",
    "# Image.open(src)._Image__transformer()\n",
    "# dir(img._Image__transformer)\n",
    "# skimage.show(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e182c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "cimaq_stims = list(map(str, list(Path('../../WMStim').rglob('*.bmp'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b794fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(src:Union[str,os.PathLike]):\n",
    "    # Open image\n",
    "#     input_image = PIL.Image.fromarray(np.uint8(PIL.Image.open(src)))\n",
    "#     input_image = PIL.Image.fromarray(PIL.Image.open(src))\n",
    "    input_image = PIL.Image.open(src)\n",
    "    # Preprocess image\n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])])\n",
    "    return preprocess(input_image).unsqueeze(0)\n",
    "preprocess_image(cimaq_stims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(imageio.imread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c576dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(filter(lambda x: x.endswith('.bmp'), cimaq_stims)))==len(cimaq_stims)\n",
    "sorted(cimaq_stims)[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a287f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogLeNet.from_pretrained(\"googlenet\")\n",
    "\n",
    "def get_features(src, model):\n",
    "#     input_tensor = preprocess_image(src)\n",
    "#     model.eval()\n",
    "    return model.extract_features(preprocess_image(src))\n",
    "features = [get_features(src, model) for src in sorted(cimaq_stims)[6:]]\n",
    "labels = list(map(os.path.basename, sorted(cimaq_stims)[6:]))\n",
    "# list(get_features(cimaq_stims[x], model) for x in range(len(cimaq_stims)))\n",
    "# help(model.eval)\n",
    "# eval mode shape == torch.Size([1, 1024, 7, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc902a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "apath = '../../flat_bold_png/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold_t306_z46.png'\n",
    "from googlenet_pytorch import GoogLeNet\n",
    "from typing import Union\n",
    "# model = GoogLeNet.from_pretrained(\"ImageNet\")\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "import skimage\n",
    "\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from googlenet_pytorch import GoogLeNet \n",
    "\n",
    "\n",
    "def load_inputs(src:Union[str,os.PathLike]):\n",
    "    # Open image\n",
    "#     input_image = PIL.Image.open(BytesIO(imageio.imread(src)))\n",
    "#     model = VGG16()\n",
    "    model = GoogLeNet.from_pretrained(\"googlenet\")\n",
    "    input_image = PIL.Image.fromarray(np.stack(np.repeat(np.uint8(PIL.Image.open(src)),3, axis=0)))\n",
    "    # Preprocess image\n",
    "    preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.CenterCrop(224),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                          std=[0.229, 0.224, 0.225])])\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
    "\n",
    "    # Load class names\n",
    "#     labels_map = json.load(open(\"labels_map.txt\"))\n",
    "#     labels_map = [labels_map[str(i)] for i in range(1000)]\n",
    "\n",
    "    # Feature extraction\n",
    "    # ... image preprocessing as in the classification example ...\n",
    "    # inputs = torch.randn(1, 3, 224, 224)\n",
    "#     print(inputs.shape) # torch.Size([1, 3, 224, 224])\n",
    "\n",
    "    features = model.extract_features(input_batch)\n",
    "    print(features.shape) # torch.Size([1, 1024, 7, 7])\n",
    "\n",
    "    # Classify with GoogLeNet\n",
    "    model.eval()\n",
    "\n",
    "    # move the input and model to GPU for speed if available\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to(\"cuda\")\n",
    "        model.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_batch)\n",
    "        preds = torch.topk(logits, k=5).indices.squeeze(0).tolist()\n",
    "        print(\"-----\")\n",
    "        print(f'Predictions: {preds}')\n",
    "        for idx in preds:\n",
    "#             label = labels_map[idx]\n",
    "            prob = torch.softmax(logits, dim=1)[0, idx].item()\n",
    "#             print(f\"{label:<75} ({prob * 100:.2f}%)\")\n",
    "    return input_tensor, input_batch, features, prob\n",
    "test_tensors, test_batch, test_features, test_prob = load_inputs(cimaq_stims[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from googlenet_pytorch import GoogLeNet\n",
    "from numpy import asarray\n",
    "model = GoogLeNet.from_pretrained('ImageNet')\n",
    "apath = '../../flat_bold_png/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold/sub-3002498_ses-V10_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold_t306_z46.png'\n",
    "from typing import Union\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from googlenet_pytorch import GoogLeNet \n",
    "# ... image preprocessing as in the classification example ...\n",
    "# inputs = torch.randn(1, 3, 224, 224)\n",
    "# inputs = sorted(map(str,list(Path(testsess).iterdir())))\n",
    "# input_image = asarray(Image.open(apath))\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225])])\n",
    "input_tensor = preprocess(Image.open(apath))\n",
    "print(input_tensor.shape) # torch.Size([1, 3, 224, 224])\n",
    "\n",
    "features = model.extract_features(input_tensor)\n",
    "torch.Size([1, 1024, 7, 7])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d46edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(Image.open(apath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7facb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c631cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlenet_pytorch import GoogLeNet\n",
    "model = GoogLeNet.from_pretrained(\"googlenet\")\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from googlenet_pytorch import GoogLeNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6111664",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U IProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56801597",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('sub-3163875_ses-V03_task-memory_space-MNI152NLin2009cAsym_desc-preproc_bold'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772374",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatdir = '../../flat_bold_png/'\n",
    "from itertools import starmap\n",
    "from pathlib import Path\n",
    "testsess = sorted(Path(flatdir).iterdir())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ca3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(list(Path(testsess).iterdir())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c684637a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
